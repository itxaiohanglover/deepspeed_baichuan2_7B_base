[2023-11-23 13:31:28,242] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)===================================BUG REPORT===================================Welcome to bitsandbytes. For bug reports, please runpython -m bitsandbytes and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues================================================================================bin /usr/local/miniconda3/envs/llm_env/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda118.soCUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0CUDA SETUP: Highest compute capability among GPUs detected: 8.0CUDA SETUP: Detected CUDA version 118CUDA SETUP: Loading binary /usr/local/miniconda3/envs/llm_env/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...[2023-11-23 13:31:30,080] [WARNING] [runner.py:196:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.Detected CUDA_VISIBLE_DEVICES=0,1,2,3 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.[2023-11-23 13:31:30,080] [INFO] [runner.py:555:main] cmd = /usr/local/miniconda3/envs/llm_env/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=65224 --enable_each_rank_log=None train.py --train_args_file /hy-tmp/autodl-tmp/artboy/finetune/llm_code/training_config/baichuan2_config.json[2023-11-23 13:31:31,398] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)===================================BUG REPORT===================================Welcome to bitsandbytes. For bug reports, please runpython -m bitsandbytes and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues================================================================================bin /usr/local/miniconda3/envs/llm_env/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda118.so/usr/local/miniconda3/envs/llm_env/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/miniconda3/envs/llm_env did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...  warn(msg)CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths.../usr/local/miniconda3/envs/llm_env/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.Either way, this might cause trouble in the future:If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.  warn(msg)CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.soCUDA SETUP: Highest compute capability among GPUs detected: 8.0CUDA SETUP: Detected CUDA version 118CUDA SETUP: Loading binary /usr/local/miniconda3/envs/llm_env/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...[2023-11-23 13:31:33,015] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}[2023-11-23 13:31:33,016] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0[2023-11-23 13:31:33,016] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})[2023-11-23 13:31:33,016] [INFO] [launch.py:163:main] dist_world_size=4[2023-11-23 13:31:33,016] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3===================================BUG REPORT===================================Welcome to bitsandbytes. For bug reports, please runpython -m bitsandbytes and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues===================================================================================================================BUG REPORT===================================Welcome to bitsandbytes. For bug reports, please runpython -m bitsandbytes and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues================================================================================bin /usr/local/miniconda3/envs/llm_env/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda118.so/usr/local/miniconda3/envs/llm_env/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/miniconda3/envs/llm_env did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...  warn(msg)CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths.../usr/local/miniconda3/envs/llm_env/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.Either way, this might cause trouble in the future:If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.  warn(msg)CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.soCUDA SETUP: Highest compute capability among GPUs detected: 8.0CUDA SETUP: Detected CUDA version 118CUDA SETUP: Loading binary /usr/local/miniconda3/envs/llm_env/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...bin /usr/local/miniconda3/envs/llm_env/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda118.so/usr/local/miniconda3/envs/llm_env/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/miniconda3/envs/llm_env did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...  warn(msg)CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths.../usr/local/miniconda3/envs/llm_env/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.Either way, this might cause trouble in the future:If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.  warn(msg)CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.soCUDA SETUP: Highest compute capability among GPUs detected: 8.0CUDA SETUP: Detected CUDA version 118CUDA SETUP: Loading binary /usr/local/miniconda3/envs/llm_env/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...===================================BUG REPORT===================================Welcome to bitsandbytes. For bug reports, please runpython -m bitsandbytes and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues================================================================================bin /usr/local/miniconda3/envs/llm_env/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda118.so/usr/local/miniconda3/envs/llm_env/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/miniconda3/envs/llm_env did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...  warn(msg)CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths.../usr/local/miniconda3/envs/llm_env/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.Either way, this might cause trouble in the future:If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.  warn(msg)CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0CUDA SETUP: Highest compute capability among GPUs detected: 8.0CUDA SETUP: Detected CUDA version 118CUDA SETUP: Loading binary /usr/local/miniconda3/envs/llm_env/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...===================================BUG REPORT===================================Welcome to bitsandbytes. For bug reports, please runpython -m bitsandbytes and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues================================================================================bin /usr/local/miniconda3/envs/llm_env/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda118.so/usr/local/miniconda3/envs/llm_env/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/miniconda3/envs/llm_env did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...  warn(msg)CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths.../usr/local/miniconda3/envs/llm_env/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.Either way, this might cause trouble in the future:If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.  warn(msg)CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.soCUDA SETUP: Highest compute capability among GPUs detected: 8.0CUDA SETUP: Detected CUDA version 118CUDA SETUP: Loading binary /usr/local/miniconda3/envs/llm_env/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...[2023-11-23 13:31:36,049] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)[2023-11-23 13:31:36,050] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)[2023-11-23 13:31:36,085] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)[2023-11-23 13:31:36,085] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)[2023-11-23 13:31:36,924] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented[2023-11-23 13:31:36,924] [INFO] [comm.py:594:init_distributed] cdb=None[2023-11-23 13:31:36,932] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented[2023-11-23 13:31:36,932] [INFO] [comm.py:594:init_distributed] cdb=None[2023-11-23 13:31:36,932] [INFO] [comm.py:625:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl[2023-11-23 13:31:36,933] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented[2023-11-23 13:31:36,933] [INFO] [comm.py:594:init_distributed] cdb=None[2023-11-23 13:31:36,955] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented[2023-11-23 13:31:36,955] [INFO] [comm.py:594:init_distributed] cdb=None2023-11-23 13:31:37.950 | INFO     | __main__:setup_everything:35 - train_args:TrainingArguments(_n_gpu=1,adafactor=False,adam_beta1=0.9,adam_beta2=0.999,adam_epsilon=1e-08,auto_find_batch_size=False,bf16=False,bf16_full_eval=False,data_seed=None,dataloader_drop_last=False,dataloader_num_workers=5,dataloader_pin_memory=True,ddp_backend=None,ddp_bucket_cap_mb=None,ddp_find_unused_parameters=None,ddp_timeout=1800,debug=[],deepspeed=/hy-tmp/autodl-tmp/artboy/finetune/llm_code/deepspeed_config/deepspeed_config.json,disable_tqdm=False,do_eval=False,do_predict=False,do_train=False,eval_accumulation_steps=None,eval_delay=0,eval_steps=None,evaluation_strategy=no,fp16=True,fp16_backend=auto,fp16_full_eval=False,fp16_opt_level=O1,fsdp=[],fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},fsdp_min_num_params=0,fsdp_transformer_layer_cls_to_wrap=None,full_determinism=False,gradient_accumulation_steps=2,gradient_checkpointing=False,greater_is_better=None,group_by_length=False,half_precision_backend=auto,hub_model_id=None,hub_private_repo=False,hub_strategy=every_save,hub_token=<HUB_TOKEN>,ignore_data_skip=False,include_inputs_for_metrics=False,jit_mode_eval=False,label_names=None,label_smoothing_factor=0.0,learning_rate=1e-05,length_column_name=length,load_best_model_at_end=False,local_rank=0,log_level=passive,log_level_replica=warning,log_on_each_node=True,logging_dir=./output/baichuan2-sft-1e5-1123-2130/runs/Nov23_13-31-36_I1678c166a701b012d0,logging_first_step=False,logging_nan_inf_filter=True,logging_steps=10,logging_strategy=steps,lr_scheduler_type=cosine,max_grad_norm=1.0,max_steps=-1,metric_for_best_model=None,mp_parameters=,no_cuda=False,num_train_epochs=2,optim=adamw_hf,optim_args=None,output_dir=./output/baichuan2-sft-1e5-1123-2130,overwrite_output_dir=False,past_index=-1,per_device_eval_batch_size=8,per_device_train_batch_size=16,prediction_loss_only=False,push_to_hub=False,push_to_hub_model_id=None,push_to_hub_organization=None,push_to_hub_token=<PUSH_TO_HUB_TOKEN>,ray_scope=last,remove_unused_columns=False,report_to=['tensorboard'],resume_from_checkpoint=None,run_name=./output/baichuan2-sft-1e5-1123-2130,save_on_each_node=False,save_safetensors=False,save_steps=100,save_strategy=steps,save_total_limit=1,seed=42,sharded_ddp=[],skip_memory_metrics=True,tf32=None,torch_compile=False,torch_compile_backend=None,torch_compile_mode=None,torchdynamo=None,tpu_metrics_debug=False,tpu_num_cores=None,use_ipex=False,use_legacy_prediction_loop=False,use_mps_device=False,warmup_ratio=0.0,warmup_steps=200,weight_decay=0,xpu_backend=None,)2023-11-23 13:31:37.951 | INFO     | __main__:setup_everything:35 - train_args:TrainingArguments(_n_gpu=1,adafactor=False,adam_beta1=0.9,adam_beta2=0.999,adam_epsilon=1e-08,auto_find_batch_size=False,bf16=False,bf16_full_eval=False,data_seed=None,dataloader_drop_last=False,dataloader_num_workers=5,dataloader_pin_memory=True,ddp_backend=None,ddp_bucket_cap_mb=None,ddp_find_unused_parameters=None,ddp_timeout=1800,debug=[],deepspeed=/hy-tmp/autodl-tmp/artboy/finetune/llm_code/deepspeed_config/deepspeed_config.json,disable_tqdm=False,do_eval=False,do_predict=False,do_train=False,eval_accumulation_steps=None,eval_delay=0,eval_steps=None,evaluation_strategy=no,fp16=True,fp16_backend=auto,fp16_full_eval=False,fp16_opt_level=O1,fsdp=[],fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},fsdp_min_num_params=0,fsdp_transformer_layer_cls_to_wrap=None,full_determinism=False,gradient_accumulation_steps=2,gradient_checkpointing=False,greater_is_better=None,group_by_length=False,half_precision_backend=auto,hub_model_id=None,hub_private_repo=False,hub_strategy=every_save,hub_token=<HUB_TOKEN>,ignore_data_skip=False,include_inputs_for_metrics=False,jit_mode_eval=False,label_names=None,label_smoothing_factor=0.0,learning_rate=1e-05,length_column_name=length,load_best_model_at_end=False,local_rank=2,log_level=passive,log_level_replica=warning,log_on_each_node=True,logging_dir=./output/baichuan2-sft-1e5-1123-2130/runs/Nov23_13-31-36_I1678c166a701b012d0,logging_first_step=False,logging_nan_inf_filter=True,logging_steps=10,logging_strategy=steps,lr_scheduler_type=cosine,max_grad_norm=1.0,max_steps=-1,metric_for_best_model=None,mp_parameters=,no_cuda=False,num_train_epochs=2,optim=adamw_hf,optim_args=None,output_dir=./output/baichuan2-sft-1e5-1123-2130,overwrite_output_dir=False,past_index=-1,per_device_eval_batch_size=8,per_device_train_batch_size=16,prediction_loss_only=False,push_to_hub=False,push_to_hub_model_id=None,push_to_hub_organization=None,push_to_hub_token=<PUSH_TO_HUB_TOKEN>,ray_scope=last,remove_unused_columns=False,report_to=['tensorboard'],resume_from_checkpoint=None,run_name=./output/baichuan2-sft-1e5-1123-2130,save_on_each_node=False,save_safetensors=False,save_steps=100,save_strategy=steps,save_total_limit=1,seed=42,sharded_ddp=[],skip_memory_metrics=True,tf32=None,torch_compile=False,torch_compile_backend=None,torch_compile_mode=None,torchdynamo=None,tpu_metrics_debug=False,tpu_num_cores=None,use_ipex=False,use_legacy_prediction_loop=False,use_mps_device=False,warmup_ratio=0.0,warmup_steps=200,weight_decay=0,xpu_backend=None,)2023-11-23 13:31:37.951 | INFO     | __main__:init_components:45 - Initializing components...2023-11-23 13:31:37.952 | INFO     | __main__:init_components:45 - Initializing components...2023-11-23 13:31:37.960 | INFO     | __main__:setup_everything:35 - train_args:TrainingArguments(_n_gpu=1,adafactor=False,adam_beta1=0.9,adam_beta2=0.999,adam_epsilon=1e-08,auto_find_batch_size=False,bf16=False,bf16_full_eval=False,data_seed=None,dataloader_drop_last=False,dataloader_num_workers=5,dataloader_pin_memory=True,ddp_backend=None,ddp_bucket_cap_mb=None,ddp_find_unused_parameters=None,ddp_timeout=1800,debug=[],deepspeed=/hy-tmp/autodl-tmp/artboy/finetune/llm_code/deepspeed_config/deepspeed_config.json,disable_tqdm=False,do_eval=False,do_predict=False,do_train=False,eval_accumulation_steps=None,eval_delay=0,eval_steps=None,evaluation_strategy=no,fp16=True,fp16_backend=auto,fp16_full_eval=False,fp16_opt_level=O1,fsdp=[],fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},fsdp_min_num_params=0,fsdp_transformer_layer_cls_to_wrap=None,full_determinism=False,gradient_accumulation_steps=2,gradient_checkpointing=False,greater_is_better=None,group_by_length=False,half_precision_backend=auto,hub_model_id=None,hub_private_repo=False,hub_strategy=every_save,hub_token=<HUB_TOKEN>,ignore_data_skip=False,include_inputs_for_metrics=False,jit_mode_eval=False,label_names=None,label_smoothing_factor=0.0,learning_rate=1e-05,length_column_name=length,load_best_model_at_end=False,local_rank=3,log_level=passive,log_level_replica=warning,log_on_each_node=True,logging_dir=./output/baichuan2-sft-1e5-1123-2130/runs/Nov23_13-31-36_I1678c166a701b012d0,logging_first_step=False,logging_nan_inf_filter=True,logging_steps=10,logging_strategy=steps,lr_scheduler_type=cosine,max_grad_norm=1.0,max_steps=-1,metric_for_best_model=None,mp_parameters=,no_cuda=False,num_train_epochs=2,optim=adamw_hf,optim_args=None,output_dir=./output/baichuan2-sft-1e5-1123-2130,overwrite_output_dir=False,past_index=-1,per_device_eval_batch_size=8,per_device_train_batch_size=16,prediction_loss_only=False,push_to_hub=False,push_to_hub_model_id=None,push_to_hub_organization=None,push_to_hub_token=<PUSH_TO_HUB_TOKEN>,ray_scope=last,remove_unused_columns=False,report_to=['tensorboard'],resume_from_checkpoint=None,run_name=./output/baichuan2-sft-1e5-1123-2130,save_on_each_node=False,save_safetensors=False,save_steps=100,save_strategy=steps,save_total_limit=1,seed=42,sharded_ddp=[],skip_memory_metrics=True,tf32=None,torch_compile=False,torch_compile_backend=None,torch_compile_mode=None,torchdynamo=None,tpu_metrics_debug=False,tpu_num_cores=None,use_ipex=False,use_legacy_prediction_loop=False,use_mps_device=False,warmup_ratio=0.0,warmup_steps=200,weight_decay=0,xpu_backend=None,)2023-11-23 13:31:37.960 | INFO     | __main__:setup_everything:35 - train_args:TrainingArguments(_n_gpu=1,adafactor=False,adam_beta1=0.9,adam_beta2=0.999,adam_epsilon=1e-08,auto_find_batch_size=False,bf16=False,bf16_full_eval=False,data_seed=None,dataloader_drop_last=False,dataloader_num_workers=5,dataloader_pin_memory=True,ddp_backend=None,ddp_bucket_cap_mb=None,ddp_find_unused_parameters=None,ddp_timeout=1800,debug=[],deepspeed=/hy-tmp/autodl-tmp/artboy/finetune/llm_code/deepspeed_config/deepspeed_config.json,disable_tqdm=False,do_eval=False,do_predict=False,do_train=False,eval_accumulation_steps=None,eval_delay=0,eval_steps=None,evaluation_strategy=no,fp16=True,fp16_backend=auto,fp16_full_eval=False,fp16_opt_level=O1,fsdp=[],fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},fsdp_min_num_params=0,fsdp_transformer_layer_cls_to_wrap=None,full_determinism=False,gradient_accumulation_steps=2,gradient_checkpointing=False,greater_is_better=None,group_by_length=False,half_precision_backend=auto,hub_model_id=None,hub_private_repo=False,hub_strategy=every_save,hub_token=<HUB_TOKEN>,ignore_data_skip=False,include_inputs_for_metrics=False,jit_mode_eval=False,label_names=None,label_smoothing_factor=0.0,learning_rate=1e-05,length_column_name=length,load_best_model_at_end=False,local_rank=1,log_level=passive,log_level_replica=warning,log_on_each_node=True,logging_dir=./output/baichuan2-sft-1e5-1123-2130/runs/Nov23_13-31-36_I1678c166a701b012d0,logging_first_step=False,logging_nan_inf_filter=True,logging_steps=10,logging_strategy=steps,lr_scheduler_type=cosine,max_grad_norm=1.0,max_steps=-1,metric_for_best_model=None,mp_parameters=,no_cuda=False,num_train_epochs=2,optim=adamw_hf,optim_args=None,output_dir=./output/baichuan2-sft-1e5-1123-2130,overwrite_output_dir=False,past_index=-1,per_device_eval_batch_size=8,per_device_train_batch_size=16,prediction_loss_only=False,push_to_hub=False,push_to_hub_model_id=None,push_to_hub_organization=None,push_to_hub_token=<PUSH_TO_HUB_TOKEN>,ray_scope=last,remove_unused_columns=False,report_to=['tensorboard'],resume_from_checkpoint=None,run_name=./output/baichuan2-sft-1e5-1123-2130,save_on_each_node=False,save_safetensors=False,save_steps=100,save_strategy=steps,save_total_limit=1,seed=42,sharded_ddp=[],skip_memory_metrics=True,tf32=None,torch_compile=False,torch_compile_backend=None,torch_compile_mode=None,torchdynamo=None,tpu_metrics_debug=False,tpu_num_cores=None,use_ipex=False,use_legacy_prediction_loop=False,use_mps_device=False,warmup_ratio=0.0,warmup_steps=200,weight_decay=0,xpu_backend=None,)2023-11-23 13:31:37.961 | INFO     | __main__:init_components:45 - Initializing components...2023-11-23 13:31:37.961 | INFO     | __main__:init_components:45 - Initializing components...[2023-11-23 13:31:47,224] [INFO] [partition_parameters.py:453:__exit__] finished initializing model with 7.51B parametersLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.22s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.28s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.30s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.33s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.35s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.79s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.35s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.80s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.37s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.81s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.36s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.79s/it]2023-11-23 13:32:02.919 | INFO     | __main__:init_components:88 - Total model params: 0.00M2023-11-23 13:32:02.919 | INFO     | component.dataset:__init__:12 - Loading data: /hy-tmp/autodl-tmp/artboy/finetune/llm_code/data/psychology_data.jsonl2023-11-23 13:32:02.925 | INFO     | __main__:init_components:88 - Total model params: 0.00M2023-11-23 13:32:02.925 | INFO     | component.dataset:__init__:12 - Loading data: /hy-tmp/autodl-tmp/artboy/finetune/llm_code/data/psychology_data.jsonl2023-11-23 13:32:02.944 | INFO     | __main__:init_components:88 - Total model params: 0.00M2023-11-23 13:32:02.945 | INFO     | component.dataset:__init__:12 - Loading data: /hy-tmp/autodl-tmp/artboy/finetune/llm_code/data/psychology_data.jsonl2023-11-23 13:32:02.950 | INFO     | __main__:init_components:88 - Total model params: 0.00M2023-11-23 13:32:02.951 | INFO     | component.dataset:__init__:12 - Loading data: /hy-tmp/autodl-tmp/artboy/finetune/llm_code/data/psychology_data.jsonl2023-11-23 13:32:03.097 | INFO     | component.dataset:__init__:15 - there are 77250 data in dataset2023-11-23 13:32:03.102 | INFO     | component.dataset:__init__:15 - there are 77250 data in dataset2023-11-23 13:32:03.106 | INFO     | __main__:main:117 - *** starting training ***2023-11-23 13:32:03.112 | INFO     | __main__:main:117 - *** starting training ***2023-11-23 13:32:03.123 | INFO     | component.dataset:__init__:15 - there are 77250 data in dataset2023-11-23 13:32:03.133 | INFO     | __main__:main:117 - *** starting training ***2023-11-23 13:32:03.158 | INFO     | component.dataset:__init__:15 - there are 77250 data in dataset2023-11-23 13:32:03.170 | INFO     | __main__:main:117 - *** starting training ***Installed CUDA version 11.8 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combinationInstalled CUDA version 11.8 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combinationInstalled CUDA version 11.8 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combinationInstalled CUDA version 11.8 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combinationInstalled CUDA version 11.8 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combinationUsing /root/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...Installed CUDA version 11.8 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combinationUsing /root/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...Installed CUDA version 11.8 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combinationUsing /root/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...Installed CUDA version 11.8 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combinationUsing /root/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...Detected CUDA files, patching ldflagsEmitting ninja build file /root/.cache/torch_extensions/py38_cu117/cpu_adam/build.ninja...Building extension module cpu_adam...Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)ninja: no work to do.Loading extension module cpu_adam...Loading extension module cpu_adam...Time to load cpu_adam op: 3.1233010292053223 secondsTime to load cpu_adam op: 3.145629644393921 secondsLoading extension module cpu_adam...Time to load cpu_adam op: 3.1779661178588867 secondsLoading extension module cpu_adam...Time to load cpu_adam op: 3.067791700363159 secondsParameter Offload: Total persistent parameters: 266240 in 65 params  0%|          | 0/1208 [00:00<?, ?it/s]/usr/local/miniconda3/envs/llm_env/lib/python3.8/site-packages/xformers/ops/fmha/flash.py:338: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()  and inp.query.storage().data_ptr() == inp.key.storage().data_ptr()/usr/local/miniconda3/envs/llm_env/lib/python3.8/site-packages/xformers/ops/fmha/flash.py:338: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()  and inp.query.storage().data_ptr() == inp.key.storage().data_ptr()/usr/local/miniconda3/envs/llm_env/lib/python3.8/site-packages/xformers/ops/fmha/flash.py:338: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()  and inp.query.storage().data_ptr() == inp.key.storage().data_ptr()/usr/local/miniconda3/envs/llm_env/lib/python3.8/site-packages/xformers/ops/fmha/flash.py:338: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()  and inp.query.storage().data_ptr() == inp.key.storage().data_ptr()  0%|          | 1/1208 [00:17<5:45:51, 17.19s/it]  0%|          | 2/1208 [00:30<4:54:49, 14.67s/it]  0%|          | 3/1208 [00:43<4:39:59, 13.94s/it]  0%|          | 4/1208 [00:56<4:31:26, 13.53s/it]  0%|          | 5/1208 [01:08<4:26:41, 13.30s/it]  0%|          | 6/1208 [01:21<4:23:32, 13.16s/it]  1%|          | 7/1208 [01:34<4:20:56, 13.04s/it]  1%|          | 8/1208 [01:47<4:19:25, 12.97s/it]  1%|          | 9/1208 [02:00<4:19:23, 12.98s/it]  1%|          | 10/1208 [02:13<4:18:08, 12.93s/it]tried to get lr value before scheduler/optimizer started stepping, returning lr=0tried to get lr value before scheduler/optimizer started stepping, returning lr=0tried to get lr value before scheduler/optimizer started stepping, returning lr=0tried to get lr value before scheduler/optimizer started stepping, returning lr=0                                                   {'loss': 3.0184, 'learning_rate': 0, 'epoch': 0.02}  1%|          | 10/1208 [02:13<4:18:08, 12.93s/it]  1%|          | 11/1208 [02:26<4:17:02, 12.88s/it]  1%|          | 12/1208 [02:52<5:40:04, 17.06s/it]  1%|          | 13/1208 [03:09<5:40:12, 17.08s/it]  1%|          | 14/1208 [03:26<5:37:22, 16.95s/it]  1%|          | 15/1208 [03:42<5:33:18, 16.76s/it]  1%|▏         | 16/1208 [03:59<5:30:51, 16.65s/it]  1%|▏         | 17/1208 [04:15<5:27:25, 16.49s/it][2023-11-23 13:37:06,694] [WARNING] [stage3.py:1850:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time  1%|▏         | 18/1208 [04:31<5:27:11, 16.50s/it]  2%|▏         | 19/1208 [04:48<5:25:16, 16.41s/it]  2%|▏         | 20/1208 [05:04<5:25:59, 16.46s/it]                                                   {'loss': 2.6833, 'learning_rate': 4.1470233384070705e-06, 'epoch': 0.03}  2%|▏         | 20/1208 [05:04<5:25:59, 16.46s/it]  2%|▏         | 21/1208 [05:20<5:22:54, 16.32s/it]  2%|▏         | 22/1208 [05:36<5:20:51, 16.23s/it]  2%|▏         | 23/1208 [05:52<5:19:51, 16.20s/it]  2%|▏         | 24/1208 [06:08<5:20:00, 16.22s/it]  2%|▏         | 25/1208 [06:25<5:19:51, 16.22s/it]  2%|▏         | 26/1208 [06:41<5:18:40, 16.18s/it]  2%|▏         | 27/1208 [06:57<5:17:58, 16.15s/it]  2%|▏         | 28/1208 [07:13<5:19:00, 16.22s/it]  2%|▏         | 29/1208 [07:30<5:19:05, 16.24s/it]  2%|▏         | 30/1208 [07:46<5:18:04, 16.20s/it]                                                   {'loss': 2.4079, 'learning_rate': 5.557309567291557e-06, 'epoch': 0.05}  2%|▏         | 30/1208 [07:46<5:18:04, 16.20s/it]  3%|▎         | 31/1208 [08:02<5:15:47, 16.10s/it]  3%|▎         | 32/1208 [08:18<5:16:43, 16.16s/it]  3%|▎         | 33/1208 [08:34<5:15:41, 16.12s/it]  3%|▎         | 34/1208 [08:50<5:15:01, 16.10s/it]  3%|▎         | 35/1208 [09:06<5:14:06, 16.07s/it]  3%|▎         | 36/1208 [09:22<5:12:48, 16.01s/it]  3%|▎         | 37/1208 [09:38<5:13:50, 16.08s/it]  3%|▎         | 38/1208 [09:54<5:13:22, 16.07s/it]  3%|▎         | 39/1208 [10:10<5:10:38, 15.94s/it]  3%|▎         | 40/1208 [10:26<5:10:42, 15.96s/it]                                                   {'loss': 2.3059, 'learning_rate': 6.355406060132516e-06, 'epoch': 0.07}  3%|▎         | 40/1208 [10:26<5:10:42, 15.96s/it]  3%|▎         | 41/1208 [10:42<5:11:35, 16.02s/it]  3%|▎         | 42/1208 [10:58<5:10:08, 15.96s/it]  4%|▎         | 43/1208 [11:13<5:08:49, 15.90s/it]  4%|▎         | 44/1208 [11:30<5:11:16, 16.05s/it]  4%|▎         | 45/1208 [11:46<5:10:57, 16.04s/it]  4%|▍         | 46/1208 [12:02<5:11:07, 16.07s/it]  4%|▍         | 47/1208 [12:18<5:12:25, 16.15s/it]  4%|▍         | 48/1208 [12:34<5:08:56, 15.98s/it]  4%|▍         | 49/1208 [12:50<5:07:53, 15.94s/it]  4%|▍         | 50/1208 [13:06<5:10:59, 16.11s/it]                                                   {'loss': 2.2656, 'learning_rate': 6.914575690124303e-06, 'epoch': 0.08}  4%|▍         | 50/1208 [13:06<5:10:59, 16.11s/it]  4%|▍         | 51/1208 [13:23<5:11:43, 16.17s/it]  4%|▍         | 52/1208 [13:38<5:09:44, 16.08s/it]  4%|▍         | 53/1208 [13:54<5:06:46, 15.94s/it]  4%|▍         | 54/1208 [14:10<5:07:37, 15.99s/it]  5%|▍         | 55/1208 [14:26<5:04:32, 15.85s/it]  5%|▍         | 56/1208 [14:41<5:03:31, 15.81s/it]  5%|▍         | 57/1208 [14:57<5:04:12, 15.86s/it]  5%|▍         | 58/1208 [15:13<5:04:48, 15.90s/it]  5%|▍         | 59/1208 [15:29<5:03:00, 15.82s/it]  5%|▍         | 60/1208 [15:45<5:05:36, 15.97s/it]                                                   {'loss': 2.2277, 'learning_rate': 7.345389165780057e-06, 'epoch': 0.1}  5%|▍         | 60/1208 [15:45<5:05:36, 15.97s/it]  5%|▌         | 61/1208 [16:01<5:04:25, 15.92s/it]  5%|▌         | 62/1208 [16:17<5:03:03, 15.87s/it]  5%|▌         | 63/1208 [16:33<5:04:02, 15.93s/it]  5%|▌         | 64/1208 [16:49<5:03:40, 15.93s/it]  5%|▌         | 65/1208 [17:05<5:04:58, 16.01s/it]  5%|▌         | 66/1208 [17:21<5:03:49, 15.96s/it]  6%|▌         | 67/1208 [17:37<5:04:44, 16.03s/it]  6%|▌         | 68/1208 [17:53<5:05:23, 16.07s/it]  6%|▌         | 69/1208 [18:09<5:03:46, 16.00s/it]  6%|▌         | 70/1208 [18:25<5:03:45, 16.02s/it]                                                   {'loss': 2.2219, 'learning_rate': 7.69591015753426e-06, 'epoch': 0.12}  6%|▌         | 70/1208 [18:25<5:03:45, 16.02s/it]  6%|▌         | 71/1208 [18:41<5:03:45, 16.03s/it]  6%|▌         | 72/1208 [18:57<5:04:15, 16.07s/it]  6%|▌         | 73/1208 [19:13<5:01:53, 15.96s/it]  6%|▌         | 74/1208 [19:29<4:59:10, 15.83s/it]  6%|▌         | 75/1208 [19:45<4:59:07, 15.84s/it]  6%|▋         | 76/1208 [20:00<4:59:10, 15.86s/it]  6%|▋         | 77/1208 [20:16<4:57:13, 15.77s/it]  6%|▋         | 78/1208 [20:31<4:54:06, 15.62s/it]  7%|▋         | 79/1208 [20:47<4:56:52, 15.78s/it]  7%|▋         | 80/1208 [21:03<4:57:32, 15.83s/it]                                                   {'loss': 2.1926, 'learning_rate': 7.991417296612165e-06, 'epoch': 0.13}  7%|▋         | 80/1208 [21:03<4:57:32, 15.83s/it]  7%|▋         | 81/1208 [21:19<4:59:00, 15.92s/it]  7%|▋         | 82/1208 [21:35<4:58:55, 15.93s/it]  7%|▋         | 83/1208 [21:51<4:57:04, 15.84s/it]  7%|▋         | 84/1208 [22:07<4:56:41, 15.84s/it]  7%|▋         | 85/1208 [22:23<4:58:04, 15.93s/it]  7%|▋         | 86/1208 [22:39<4:56:33, 15.86s/it]  7%|▋         | 87/1208 [22:54<4:54:59, 15.79s/it]  7%|▋         | 88/1208 [23:10<4:54:34, 15.78s/it]  7%|▋         | 89/1208 [23:26<4:55:40, 15.85s/it]  7%|▋         | 90/1208 [23:42<4:54:34, 15.81s/it]                                                   {'loss': 2.1912, 'learning_rate': 8.246859427588061e-06, 'epoch': 0.15}  7%|▋         | 90/1208 [23:42<4:54:34, 15.81s/it]  8%|▊         | 91/1208 [23:58<4:57:42, 15.99s/it]  8%|▊         | 92/1208 [24:14<4:58:15, 16.04s/it]  8%|▊         | 93/1208 [24:30<4:56:41, 15.97s/it]  8%|▊         | 94/1208 [24:46<4:55:12, 15.90s/it]  8%|▊         | 95/1208 [25:02<4:54:56, 15.90s/it]  8%|▊         | 96/1208 [25:18<4:54:51, 15.91s/it]  8%|▊         | 97/1208 [25:34<4:55:26, 15.96s/it]  8%|▊         | 98/1208 [25:50<4:54:11, 15.90s/it]  8%|▊         | 99/1208 [26:06<4:54:48, 15.95s/it]  8%|▊         | 100/1208 [26:21<4:52:09, 15.82s/it]                                                    {'loss': 2.1935, 'learning_rate': 8.471814840824796e-06, 'epoch': 0.17}  8%|▊         | 100/1208 [26:21<4:52:09, 15.82s/it]/usr/local/miniconda3/envs/llm_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.  warnings.warn(/usr/local/miniconda3/envs/llm_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.  warnings.warn(/usr/local/miniconda3/envs/llm_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.  warnings.warn(/usr/local/miniconda3/envs/llm_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.  warnings.warn(  8%|▊         | 101/1208 [27:20<8:49:10, 28.68s/it]  8%|▊         | 102/1208 [27:36<7:38:57, 24.90s/it]  9%|▊         | 103/1208 [27:51<6:45:04, 22.00s/it]  9%|▊         | 104/1208 [28:06<6:07:06, 19.95s/it]  9%|▊         | 105/1208 [28:22<5:40:20, 18.51s/it]  9%|▉         | 106/1208 [28:37<5:21:27, 17.50s/it]  9%|▉         | 107/1208 [28:52<5:06:58, 16.73s/it]  9%|▉         | 108/1208 [29:07<4:59:54, 16.36s/it]  9%|▉         | 109/1208 [29:22<4:52:40, 15.98s/it]  9%|▉         | 110/1208 [29:37<4:47:40, 15.72s/it]                                                    {'loss': 2.1941, 'learning_rate': 8.672790873470093e-06, 'epoch': 0.18}  9%|▉         | 110/1208 [29:37<4:47:40, 15.72s/it]  9%|▉         | 111/1208 [29:52<4:43:07, 15.49s/it]  9%|▉         | 112/1208 [30:07<4:40:40, 15.37s/it]  9%|▉         | 113/1208 [30:22<4:38:56, 15.28s/it]  9%|▉         | 114/1208 [30:38<4:37:39, 15.23s/it] 10%|▉         | 115/1208 [30:53<4:38:19, 15.28s/it] 10%|▉         | 116/1208 [31:08<4:38:58, 15.33s/it] 10%|▉         | 117/1208 [31:24<4:38:02, 15.29s/it] 10%|▉         | 118/1208 [31:39<4:38:12, 15.31s/it] 10%|▉         | 119/1208 [31:54<4:37:21, 15.28s/it] 10%|▉         | 120/1208 [32:10<4:39:06, 15.39s/it]                                                    {'loss': 2.1825, 'learning_rate': 8.854410858528194e-06, 'epoch': 0.2} 10%|▉         | 120/1208 [32:10<4:39:06, 15.39s/it] 10%|█         | 121/1208 [32:25<4:40:09, 15.46s/it] 10%|█         | 122/1208 [32:42<4:45:18, 15.76s/it] 10%|█         | 123/1208 [32:57<4:43:43, 15.69s/it] 10%|█         | 124/1208 [33:13<4:42:44, 15.65s/it] 10%|█         | 125/1208 [33:28<4:40:27, 15.54s/it] 10%|█         | 126/1208 [33:44<4:40:32, 15.56s/it] 11%|█         | 127/1208 [33:59<4:40:47, 15.59s/it] 11%|█         | 128/1208 [34:15<4:39:42, 15.54s/it] 11%|█         | 129/1208 [34:30<4:39:12, 15.53s/it] 11%|█         | 130/1208 [34:46<4:39:11, 15.54s/it]                                                    {'loss': 2.1729, 'learning_rate': 9.020077814299048e-06, 'epoch': 0.22} 11%|█         | 130/1208 [34:46<4:39:11, 15.54s/it] 11%|█         | 131/1208 [35:02<4:39:18, 15.56s/it] 11%|█         | 132/1208 [35:17<4:39:35, 15.59s/it] 11%|█         | 133/1208 [35:33<4:38:37, 15.55s/it] 11%|█         | 134/1208 [35:48<4:38:11, 15.54s/it] 11%|█         | 135/1208 [36:04<4:38:05, 15.55s/it] 11%|█▏        | 136/1208 [36:19<4:37:18, 15.52s/it] 11%|█▏        | 137/1208 [36:35<4:37:07, 15.53s/it] 11%|█▏        | 138/1208 [36:50<4:36:36, 15.51s/it] 12%|█▏        | 139/1208 [37:06<4:36:26, 15.52s/it] 12%|█▏        | 140/1208 [37:21<4:37:17, 15.58s/it]                                                    {'loss': 2.1576, 'learning_rate': 9.172369392299995e-06, 'epoch': 0.23} 12%|█▏        | 140/1208 [37:22<4:37:17, 15.58s/it] 12%|█▏        | 141/1208 [37:37<4:35:40, 15.50s/it] 12%|█▏        | 142/1208 [37:52<4:35:51, 15.53s/it] 12%|█▏        | 143/1208 [38:08<4:37:11, 15.62s/it] 12%|█▏        | 144/1208 [38:24<4:38:11, 15.69s/it] 12%|█▏        | 145/1208 [38:40<4:38:14, 15.71s/it] 12%|█▏        | 146/1208 [38:56<4:37:51, 15.70s/it] 12%|█▏        | 147/1208 [39:11<4:37:50, 15.71s/it] 12%|█▏        | 148/1208 [39:27<4:35:08, 15.57s/it] 12%|█▏        | 149/1208 [39:42<4:34:30, 15.55s/it] 12%|█▏        | 150/1208 [39:57<4:31:46, 15.41s/it]                                                    {'loss': 2.1637, 'learning_rate': 9.313284938885426e-06, 'epoch': 0.25} 12%|█▏        | 150/1208 [39:57<4:31:46, 15.41s/it] 12%|█▎        | 151/1208 [40:13<4:32:52, 15.49s/it] 13%|█▎        | 152/1208 [40:28<4:31:19, 15.42s/it] 13%|█▎        | 153/1208 [40:44<4:32:04, 15.47s/it] 13%|█▎        | 154/1208 [40:59<4:33:00, 15.54s/it] 13%|█▎        | 155/1208 [41:15<4:31:55, 15.49s/it] 13%|█▎        | 156/1208 [41:30<4:33:05, 15.58s/it] 13%|█▎        | 157/1208 [41:46<4:32:33, 15.56s/it] 13%|█▎        | 158/1208 [42:02<4:32:18, 15.56s/it] 13%|█▎        | 159/1208 [42:17<4:30:56, 15.50s/it] 13%|█▎        | 160/1208 [42:32<4:28:43, 15.38s/it]                                                    {'loss': 2.12, 'learning_rate': 9.44440651580982e-06, 'epoch': 0.26} 13%|█▎        | 160/1208 [42:32<4:28:43, 15.38s/it] 13%|█▎        | 161/1208 [42:47<4:28:47, 15.40s/it] 13%|█▎        | 162/1208 [43:03<4:29:32, 15.46s/it] 13%|█▎        | 163/1208 [43:18<4:28:11, 15.40s/it] 14%|█▎        | 164/1208 [43:34<4:27:09, 15.35s/it] 14%|█▎        | 165/1208 [43:49<4:26:18, 15.32s/it] 14%|█▎        | 166/1208 [44:04<4:26:48, 15.36s/it] 14%|█▍        | 167/1208 [44:20<4:26:20, 15.35s/it] 14%|█▍        | 168/1208 [44:35<4:25:57, 15.34s/it] 14%|█▍        | 169/1208 [44:50<4:26:49, 15.41s/it] 14%|█▍        | 170/1208 [45:06<4:27:28, 15.46s/it]                                                    {'loss': 2.1189, 'learning_rate': 9.567007507371588e-06, 'epoch': 0.28} 14%|█▍        | 170/1208 [45:06<4:27:28, 15.46s/it] 14%|█▍        | 171/1208 [45:22<4:27:45, 15.49s/it] 14%|█▍        | 172/1208 [45:37<4:27:55, 15.52s/it] 14%|█▍        | 173/1208 [45:53<4:27:38, 15.52s/it] 14%|█▍        | 174/1208 [46:08<4:27:49, 15.54s/it] 14%|█▍        | 175/1208 [46:24<4:28:04, 15.57s/it] 15%|█▍        | 176/1208 [46:39<4:27:24, 15.55s/it] 15%|█▍        | 177/1208 [46:55<4:26:00, 15.48s/it] 15%|█▍        | 178/1208 [47:10<4:25:24, 15.46s/it] 15%|█▍        | 179/1208 [47:26<4:24:24, 15.42s/it] 15%|█▍        | 180/1208 [47:41<4:22:58, 15.35s/it]                                                    {'loss': 2.1293, 'learning_rate': 9.682128041841535e-06, 'epoch': 0.3} 15%|█▍        | 180/1208 [47:41<4:22:58, 15.35s/it] 15%|█▍        | 181/1208 [47:56<4:21:58, 15.31s/it] 15%|█▌        | 182/1208 [48:11<4:20:38, 15.24s/it] 15%|█▌        | 183/1208 [48:26<4:20:35, 15.25s/it] 15%|█▌        | 184/1208 [48:41<4:19:05, 15.18s/it] 15%|█▌        | 185/1208 [48:57<4:19:55, 15.24s/it] 15%|█▌        | 186/1208 [49:12<4:18:26, 15.17s/it] 15%|█▌        | 187/1208 [49:27<4:19:12, 15.23s/it] 16%|█▌        | 188/1208 [49:42<4:19:09, 15.24s/it] 16%|█▌        | 189/1208 [49:57<4:17:50, 15.18s/it] 16%|█▌        | 190/1208 [50:12<4:16:57, 15.15s/it]                                                    {'loss': 2.1288, 'learning_rate': 9.79062869769246e-06, 'epoch': 0.31} 16%|█▌        | 190/1208 [50:12<4:16:57, 15.15s/it] 16%|█▌        | 191/1208 [50:28<4:18:09, 15.23s/it] 16%|█▌        | 192/1208 [50:43<4:17:03, 15.18s/it] 16%|█▌        | 193/1208 [50:58<4:16:59, 15.19s/it] 16%|█▌        | 194/1208 [51:13<4:16:46, 15.19s/it] 16%|█▌        | 195/1208 [51:29<4:18:36, 15.32s/it] 16%|█▌        | 196/1208 [51:44<4:17:48, 15.29s/it] 16%|█▋        | 197/1208 [51:59<4:16:38, 15.23s/it] 16%|█▋        | 198/1208 [52:15<4:17:44, 15.31s/it] 16%|█▋        | 199/1208 [52:30<4:18:06, 15.35s/it] 17%|█▋        | 200/1208 [52:45<4:17:26, 15.32s/it]                                                    {'loss': 2.1197, 'learning_rate': 9.893229590500635e-06, 'epoch': 0.33} 17%|█▋        | 200/1208 [52:45<4:17:26, 15.32s/it] 17%|█▋        | 201/1208 [53:50<8:26:21, 30.17s/it] 17%|█▋        | 202/1208 [54:06<7:13:12, 25.84s/it] 17%|█▋        | 203/1208 [54:21<6:18:32, 22.60s/it] 17%|█▋        | 204/1208 [54:36<5:40:15, 20.33s/it] 17%|█▋        | 205/1208 [54:51<5:13:18, 18.74s/it] 17%|█▋        | 206/1208 [55:06<4:54:43, 17.65s/it] 17%|█▋        | 207/1208 [55:21<4:42:14, 16.92s/it] 17%|█▋        | 208/1208 [55:37<4:33:46, 16.43s/it] 17%|█▋        | 209/1208 [55:52<4:26:43, 16.02s/it] 17%|█▋        | 210/1208 [56:07<4:22:20, 15.77s/it]                                                    {'loss': 2.1315, 'learning_rate': 9.990539370375978e-06, 'epoch': 0.35} 17%|█▋        | 210/1208 [56:07<4:22:20, 15.77s/it] 17%|█▋        | 211/1208 [56:22<4:18:53, 15.58s/it] 18%|█▊        | 212/1208 [56:37<4:16:01, 15.42s/it] 18%|█▊        | 213/1208 [56:53<4:15:45, 15.42s/it] 18%|█▊        | 214/1208 [57:08<4:15:35, 15.43s/it] 18%|█▊        | 215/1208 [57:23<4:14:11, 15.36s/it] 18%|█▊        | 216/1208 [57:38<4:12:54, 15.30s/it] 18%|█▊        | 217/1208 [57:53<4:10:52, 15.19s/it] 18%|█▊        | 218/1208 [58:09<4:11:01, 15.21s/it] 18%|█▊        | 219/1208 [58:24<4:10:18, 15.19s/it] 18%|█▊        | 220/1208 [58:39<4:10:04, 15.19s/it]                                                    {'loss': 2.1244, 'learning_rate': 1e-05, 'epoch': 0.36} 18%|█▊        | 220/1208 [58:39<4:10:04, 15.19s/it] 18%|█▊        | 221/1208 [58:54<4:10:01, 15.20s/it] 18%|█▊        | 222/1208 [59:09<4:10:32, 15.25s/it] 18%|█▊        | 223/1208 [59:25<4:11:00, 15.29s/it] 19%|█▊        | 224/1208 [59:40<4:09:20, 15.20s/it] 19%|█▊        | 225/1208 [59:55<4:10:42, 15.30s/it] 19%|█▊        | 226/1208 [1:00:11<4:12:55, 15.45s/it] 19%|█▉        | 227/1208 [1:00:27<4:12:00, 15.41s/it] 19%|█▉        | 228/1208 [1:00:42<4:11:20, 15.39s/it] 19%|█▉        | 229/1208 [1:00:57<4:12:07, 15.45s/it] 19%|█▉        | 230/1208 [1:01:13<4:10:48, 15.39s/it]                                                      {'loss': 2.1112, 'learning_rate': 1e-05, 'epoch': 0.38} 19%|█▉        | 230/1208 [1:01:13<4:10:48, 15.39s/it] 19%|█▉        | 231/1208 [1:01:28<4:10:12, 15.37s/it] 19%|█▉        | 232/1208 [1:01:43<4:09:43, 15.35s/it] 19%|█▉        | 233/1208 [1:01:58<4:08:30, 15.29s/it] 19%|█▉        | 234/1208 [1:02:14<4:08:57, 15.34s/it] 19%|█▉        | 235/1208 [1:02:30<4:10:40, 15.46s/it] 20%|█▉        | 236/1208 [1:02:45<4:12:01, 15.56s/it] 20%|█▉        | 237/1208 [1:03:01<4:12:13, 15.59s/it] 20%|█▉        | 238/1208 [1:03:17<4:11:03, 15.53s/it] 20%|█▉        | 239/1208 [1:03:32<4:09:05, 15.42s/it] 20%|█▉        | 240/1208 [1:03:47<4:07:59, 15.37s/it]                                                      {'loss': 2.1119, 'learning_rate': 1e-05, 'epoch': 0.4} 20%|█▉        | 240/1208 [1:03:47<4:07:59, 15.37s/it] 20%|█▉        | 241/1208 [1:04:02<4:07:33, 15.36s/it] 20%|██        | 242/1208 [1:04:17<4:06:17, 15.30s/it] 20%|██        | 243/1208 [1:04:33<4:06:04, 15.30s/it] 20%|██        | 244/1208 [1:04:48<4:05:33, 15.28s/it] 20%|██        | 245/1208 [1:05:03<4:05:37, 15.30s/it] 20%|██        | 246/1208 [1:05:19<4:06:45, 15.39s/it] 20%|██        | 247/1208 [1:05:34<4:05:57, 15.36s/it] 21%|██        | 248/1208 [1:05:50<4:06:29, 15.41s/it] 21%|██        | 249/1208 [1:06:05<4:05:10, 15.34s/it] 21%|██        | 250/1208 [1:06:20<4:06:00, 15.41s/it]                                                      {'loss': 2.0955, 'learning_rate': 1e-05, 'epoch': 0.41} 21%|██        | 250/1208 [1:06:20<4:06:00, 15.41s/it] 21%|██        | 251/1208 [1:06:36<4:06:06, 15.43s/it] 21%|██        | 252/1208 [1:06:51<4:05:33, 15.41s/it] 21%|██        | 253/1208 [1:07:07<4:06:49, 15.51s/it] 21%|██        | 254/1208 [1:07:23<4:06:20, 15.49s/it] 21%|██        | 255/1208 [1:07:38<4:04:44, 15.41s/it] 21%|██        | 256/1208 [1:07:53<4:05:19, 15.46s/it] 21%|██▏       | 257/1208 [1:08:09<4:04:38, 15.43s/it] 21%|██▏       | 258/1208 [1:08:24<4:04:09, 15.42s/it] 21%|██▏       | 259/1208 [1:08:40<4:04:25, 15.45s/it] 22%|██▏       | 260/1208 [1:08:55<4:03:25, 15.41s/it]                                                      {'loss': 2.098, 'learning_rate': 1e-05, 'epoch': 0.43} 22%|██▏       | 260/1208 [1:08:55<4:03:25, 15.41s/it] 22%|██▏       | 261/1208 [1:09:11<4:04:12, 15.47s/it] 22%|██▏       | 262/1208 [1:09:26<4:03:32, 15.45s/it] 22%|██▏       | 263/1208 [1:09:41<4:02:01, 15.37s/it] 22%|██▏       | 264/1208 [1:09:56<4:00:55, 15.31s/it] 22%|██▏       | 265/1208 [1:10:12<4:00:59, 15.33s/it] 22%|██▏       | 266/1208 [1:10:27<4:00:25, 15.31s/it] 22%|██▏       | 267/1208 [1:10:42<4:01:11, 15.38s/it] 22%|██▏       | 268/1208 [1:10:58<3:59:25, 15.28s/it] 22%|██▏       | 269/1208 [1:11:13<4:00:00, 15.34s/it] 22%|██▏       | 270/1208 [1:11:29<4:01:42, 15.46s/it]                                                      {'loss': 2.1013, 'learning_rate': 1e-05, 'epoch': 0.45} 22%|██▏       | 270/1208 [1:11:29<4:01:42, 15.46s/it] 22%|██▏       | 271/1208 [1:11:44<4:00:30, 15.40s/it] 23%|██▎       | 272/1208 [1:11:59<3:59:43, 15.37s/it] 23%|██▎       | 273/1208 [1:12:15<3:58:57, 15.33s/it] 23%|██▎       | 274/1208 [1:12:30<3:58:14, 15.30s/it] 23%|██▎       | 275/1208 [1:12:45<3:57:10, 15.25s/it] 23%|██▎       | 276/1208 [1:13:00<3:57:06, 15.26s/it] 23%|██▎       | 277/1208 [1:13:16<3:57:22, 15.30s/it] 23%|██▎       | 278/1208 [1:13:31<3:56:43, 15.27s/it] 23%|██▎       | 279/1208 [1:13:46<3:57:21, 15.33s/it] 23%|██▎       | 280/1208 [1:14:01<3:56:47, 15.31s/it]                                                      {'loss': 2.1011, 'learning_rate': 1e-05, 'epoch': 0.46} 23%|██▎       | 280/1208 [1:14:02<3:56:47, 15.31s/it] 23%|██▎       | 281/1208 [1:14:17<3:56:40, 15.32s/it] 23%|██▎       | 282/1208 [1:14:32<3:56:19, 15.31s/it] 23%|██▎       | 283/1208 [1:14:48<3:56:55, 15.37s/it] 24%|██▎       | 284/1208 [1:15:03<3:57:40, 15.43s/it] 24%|██▎       | 285/1208 [1:15:19<3:57:23, 15.43s/it] 24%|██▎       | 286/1208 [1:15:34<3:56:47, 15.41s/it] 24%|██▍       | 287/1208 [1:15:50<3:57:22, 15.46s/it] 24%|██▍       | 288/1208 [1:16:05<3:57:27, 15.49s/it] 24%|██▍       | 289/1208 [1:16:20<3:56:07, 15.42s/it] 24%|██▍       | 290/1208 [1:16:36<3:55:15, 15.38s/it]                                                      {'loss': 2.0892, 'learning_rate': 1e-05, 'epoch': 0.48} 24%|██▍       | 290/1208 [1:16:36<3:55:15, 15.38s/it] 24%|██▍       | 291/1208 [1:16:51<3:54:59, 15.38s/it] 24%|██▍       | 292/1208 [1:17:07<3:55:10, 15.40s/it] 24%|██▍       | 293/1208 [1:17:22<3:55:33, 15.45s/it] 24%|██▍       | 294/1208 [1:17:37<3:55:11, 15.44s/it] 24%|██▍       | 295/1208 [1:17:53<3:53:30, 15.35s/it] 25%|██▍       | 296/1208 [1:18:08<3:52:25, 15.29s/it] 25%|██▍       | 297/1208 [1:18:23<3:51:37, 15.26s/it] 25%|██▍       | 298/1208 [1:18:38<3:51:08, 15.24s/it] 25%|██▍       | 299/1208 [1:18:53<3:50:52, 15.24s/it] 25%|██▍       | 300/1208 [1:19:09<3:51:20, 15.29s/it]                                                      {'loss': 2.0928, 'learning_rate': 1e-05, 'epoch': 0.5} 25%|██▍       | 300/1208 [1:19:09<3:51:20, 15.29s/it] 25%|██▍       | 301/1208 [1:20:13<7:30:46, 29.82s/it] 25%|██▌       | 302/1208 [1:20:28<6:23:36, 25.40s/it] 25%|██▌       | 303/1208 [1:20:43<5:36:24, 22.30s/it] 25%|██▌       | 304/1208 [1:20:58<5:02:42, 20.09s/it] 25%|██▌       | 305/1208 [1:21:13<4:39:10, 18.55s/it] 25%|██▌       | 306/1208 [1:21:28<4:22:42, 17.48s/it] 25%|██▌       | 307/1208 [1:21:43<4:11:24, 16.74s/it] 25%|██▌       | 308/1208 [1:21:58<4:04:00, 16.27s/it] 26%|██▌       | 309/1208 [1:22:13<3:57:35, 15.86s/it] 26%|██▌       | 310/1208 [1:22:28<3:53:26, 15.60s/it]                                                      {'loss': 2.1067, 'learning_rate': 1e-05, 'epoch': 0.51} 26%|██▌       | 310/1208 [1:22:28<3:53:26, 15.60s/it] 26%|██▌       | 311/1208 [1:22:43<3:50:17, 15.40s/it] 26%|██▌       | 312/1208 [1:22:58<3:48:21, 15.29s/it] 26%|██▌       | 313/1208 [1:23:13<3:47:16, 15.24s/it] 26%|██▌       | 314/1208 [1:23:28<3:46:18, 15.19s/it] 26%|██▌       | 315/1208 [1:23:43<3:45:07, 15.13s/it] 26%|██▌       | 316/1208 [1:23:58<3:44:31, 15.10s/it] 26%|██▌       | 317/1208 [1:24:13<3:43:37, 15.06s/it] 26%|██▋       | 318/1208 [1:24:28<3:43:36, 15.07s/it] 26%|██▋       | 319/1208 [1:24:43<3:43:14, 15.07s/it] 26%|██▋       | 320/1208 [1:24:58<3:43:54, 15.13s/it]                                                      {'loss': 2.0866, 'learning_rate': 1e-05, 'epoch': 0.53} 26%|██▋       | 320/1208 [1:24:58<3:43:54, 15.13s/it] 27%|██▋       | 321/1208 [1:25:13<3:44:18, 15.17s/it] 27%|██▋       | 322/1208 [1:25:29<3:44:11, 15.18s/it] 27%|██▋       | 323/1208 [1:25:44<3:43:27, 15.15s/it] 27%|██▋       | 324/1208 [1:25:59<3:43:02, 15.14s/it] 27%|██▋       | 325/1208 [1:26:14<3:44:04, 15.23s/it] 27%|██▋       | 326/1208 [1:26:30<3:44:09, 15.25s/it] 27%|██▋       | 327/1208 [1:26:45<3:43:30, 15.22s/it] 27%|██▋       | 328/1208 [1:27:00<3:42:40, 15.18s/it] 27%|██▋       | 329/1208 [1:27:15<3:41:45, 15.14s/it] 27%|██▋       | 330/1208 [1:27:30<3:41:39, 15.15s/it]                                                      {'loss': 2.0996, 'learning_rate': 1e-05, 'epoch': 0.55} 27%|██▋       | 330/1208 [1:27:30<3:41:39, 15.15s/it] 27%|██▋       | 331/1208 [1:27:45<3:41:05, 15.13s/it] 27%|██▋       | 332/1208 [1:28:00<3:40:32, 15.11s/it] 28%|██▊       | 333/1208 [1:28:16<3:41:15, 15.17s/it] 28%|██▊       | 334/1208 [1:28:31<3:41:55, 15.24s/it] 28%|██▊       | 335/1208 [1:28:46<3:42:49, 15.31s/it] 28%|██▊       | 336/1208 [1:29:02<3:42:12, 15.29s/it] 28%|██▊       | 337/1208 [1:29:17<3:42:39, 15.34s/it] 28%|██▊       | 338/1208 [1:29:33<3:42:52, 15.37s/it] 28%|██▊       | 339/1208 [1:29:48<3:41:49, 15.32s/it] 28%|██▊       | 340/1208 [1:30:03<3:41:17, 15.30s/it]                                                      {'loss': 2.1014, 'learning_rate': 1e-05, 'epoch': 0.56} 28%|██▊       | 340/1208 [1:30:03<3:41:17, 15.30s/it] 28%|██▊       | 341/1208 [1:30:18<3:41:17, 15.31s/it] 28%|██▊       | 342/1208 [1:30:34<3:40:56, 15.31s/it] 28%|██▊       | 343/1208 [1:30:49<3:40:41, 15.31s/it] 28%|██▊       | 344/1208 [1:31:04<3:41:18, 15.37s/it] 29%|██▊       | 345/1208 [1:31:20<3:41:16, 15.38s/it] 29%|██▊       | 346/1208 [1:31:35<3:41:10, 15.39s/it] 29%|██▊       | 347/1208 [1:31:51<3:41:45, 15.45s/it] 29%|██▉       | 348/1208 [1:32:06<3:40:48, 15.41s/it] 29%|██▉       | 349/1208 [1:32:22<3:41:22, 15.46s/it] 29%|██▉       | 350/1208 [1:32:37<3:40:24, 15.41s/it]                                                      {'loss': 2.1144, 'learning_rate': 1e-05, 'epoch': 0.58} 29%|██▉       | 350/1208 [1:32:37<3:40:24, 15.41s/it] 29%|██▉       | 351/1208 [1:32:53<3:40:42, 15.45s/it] 29%|██▉       | 352/1208 [1:33:08<3:42:14, 15.58s/it] 29%|██▉       | 353/1208 [1:33:24<3:41:51, 15.57s/it] 29%|██▉       | 354/1208 [1:33:39<3:41:01, 15.53s/it] 29%|██▉       | 355/1208 [1:33:55<3:42:09, 15.63s/it] 29%|██▉       | 356/1208 [1:34:11<3:42:26, 15.66s/it] 30%|██▉       | 357/1208 [1:34:27<3:41:47, 15.64s/it] 30%|██▉       | 358/1208 [1:34:42<3:41:35, 15.64s/it] 30%|██▉       | 359/1208 [1:34:58<3:42:47, 15.75s/it] 30%|██▉       | 360/1208 [1:35:14<3:43:33, 15.82s/it]                                                      {'loss': 2.1131, 'learning_rate': 1e-05, 'epoch': 0.6} 30%|██▉       | 360/1208 [1:35:14<3:43:33, 15.82s/it] 30%|██▉       | 361/1208 [1:35:30<3:42:02, 15.73s/it] 30%|██▉       | 362/1208 [1:35:45<3:40:44, 15.66s/it] 30%|███       | 363/1208 [1:36:01<3:41:01, 15.69s/it] 30%|███       | 364/1208 [1:36:17<3:40:39, 15.69s/it] 30%|███       | 365/1208 [1:36:32<3:39:19, 15.61s/it] 30%|███       | 366/1208 [1:36:48<3:41:34, 15.79s/it] 30%|███       | 367/1208 [1:37:04<3:42:09, 15.85s/it] 30%|███       | 368/1208 [1:37:20<3:40:11, 15.73s/it] 31%|███       | 369/1208 [1:37:35<3:38:43, 15.64s/it] 31%|███       | 370/1208 [1:37:51<3:38:23, 15.64s/it]                                                      {'loss': 2.0926, 'learning_rate': 1e-05, 'epoch': 0.61} 31%|███       | 370/1208 [1:37:51<3:38:23, 15.64s/it] 31%|███       | 371/1208 [1:38:07<3:38:45, 15.68s/it] 31%|███       | 372/1208 [1:38:22<3:39:11, 15.73s/it] 31%|███       | 373/1208 [1:38:38<3:37:23, 15.62s/it] 31%|███       | 374/1208 [1:38:53<3:35:55, 15.53s/it] 31%|███       | 375/1208 [1:39:09<3:37:33, 15.67s/it] 31%|███       | 376/1208 [1:39:26<3:40:26, 15.90s/it] 31%|███       | 377/1208 [1:39:41<3:39:23, 15.84s/it] 31%|███▏      | 378/1208 [1:39:57<3:37:38, 15.73s/it] 31%|███▏      | 379/1208 [1:40:13<3:37:31, 15.74s/it] 31%|███▏      | 380/1208 [1:40:29<3:38:07, 15.81s/it]                                                      {'loss': 2.0825, 'learning_rate': 1e-05, 'epoch': 0.63} 31%|███▏      | 380/1208 [1:40:29<3:38:07, 15.81s/it] 32%|███▏      | 381/1208 [1:40:45<3:38:45, 15.87s/it] 32%|███▏      | 382/1208 [1:41:00<3:37:56, 15.83s/it] 32%|███▏      | 383/1208 [1:41:16<3:36:46, 15.77s/it] 32%|███▏      | 384/1208 [1:41:31<3:35:51, 15.72s/it] 32%|███▏      | 385/1208 [1:41:47<3:34:16, 15.62s/it] 32%|███▏      | 386/1208 [1:42:02<3:33:07, 15.56s/it] 32%|███▏      | 387/1208 [1:42:17<3:31:00, 15.42s/it] 32%|███▏      | 388/1208 [1:42:33<3:30:38, 15.41s/it] 32%|███▏      | 389/1208 [1:42:48<3:31:21, 15.48s/it] 32%|███▏      | 390/1208 [1:43:04<3:31:31, 15.52s/it]                                                      {'loss': 2.0903, 'learning_rate': 1e-05, 'epoch': 0.65} 32%|███▏      | 390/1208 [1:43:04<3:31:31, 15.52s/it] 32%|███▏      | 391/1208 [1:43:20<3:32:20, 15.59s/it] 32%|███▏      | 392/1208 [1:43:35<3:31:34, 15.56s/it] 33%|███▎      | 393/1208 [1:43:50<3:29:32, 15.43s/it] 33%|███▎      | 394/1208 [1:44:06<3:28:42, 15.38s/it] 33%|███▎      | 395/1208 [1:44:21<3:28:09, 15.36s/it] 33%|███▎      | 396/1208 [1:44:36<3:27:57, 15.37s/it] 33%|███▎      | 397/1208 [1:44:52<3:27:32, 15.35s/it] 33%|███▎      | 398/1208 [1:45:07<3:26:52, 15.32s/it] 33%|███▎      | 399/1208 [1:45:22<3:26:36, 15.32s/it] 33%|███▎      | 400/1208 [1:45:38<3:26:47, 15.36s/it]                                                      {'loss': 2.09, 'learning_rate': 1e-05, 'epoch': 0.66} 33%|███▎      | 400/1208 [1:45:38<3:26:47, 15.36s/it] 33%|███▎      | 401/1208 [1:46:42<6:44:30, 30.08s/it] 33%|███▎      | 402/1208 [1:46:57<5:44:36, 25.65s/it] 33%|███▎      | 403/1208 [1:47:12<5:01:22, 22.46s/it] 33%|███▎      | 404/1208 [1:47:28<4:31:29, 20.26s/it] 34%|███▎      | 405/1208 [1:47:43<4:10:44, 18.74s/it] 34%|███▎      | 406/1208 [1:47:58<3:55:20, 17.61s/it] 34%|███▎      | 407/1208 [1:48:13<3:44:47, 16.84s/it] 34%|███▍      | 408/1208 [1:48:28<3:37:26, 16.31s/it] 34%|███▍      | 409/1208 [1:48:43<3:32:03, 15.92s/it] 34%|███▍      | 410/1208 [1:48:58<3:28:14, 15.66s/it]                                                      {'loss': 2.0763, 'learning_rate': 1e-05, 'epoch': 0.68} 34%|███▍      | 410/1208 [1:48:58<3:28:14, 15.66s/it] 34%|███▍      | 411/1208 [1:49:13<3:25:22, 15.46s/it] 34%|███▍      | 412/1208 [1:49:28<3:24:32, 15.42s/it] 34%|███▍      | 413/1208 [1:49:43<3:22:59, 15.32s/it] 34%|███▍      | 414/1208 [1:49:59<3:22:07, 15.27s/it] 34%|███▍      | 415/1208 [1:50:13<3:20:39, 15.18s/it] 34%|███▍      | 416/1208 [1:50:29<3:20:06, 15.16s/it] 35%|███▍      | 417/1208 [1:50:44<3:19:41, 15.15s/it] 35%|███▍      | 418/1208 [1:50:59<3:19:27, 15.15s/it] 35%|███▍      | 419/1208 [1:51:14<3:19:32, 15.17s/it] 35%|███▍      | 420/1208 [1:51:29<3:18:44, 15.13s/it]                                                      {'loss': 2.0819, 'learning_rate': 1e-05, 'epoch': 0.7} 35%|███▍      | 420/1208 [1:51:29<3:18:44, 15.13s/it] 35%|███▍      | 421/1208 [1:51:44<3:17:10, 15.03s/it] 35%|███▍      | 422/1208 [1:51:59<3:17:08, 15.05s/it] 35%|███▌      | 423/1208 [1:52:14<3:18:19, 15.16s/it] 35%|███▌      | 424/1208 [1:52:29<3:17:23, 15.11s/it] 35%|███▌      | 425/1208 [1:52:45<3:17:36, 15.14s/it] 35%|███▌      | 426/1208 [1:53:00<3:17:00, 15.12s/it] 35%|███▌      | 427/1208 [1:53:15<3:17:04, 15.14s/it] 35%|███▌      | 428/1208 [1:53:30<3:17:31, 15.19s/it] 36%|███▌      | 429/1208 [1:53:45<3:17:31, 15.21s/it] 36%|███▌      | 430/1208 [1:54:01<3:17:12, 15.21s/it]                                                      {'loss': 2.0893, 'learning_rate': 1e-05, 'epoch': 0.71} 36%|███▌      | 430/1208 [1:54:01<3:17:12, 15.21s/it] 36%|███▌      | 431/1208 [1:54:16<3:17:15, 15.23s/it] 36%|███▌      | 432/1208 [1:54:31<3:15:59, 15.15s/it] 36%|███▌      | 433/1208 [1:54:46<3:16:07, 15.18s/it] 36%|███▌      | 434/1208 [1:55:02<3:16:35, 15.24s/it] 36%|███▌      | 435/1208 [1:55:16<3:15:01, 15.14s/it] 36%|███▌      | 436/1208 [1:55:32<3:15:39, 15.21s/it] 36%|███▌      | 437/1208 [1:55:47<3:15:13, 15.19s/it] 36%|███▋      | 438/1208 [1:56:02<3:15:33, 15.24s/it] 36%|███▋      | 439/1208 [1:56:18<3:15:38, 15.26s/it] 36%|███▋      | 440/1208 [1:56:33<3:15:20, 15.26s/it]                                                      {'loss': 2.0796, 'learning_rate': 1e-05, 'epoch': 0.73} 36%|███▋      | 440/1208 [1:56:33<3:15:20, 15.26s/it] 37%|███▋      | 441/1208 [1:56:48<3:15:20, 15.28s/it] 37%|███▋      | 442/1208 [1:57:04<3:15:08, 15.29s/it] 37%|███▋      | 443/1208 [1:57:19<3:15:04, 15.30s/it] 37%|███▋      | 444/1208 [1:57:34<3:14:16, 15.26s/it] 37%|███▋      | 445/1208 [1:57:49<3:14:02, 15.26s/it] 37%|███▋      | 446/1208 [1:58:05<3:14:09, 15.29s/it] 37%|███▋      | 447/1208 [1:58:20<3:14:39, 15.35s/it] 37%|███▋      | 448/1208 [1:58:36<3:14:32, 15.36s/it] 37%|███▋      | 449/1208 [1:58:51<3:13:39, 15.31s/it] 37%|███▋      | 450/1208 [1:59:06<3:13:36, 15.33s/it]                                                      {'loss': 2.0858, 'learning_rate': 1e-05, 'epoch': 0.75} 37%|███▋      | 450/1208 [1:59:06<3:13:36, 15.33s/it] 37%|███▋      | 451/1208 [1:59:22<3:13:53, 15.37s/it] 37%|███▋      | 452/1208 [1:59:37<3:13:21, 15.35s/it] 38%|███▊      | 453/1208 [1:59:52<3:12:20, 15.29s/it] 38%|███▊      | 454/1208 [2:00:07<3:11:16, 15.22s/it] 38%|███▊      | 455/1208 [2:00:22<3:10:41, 15.20s/it] 38%|███▊      | 456/1208 [2:00:37<3:10:14, 15.18s/it] 38%|███▊      | 457/1208 [2:00:53<3:10:20, 15.21s/it] 38%|███▊      | 458/1208 [2:01:08<3:11:10, 15.29s/it] 38%|███▊      | 459/1208 [2:01:24<3:11:47, 15.36s/it] 38%|███▊      | 460/1208 [2:01:39<3:11:29, 15.36s/it]                                                      {'loss': 2.0759, 'learning_rate': 1e-05, 'epoch': 0.76} 38%|███▊      | 460/1208 [2:01:39<3:11:29, 15.36s/it] 38%|███▊      | 461/1208 [2:01:54<3:10:58, 15.34s/it] 38%|███▊      | 462/1208 [2:02:10<3:11:21, 15.39s/it] 38%|███▊      | 463/1208 [2:02:25<3:11:11, 15.40s/it] 38%|███▊      | 464/1208 [2:02:41<3:11:22, 15.43s/it] 38%|███▊      | 465/1208 [2:02:56<3:10:55, 15.42s/it] 39%|███▊      | 466/1208 [2:03:11<3:10:23, 15.40s/it] 39%|███▊      | 467/1208 [2:03:27<3:09:17, 15.33s/it] 39%|███▊      | 468/1208 [2:03:42<3:07:53, 15.23s/it] 39%|███▉      | 469/1208 [2:03:57<3:07:35, 15.23s/it] 39%|███▉      | 470/1208 [2:04:12<3:08:20, 15.31s/it]                                                      {'loss': 2.0852, 'learning_rate': 1e-05, 'epoch': 0.78} 39%|███▉      | 470/1208 [2:04:12<3:08:20, 15.31s/it] 39%|███▉      | 471/1208 [2:04:28<3:08:10, 15.32s/it] 39%|███▉      | 472/1208 [2:04:43<3:07:57, 15.32s/it] 39%|███▉      | 473/1208 [2:04:59<3:08:38, 15.40s/it] 39%|███▉      | 474/1208 [2:05:14<3:07:46, 15.35s/it] 39%|███▉      | 475/1208 [2:05:29<3:06:55, 15.30s/it] 39%|███▉      | 476/1208 [2:05:44<3:06:47, 15.31s/it] 39%|███▉      | 477/1208 [2:06:00<3:06:25, 15.30s/it] 40%|███▉      | 478/1208 [2:06:15<3:05:23, 15.24s/it] 40%|███▉      | 479/1208 [2:06:30<3:05:41, 15.28s/it] 40%|███▉      | 480/1208 [2:06:45<3:05:13, 15.27s/it]                                                      {'loss': 2.0753, 'learning_rate': 1e-05, 'epoch': 0.79} 40%|███▉      | 480/1208 [2:06:45<3:05:13, 15.27s/it] 40%|███▉      | 481/1208 [2:07:01<3:05:17, 15.29s/it] 40%|███▉      | 482/1208 [2:07:16<3:05:09, 15.30s/it] 40%|███▉      | 483/1208 [2:07:32<3:05:51, 15.38s/it] 40%|████      | 484/1208 [2:07:47<3:05:15, 15.35s/it] 40%|████      | 485/1208 [2:08:02<3:04:34, 15.32s/it] 40%|████      | 486/1208 [2:08:17<3:04:34, 15.34s/it] 40%|████      | 487/1208 [2:08:33<3:03:33, 15.28s/it] 40%|████      | 488/1208 [2:08:48<3:03:22, 15.28s/it] 40%|████      | 489/1208 [2:09:03<3:03:57, 15.35s/it] 41%|████      | 490/1208 [2:09:19<3:03:08, 15.30s/it]                                                      {'loss': 2.0753, 'learning_rate': 1e-05, 'epoch': 0.81} 41%|████      | 490/1208 [2:09:19<3:03:08, 15.30s/it] 41%|████      | 491/1208 [2:09:34<3:02:52, 15.30s/it] 41%|████      | 492/1208 [2:09:49<3:03:00, 15.34s/it] 41%|████      | 493/1208 [2:10:04<3:01:47, 15.26s/it] 41%|████      | 494/1208 [2:10:20<3:02:05, 15.30s/it] 41%|████      | 495/1208 [2:10:35<3:01:29, 15.27s/it] 41%|████      | 496/1208 [2:10:51<3:02:08, 15.35s/it] 41%|████      | 497/1208 [2:11:06<3:02:56, 15.44s/it] 41%|████      | 498/1208 [2:11:22<3:03:19, 15.49s/it] 41%|████▏     | 499/1208 [2:11:37<3:02:15, 15.42s/it] 41%|████▏     | 500/1208 [2:11:53<3:02:24, 15.46s/it]                                                      {'loss': 2.0866, 'learning_rate': 1e-05, 'epoch': 0.83} 41%|████▏     | 500/1208 [2:11:53<3:02:24, 15.46s/it] 41%|████▏     | 501/1208 [2:12:56<5:53:24, 29.99s/it] 42%|████▏     | 502/1208 [2:13:12<5:03:30, 25.79s/it] 42%|████▏     | 503/1208 [2:13:28<4:25:59, 22.64s/it] 42%|████▏     | 504/1208 [2:13:43<3:59:10, 20.38s/it] 42%|████▏     | 505/1208 [2:13:58<3:40:23, 18.81s/it] 42%|████▏     | 506/1208 [2:14:13<3:27:28, 17.73s/it] 42%|████▏     | 507/1208 [2:14:29<3:18:44, 17.01s/it] 42%|████▏     | 508/1208 [2:14:44<3:11:50, 16.44s/it] 42%|████▏     | 509/1208 [2:14:59<3:07:33, 16.10s/it] 42%|████▏     | 510/1208 [2:15:14<3:04:18, 15.84s/it]                                                      {'loss': 2.0794, 'learning_rate': 1e-05, 'epoch': 0.84} 42%|████▏     | 510/1208 [2:15:14<3:04:18, 15.84s/it] 42%|████▏     | 511/1208 [2:15:29<3:01:59, 15.67s/it] 42%|████▏     | 512/1208 [2:15:45<2:59:56, 15.51s/it] 42%|████▏     | 513/1208 [2:16:00<2:58:12, 15.39s/it] 43%|████▎     | 514/1208 [2:16:15<2:58:00, 15.39s/it] 43%|████▎     | 515/1208 [2:16:30<2:56:57, 15.32s/it] 43%|████▎     | 516/1208 [2:16:46<2:56:34, 15.31s/it] 43%|████▎     | 517/1208 [2:17:01<2:56:46, 15.35s/it] 43%|████▎     | 518/1208 [2:17:16<2:56:37, 15.36s/it] 43%|████▎     | 519/1208 [2:17:32<2:55:31, 15.29s/it] 43%|████▎     | 520/1208 [2:17:47<2:55:47, 15.33s/it]                                                      {'loss': 2.0547, 'learning_rate': 1e-05, 'epoch': 0.86} 43%|████▎     | 520/1208 [2:17:47<2:55:47, 15.33s/it] 43%|████▎     | 521/1208 [2:18:02<2:55:44, 15.35s/it] 43%|████▎     | 522/1208 [2:18:18<2:55:10, 15.32s/it] 43%|████▎     | 523/1208 [2:18:33<2:54:56, 15.32s/it] 43%|████▎     | 524/1208 [2:18:48<2:54:25, 15.30s/it] 43%|████▎     | 525/1208 [2:19:03<2:53:40, 15.26s/it] 44%|████▎     | 526/1208 [2:19:19<2:53:41, 15.28s/it] 44%|████▎     | 527/1208 [2:19:34<2:53:34, 15.29s/it] 44%|████▎     | 528/1208 [2:19:50<2:54:20, 15.38s/it] 44%|████▍     | 529/1208 [2:20:05<2:54:18, 15.40s/it] 44%|████▍     | 530/1208 [2:20:21<2:54:51, 15.47s/it]                                                      {'loss': 2.0844, 'learning_rate': 1e-05, 'epoch': 0.88} 44%|████▍     | 530/1208 [2:20:21<2:54:51, 15.47s/it] 44%|████▍     | 531/1208 [2:20:36<2:54:22, 15.45s/it] 44%|████▍     | 532/1208 [2:20:51<2:53:27, 15.40s/it] 44%|████▍     | 533/1208 [2:21:07<2:54:06, 15.48s/it] 44%|████▍     | 534/1208 [2:21:22<2:53:31, 15.45s/it] 44%|████▍     | 535/1208 [2:21:38<2:52:45, 15.40s/it] 44%|████▍     | 536/1208 [2:21:53<2:52:22, 15.39s/it] 44%|████▍     | 537/1208 [2:22:09<2:52:34, 15.43s/it] 45%|████▍     | 538/1208 [2:22:24<2:51:36, 15.37s/it] 45%|████▍     | 539/1208 [2:22:39<2:51:25, 15.37s/it] 45%|████▍     | 540/1208 [2:22:54<2:50:38, 15.33s/it]                                                      {'loss': 2.0544, 'learning_rate': 1e-05, 'epoch': 0.89} 45%|████▍     | 540/1208 [2:22:54<2:50:38, 15.33s/it] 45%|████▍     | 541/1208 [2:23:10<2:49:42, 15.27s/it] 45%|████▍     | 542/1208 [2:23:25<2:49:51, 15.30s/it] 45%|████▍     | 543/1208 [2:23:40<2:49:43, 15.31s/it] 45%|████▌     | 544/1208 [2:23:56<2:49:21, 15.30s/it] 45%|████▌     | 545/1208 [2:24:11<2:49:23, 15.33s/it] 45%|████▌     | 546/1208 [2:24:26<2:48:37, 15.28s/it] 45%|████▌     | 547/1208 [2:24:42<2:48:50, 15.33s/it] 45%|████▌     | 548/1208 [2:24:57<2:48:06, 15.28s/it] 45%|████▌     | 549/1208 [2:25:12<2:49:31, 15.44s/it] 46%|████▌     | 550/1208 [2:25:28<2:48:54, 15.40s/it]                                                      {'loss': 2.0513, 'learning_rate': 1e-05, 'epoch': 0.91} 46%|████▌     | 550/1208 [2:25:28<2:48:54, 15.40s/it] 46%|████▌     | 551/1208 [2:25:43<2:48:09, 15.36s/it] 46%|████▌     | 552/1208 [2:25:59<2:48:46, 15.44s/it] 46%|████▌     | 553/1208 [2:26:14<2:47:42, 15.36s/it] 46%|████▌     | 554/1208 [2:26:29<2:48:06, 15.42s/it] 46%|████▌     | 555/1208 [2:26:45<2:47:55, 15.43s/it] 46%|████▌     | 556/1208 [2:27:00<2:48:00, 15.46s/it] 46%|████▌     | 557/1208 [2:27:16<2:47:28, 15.44s/it] 46%|████▌     | 558/1208 [2:27:31<2:47:09, 15.43s/it] 46%|████▋     | 559/1208 [2:27:47<2:46:41, 15.41s/it] 46%|████▋     | 560/1208 [2:28:02<2:48:02, 15.56s/it]                                                      {'loss': 2.0439, 'learning_rate': 1e-05, 'epoch': 0.93} 46%|████▋     | 560/1208 [2:28:02<2:48:02, 15.56s/it] 46%|████▋     | 561/1208 [2:28:18<2:48:02, 15.58s/it] 47%|████▋     | 562/1208 [2:28:34<2:47:17, 15.54s/it] 47%|████▋     | 563/1208 [2:28:49<2:46:01, 15.44s/it] 47%|████▋     | 564/1208 [2:29:05<2:46:40, 15.53s/it] 47%|████▋     | 565/1208 [2:29:20<2:45:56, 15.48s/it] 47%|████▋     | 566/1208 [2:29:35<2:44:34, 15.38s/it] 47%|████▋     | 567/1208 [2:29:50<2:43:52, 15.34s/it] 47%|████▋     | 568/1208 [2:30:05<2:43:07, 15.29s/it] 47%|████▋     | 569/1208 [2:30:21<2:43:09, 15.32s/it] 47%|████▋     | 570/1208 [2:30:36<2:42:42, 15.30s/it]                                                      {'loss': 2.0559, 'learning_rate': 1e-05, 'epoch': 0.94} 47%|████▋     | 570/1208 [2:30:36<2:42:42, 15.30s/it] 47%|████▋     | 571/1208 [2:30:51<2:42:20, 15.29s/it] 47%|████▋     | 572/1208 [2:31:07<2:43:09, 15.39s/it] 47%|████▋     | 573/1208 [2:31:22<2:42:53, 15.39s/it] 48%|████▊     | 574/1208 [2:31:38<2:42:57, 15.42s/it] 48%|████▊     | 575/1208 [2:31:53<2:42:21, 15.39s/it] 48%|████▊     | 576/1208 [2:32:09<2:42:18, 15.41s/it] 48%|████▊     | 577/1208 [2:32:24<2:43:09, 15.52s/it] 48%|████▊     | 578/1208 [2:32:40<2:42:17, 15.46s/it] 48%|████▊     | 579/1208 [2:32:55<2:41:52, 15.44s/it] 48%|████▊     | 580/1208 [2:33:11<2:42:06, 15.49s/it]                                                      {'loss': 2.0845, 'learning_rate': 1e-05, 'epoch': 0.96} 48%|████▊     | 580/1208 [2:33:11<2:42:06, 15.49s/it] 48%|████▊     | 581/1208 [2:33:26<2:41:05, 15.42s/it] 48%|████▊     | 582/1208 [2:33:42<2:41:39, 15.49s/it] 48%|████▊     | 583/1208 [2:33:57<2:41:10, 15.47s/it] 48%|████▊     | 584/1208 [2:34:12<2:40:23, 15.42s/it] 48%|████▊     | 585/1208 [2:34:28<2:39:34, 15.37s/it] 49%|████▊     | 586/1208 [2:34:43<2:38:54, 15.33s/it] 49%|████▊     | 587/1208 [2:34:58<2:38:40, 15.33s/it] 49%|████▊     | 588/1208 [2:35:13<2:38:04, 15.30s/it] 49%|████▉     | 589/1208 [2:35:29<2:37:55, 15.31s/it] 49%|████▉     | 590/1208 [2:35:44<2:37:36, 15.30s/it]                                                      {'loss': 2.063, 'learning_rate': 1e-05, 'epoch': 0.98} 49%|████▉     | 590/1208 [2:35:44<2:37:36, 15.30s/it] 49%|████▉     | 591/1208 [2:35:59<2:36:27, 15.22s/it] 49%|████▉     | 592/1208 [2:36:14<2:36:29, 15.24s/it] 49%|████▉     | 593/1208 [2:36:30<2:36:41, 15.29s/it] 49%|████▉     | 594/1208 [2:36:45<2:36:23, 15.28s/it] 49%|████▉     | 595/1208 [2:37:00<2:36:09, 15.28s/it] 49%|████▉     | 596/1208 [2:37:16<2:36:23, 15.33s/it] 49%|████▉     | 597/1208 [2:37:31<2:36:55, 15.41s/it] 50%|████▉     | 598/1208 [2:37:47<2:37:05, 15.45s/it] 50%|████▉     | 599/1208 [2:38:03<2:37:35, 15.53s/it] 50%|████▉     | 600/1208 [2:38:18<2:37:19, 15.53s/it]                                                      {'loss': 2.0765, 'learning_rate': 1e-05, 'epoch': 0.99} 50%|████▉     | 600/1208 [2:38:18<2:37:19, 15.53s/it] 50%|████▉     | 601/1208 [2:39:23<5:06:41, 30.32s/it] 50%|████▉     | 602/1208 [2:39:38<4:20:24, 25.78s/it] 50%|████▉     | 603/1208 [2:39:53<3:47:49, 22.59s/it] 50%|█████     | 604/1208 [2:40:08<3:24:12, 20.29s/it] 50%|█████     | 605/1208 [2:40:37<3:50:46, 22.96s/it] 50%|█████     | 606/1208 [2:40:53<3:28:43, 20.80s/it] 50%|█████     | 607/1208 [2:41:09<3:12:21, 19.20s/it] 50%|█████     | 608/1208 [2:41:24<3:00:23, 18.04s/it] 50%|█████     | 609/1208 [2:41:39<2:51:22, 17.17s/it] 50%|█████     | 610/1208 [2:41:54<2:45:29, 16.60s/it]                                                      {'loss': 1.8894, 'learning_rate': 1e-05, 'epoch': 1.01} 50%|█████     | 610/1208 [2:41:54<2:45:29, 16.60s/it] 51%|█████     | 611/1208 [2:42:09<2:40:18, 16.11s/it] 51%|█████     | 612/1208 [2:42:24<2:37:01, 15.81s/it] 51%|█████     | 613/1208 [2:42:40<2:34:48, 15.61s/it] 51%|█████     | 614/1208 [2:42:55<2:32:47, 15.43s/it] 51%|█████     | 615/1208 [2:43:10<2:31:16, 15.31s/it] 51%|█████     | 616/1208 [2:43:25<2:30:23, 15.24s/it] 51%|█████     | 617/1208 [2:43:40<2:29:49, 15.21s/it] 51%|█████     | 618/1208 [2:43:55<2:29:43, 15.23s/it] 51%|█████     | 619/1208 [2:44:10<2:29:52, 15.27s/it] 51%|█████▏    | 620/1208 [2:44:26<2:29:39, 15.27s/it]                                                      {'loss': 1.745, 'learning_rate': 1e-05, 'epoch': 1.03} 51%|█████▏    | 620/1208 [2:44:26<2:29:39, 15.27s/it] 51%|█████▏    | 621/1208 [2:44:41<2:28:20, 15.16s/it] 51%|█████▏    | 622/1208 [2:44:56<2:28:55, 15.25s/it] 52%|█████▏    | 623/1208 [2:45:11<2:29:00, 15.28s/it] 52%|█████▏    | 624/1208 [2:45:27<2:28:20, 15.24s/it] 52%|█████▏    | 625/1208 [2:45:42<2:28:23, 15.27s/it] 52%|█████▏    | 626/1208 [2:45:57<2:27:58, 15.25s/it] 52%|█████▏    | 627/1208 [2:46:12<2:27:12, 15.20s/it] 52%|█████▏    | 628/1208 [2:46:28<2:27:07, 15.22s/it] 52%|█████▏    | 629/1208 [2:46:43<2:27:12, 15.25s/it] 52%|█████▏    | 630/1208 [2:46:58<2:26:49, 15.24s/it]                                                      {'loss': 1.7376, 'learning_rate': 1e-05, 'epoch': 1.04} 52%|█████▏    | 630/1208 [2:46:58<2:26:49, 15.24s/it] 52%|█████▏    | 631/1208 [2:47:13<2:26:35, 15.24s/it] 52%|█████▏    | 632/1208 [2:47:29<2:27:49, 15.40s/it] 52%|█████▏    | 633/1208 [2:47:45<2:28:04, 15.45s/it] 52%|█████▏    | 634/1208 [2:48:00<2:27:22, 15.41s/it] 53%|█████▎    | 635/1208 [2:48:15<2:27:11, 15.41s/it] 53%|█████▎    | 636/1208 [2:48:31<2:26:52, 15.41s/it] 53%|█████▎    | 637/1208 [2:48:46<2:26:44, 15.42s/it] 53%|█████▎    | 638/1208 [2:49:01<2:25:37, 15.33s/it] 53%|█████▎    | 639/1208 [2:49:17<2:25:01, 15.29s/it] 53%|█████▎    | 640/1208 [2:49:32<2:24:59, 15.32s/it]                                                      {'loss': 1.756, 'learning_rate': 1e-05, 'epoch': 1.06} 53%|█████▎    | 640/1208 [2:49:32<2:24:59, 15.32s/it] 53%|█████▎    | 641/1208 [2:49:47<2:24:59, 15.34s/it] 53%|█████▎    | 642/1208 [2:50:02<2:23:55, 15.26s/it] 53%|█████▎    | 643/1208 [2:50:18<2:23:41, 15.26s/it] 53%|█████▎    | 644/1208 [2:50:33<2:23:25, 15.26s/it] 53%|█████▎    | 645/1208 [2:50:48<2:23:10, 15.26s/it] 53%|█████▎    | 646/1208 [2:51:04<2:23:22, 15.31s/it] 54%|█████▎    | 647/1208 [2:51:19<2:22:31, 15.24s/it] 54%|█████▎    | 648/1208 [2:51:34<2:22:07, 15.23s/it] 54%|█████▎    | 649/1208 [2:51:49<2:22:30, 15.30s/it] 54%|█████▍    | 650/1208 [2:52:05<2:22:21, 15.31s/it]                                                      {'loss': 1.7617, 'learning_rate': 1e-05, 'epoch': 1.08} 54%|█████▍    | 650/1208 [2:52:05<2:22:21, 15.31s/it] 54%|█████▍    | 651/1208 [2:52:20<2:22:22, 15.34s/it] 54%|█████▍    | 652/1208 [2:52:36<2:22:47, 15.41s/it] 54%|█████▍    | 653/1208 [2:52:51<2:22:03, 15.36s/it] 54%|█████▍    | 654/1208 [2:53:06<2:22:00, 15.38s/it] 54%|█████▍    | 655/1208 [2:53:22<2:22:09, 15.42s/it] 54%|█████▍    | 656/1208 [2:53:37<2:22:30, 15.49s/it] 54%|█████▍    | 657/1208 [2:53:53<2:21:59, 15.46s/it] 54%|█████▍    | 658/1208 [2:54:08<2:21:11, 15.40s/it] 55%|█████▍    | 659/1208 [2:54:24<2:21:56, 15.51s/it] 55%|█████▍    | 660/1208 [2:54:39<2:21:35, 15.50s/it]                                                      {'loss': 1.7403, 'learning_rate': 1e-05, 'epoch': 1.09} 55%|█████▍    | 660/1208 [2:54:39<2:21:35, 15.50s/it] 55%|█████▍    | 661/1208 [2:54:55<2:20:20, 15.39s/it] 55%|█████▍    | 662/1208 [2:55:10<2:19:39, 15.35s/it] 55%|█████▍    | 663/1208 [2:55:25<2:19:38, 15.37s/it] 55%|█████▍    | 664/1208 [2:55:41<2:19:44, 15.41s/it] 55%|█████▌    | 665/1208 [2:55:56<2:18:55, 15.35s/it] 55%|█████▌    | 666/1208 [2:56:11<2:19:15, 15.42s/it] 55%|█████▌    | 667/1208 [2:56:27<2:19:14, 15.44s/it] 55%|█████▌    | 668/1208 [2:56:42<2:17:47, 15.31s/it] 55%|█████▌    | 669/1208 [2:56:57<2:17:30, 15.31s/it] 55%|█████▌    | 670/1208 [2:57:12<2:16:59, 15.28s/it]                                                      {'loss': 1.7566, 'learning_rate': 1e-05, 'epoch': 1.11} 55%|█████▌    | 670/1208 [2:57:13<2:16:59, 15.28s/it] 56%|█████▌    | 671/1208 [2:57:28<2:17:15, 15.34s/it] 56%|█████▌    | 672/1208 [2:57:43<2:16:39, 15.30s/it] 56%|█████▌    | 673/1208 [2:57:59<2:16:57, 15.36s/it] 56%|█████▌    | 674/1208 [2:58:14<2:16:41, 15.36s/it] 56%|█████▌    | 675/1208 [2:58:29<2:16:16, 15.34s/it] 56%|█████▌    | 676/1208 [2:58:45<2:15:58, 15.34s/it] 56%|█████▌    | 677/1208 [2:59:00<2:15:36, 15.32s/it] 56%|█████▌    | 678/1208 [2:59:15<2:15:47, 15.37s/it] 56%|█████▌    | 679/1208 [2:59:31<2:15:02, 15.32s/it] 56%|█████▋    | 680/1208 [2:59:46<2:14:08, 15.24s/it]                                                      {'loss': 1.7536, 'learning_rate': 1e-05, 'epoch': 1.13} 56%|█████▋    | 680/1208 [2:59:46<2:14:08, 15.24s/it] 56%|█████▋    | 681/1208 [3:00:01<2:14:31, 15.32s/it] 56%|█████▋    | 682/1208 [3:00:17<2:14:18, 15.32s/it] 57%|█████▋    | 683/1208 [3:00:32<2:13:58, 15.31s/it] 57%|█████▋    | 684/1208 [3:00:47<2:13:33, 15.29s/it] 57%|█████▋    | 685/1208 [3:01:03<2:15:01, 15.49s/it] 57%|█████▋    | 686/1208 [3:01:18<2:13:44, 15.37s/it] 57%|█████▋    | 687/1208 [3:01:33<2:13:17, 15.35s/it] 57%|█████▋    | 688/1208 [3:01:49<2:13:45, 15.43s/it] 57%|█████▋    | 689/1208 [3:02:04<2:13:24, 15.42s/it] 57%|█████▋    | 690/1208 [3:02:20<2:13:55, 15.51s/it]                                                      {'loss': 1.7451, 'learning_rate': 1e-05, 'epoch': 1.14} 57%|█████▋    | 690/1208 [3:02:20<2:13:55, 15.51s/it] 57%|█████▋    | 691/1208 [3:02:36<2:13:48, 15.53s/it] 57%|█████▋    | 692/1208 [3:02:51<2:14:03, 15.59s/it] 57%|█████▋    | 693/1208 [3:03:07<2:13:25, 15.54s/it] 57%|█████▋    | 694/1208 [3:03:22<2:12:41, 15.49s/it] 58%|█████▊    | 695/1208 [3:03:38<2:12:57, 15.55s/it] 58%|█████▊    | 696/1208 [3:03:53<2:12:12, 15.49s/it] 58%|█████▊    | 697/1208 [3:04:08<2:11:08, 15.40s/it] 58%|█████▊    | 698/1208 [3:04:24<2:10:21, 15.34s/it] 58%|█████▊    | 699/1208 [3:04:39<2:10:05, 15.34s/it] 58%|█████▊    | 700/1208 [3:04:54<2:08:46, 15.21s/it]                                                      {'loss': 1.7618, 'learning_rate': 1e-05, 'epoch': 1.16} 58%|█████▊    | 700/1208 [3:04:54<2:08:46, 15.21s/it] 58%|█████▊    | 701/1208 [3:05:57<4:09:06, 29.48s/it] 58%|█████▊    | 702/1208 [3:06:13<3:34:13, 25.40s/it] 58%|█████▊    | 703/1208 [3:06:28<3:09:18, 22.49s/it] 58%|█████▊    | 704/1208 [3:06:44<2:51:04, 20.37s/it] 58%|█████▊    | 705/1208 [3:06:59<2:38:17, 18.88s/it] 58%|█████▊    | 706/1208 [3:07:14<2:28:27, 17.74s/it] 59%|█████▊    | 707/1208 [3:07:29<2:21:47, 16.98s/it] 59%|█████▊    | 708/1208 [3:07:45<2:16:57, 16.44s/it] 59%|█████▊    | 709/1208 [3:08:00<2:14:26, 16.17s/it] 59%|█████▉    | 710/1208 [3:08:15<2:12:13, 15.93s/it]                                                      {'loss': 1.7575, 'learning_rate': 1e-05, 'epoch': 1.18} 59%|█████▉    | 710/1208 [3:08:15<2:12:13, 15.93s/it] 59%|█████▉    | 711/1208 [3:08:31<2:10:19, 15.73s/it] 59%|█████▉    | 712/1208 [3:08:46<2:08:36, 15.56s/it] 59%|█████▉    | 713/1208 [3:09:01<2:07:27, 15.45s/it] 59%|█████▉    | 714/1208 [3:09:16<2:06:43, 15.39s/it] 59%|█████▉    | 715/1208 [3:09:32<2:06:04, 15.34s/it] 59%|█████▉    | 716/1208 [3:09:47<2:05:37, 15.32s/it] 59%|█████▉    | 717/1208 [3:10:02<2:05:38, 15.35s/it] 59%|█████▉    | 718/1208 [3:10:18<2:05:57, 15.42s/it] 60%|█████▉    | 719/1208 [3:10:33<2:05:44, 15.43s/it] 60%|█████▉    | 720/1208 [3:10:49<2:05:35, 15.44s/it]                                                      {'loss': 1.7498, 'learning_rate': 1e-05, 'epoch': 1.19} 60%|█████▉    | 720/1208 [3:10:49<2:05:35, 15.44s/it] 60%|█████▉    | 721/1208 [3:11:05<2:06:45, 15.62s/it] 60%|█████▉    | 722/1208 [3:11:20<2:05:54, 15.54s/it] 60%|█████▉    | 723/1208 [3:11:36<2:06:00, 15.59s/it] 60%|█████▉    | 724/1208 [3:11:51<2:04:46, 15.47s/it] 60%|██████    | 725/1208 [3:12:06<2:04:09, 15.42s/it] 60%|██████    | 726/1208 [3:12:22<2:03:44, 15.40s/it] 60%|██████    | 727/1208 [3:12:37<2:02:48, 15.32s/it] 60%|██████    | 728/1208 [3:12:52<2:02:20, 15.29s/it] 60%|██████    | 729/1208 [3:13:08<2:02:47, 15.38s/it] 60%|██████    | 730/1208 [3:13:23<2:02:53, 15.42s/it]                                                      {'loss': 1.7715, 'learning_rate': 1e-05, 'epoch': 1.21} 60%|██████    | 730/1208 [3:13:23<2:02:53, 15.42s/it] 61%|██████    | 731/1208 [3:13:39<2:02:26, 15.40s/it] 61%|██████    | 732/1208 [3:13:54<2:03:25, 15.56s/it] 61%|██████    | 733/1208 [3:14:11<2:04:41, 15.75s/it] 61%|██████    | 734/1208 [3:14:26<2:03:51, 15.68s/it] 61%|██████    | 735/1208 [3:14:42<2:03:55, 15.72s/it] 61%|██████    | 736/1208 [3:14:58<2:03:39, 15.72s/it] 61%|██████    | 737/1208 [3:15:13<2:03:16, 15.70s/it] 61%|██████    | 738/1208 [3:15:29<2:03:04, 15.71s/it] 61%|██████    | 739/1208 [3:15:45<2:02:40, 15.69s/it] 61%|██████▏   | 740/1208 [3:16:01<2:02:44, 15.74s/it]                                                      {'loss': 1.7594, 'learning_rate': 1e-05, 'epoch': 1.23} 61%|██████▏   | 740/1208 [3:16:01<2:02:44, 15.74s/it] 61%|██████▏   | 741/1208 [3:16:16<2:02:04, 15.68s/it] 61%|██████▏   | 742/1208 [3:16:32<2:01:35, 15.66s/it] 62%|██████▏   | 743/1208 [3:16:47<2:01:01, 15.62s/it] 62%|██████▏   | 744/1208 [3:17:03<2:01:30, 15.71s/it] 62%|██████▏   | 745/1208 [3:17:19<2:00:46, 15.65s/it] 62%|██████▏   | 746/1208 [3:17:34<2:00:43, 15.68s/it] 62%|██████▏   | 747/1208 [3:17:50<2:00:19, 15.66s/it] 62%|██████▏   | 748/1208 [3:18:06<2:00:35, 15.73s/it] 62%|██████▏   | 749/1208 [3:18:21<1:59:46, 15.66s/it] 62%|██████▏   | 750/1208 [3:18:37<1:59:14, 15.62s/it]                                                      {'loss': 1.7734, 'learning_rate': 1e-05, 'epoch': 1.24} 62%|██████▏   | 750/1208 [3:18:37<1:59:14, 15.62s/it] 62%|██████▏   | 751/1208 [3:18:53<1:59:20, 15.67s/it] 62%|██████▏   | 752/1208 [3:19:08<1:58:52, 15.64s/it] 62%|██████▏   | 753/1208 [3:19:24<1:58:17, 15.60s/it] 62%|██████▏   | 754/1208 [3:19:39<1:57:37, 15.54s/it] 62%|██████▎   | 755/1208 [3:19:55<1:57:18, 15.54s/it] 63%|██████▎   | 756/1208 [3:20:10<1:57:22, 15.58s/it] 63%|██████▎   | 757/1208 [3:20:26<1:57:06, 15.58s/it] 63%|██████▎   | 758/1208 [3:20:42<1:56:47, 15.57s/it] 63%|██████▎   | 759/1208 [3:20:57<1:56:01, 15.50s/it] 63%|██████▎   | 760/1208 [3:21:12<1:55:45, 15.50s/it]                                                      {'loss': 1.7542, 'learning_rate': 1e-05, 'epoch': 1.26} 63%|██████▎   | 760/1208 [3:21:12<1:55:45, 15.50s/it] 63%|██████▎   | 761/1208 [3:21:28<1:55:30, 15.50s/it] 63%|██████▎   | 762/1208 [3:21:44<1:55:49, 15.58s/it] 63%|██████▎   | 763/1208 [3:21:59<1:55:30, 15.57s/it] 63%|██████▎   | 764/1208 [3:22:15<1:54:56, 15.53s/it] 63%|██████▎   | 765/1208 [3:22:30<1:54:21, 15.49s/it] 63%|██████▎   | 766/1208 [3:22:46<1:54:05, 15.49s/it] 63%|██████▎   | 767/1208 [3:23:01<1:53:43, 15.47s/it] 64%|██████▎   | 768/1208 [3:23:16<1:53:26, 15.47s/it] 64%|██████▎   | 769/1208 [3:23:32<1:52:48, 15.42s/it] 64%|██████▎   | 770/1208 [3:23:47<1:52:56, 15.47s/it]                                                      {'loss': 1.7435, 'learning_rate': 1e-05, 'epoch': 1.27} 64%|██████▎   | 770/1208 [3:23:47<1:52:56, 15.47s/it] 64%|██████▍   | 771/1208 [3:24:03<1:52:44, 15.48s/it] 64%|██████▍   | 772/1208 [3:24:18<1:52:34, 15.49s/it] 64%|██████▍   | 773/1208 [3:24:34<1:52:30, 15.52s/it] 64%|██████▍   | 774/1208 [3:24:50<1:52:54, 15.61s/it] 64%|██████▍   | 775/1208 [3:25:05<1:52:48, 15.63s/it] 64%|██████▍   | 776/1208 [3:25:21<1:53:09, 15.72s/it] 64%|██████▍   | 777/1208 [3:25:38<1:53:43, 15.83s/it] 64%|██████▍   | 778/1208 [3:25:53<1:53:11, 15.79s/it] 64%|██████▍   | 779/1208 [3:26:09<1:52:55, 15.79s/it] 65%|██████▍   | 780/1208 [3:26:25<1:52:13, 15.73s/it]                                                      {'loss': 1.7699, 'learning_rate': 1e-05, 'epoch': 1.29} 65%|██████▍   | 780/1208 [3:26:25<1:52:13, 15.73s/it] 65%|██████▍   | 781/1208 [3:26:40<1:51:51, 15.72s/it] 65%|██████▍   | 782/1208 [3:26:56<1:51:21, 15.68s/it] 65%|██████▍   | 783/1208 [3:27:11<1:50:38, 15.62s/it] 65%|██████▍   | 784/1208 [3:27:27<1:50:39, 15.66s/it] 65%|██████▍   | 785/1208 [3:27:43<1:50:27, 15.67s/it] 65%|██████▌   | 786/1208 [3:27:58<1:50:04, 15.65s/it] 65%|██████▌   | 787/1208 [3:28:14<1:49:03, 15.54s/it] 65%|██████▌   | 788/1208 [3:28:29<1:48:07, 15.45s/it] 65%|██████▌   | 789/1208 [3:28:44<1:47:46, 15.43s/it] 65%|██████▌   | 790/1208 [3:29:00<1:47:21, 15.41s/it]                                                      {'loss': 1.7653, 'learning_rate': 1e-05, 'epoch': 1.31} 65%|██████▌   | 790/1208 [3:29:00<1:47:21, 15.41s/it] 65%|██████▌   | 791/1208 [3:29:15<1:46:52, 15.38s/it] 66%|██████▌   | 792/1208 [3:29:31<1:47:21, 15.48s/it] 66%|██████▌   | 793/1208 [3:29:46<1:47:19, 15.52s/it] 66%|██████▌   | 794/1208 [3:30:02<1:47:24, 15.57s/it] 66%|██████▌   | 795/1208 [3:30:18<1:47:09, 15.57s/it] 66%|██████▌   | 796/1208 [3:30:33<1:46:36, 15.53s/it] 66%|██████▌   | 797/1208 [3:30:49<1:46:39, 15.57s/it] 66%|██████▌   | 798/1208 [3:31:04<1:46:26, 15.58s/it] 66%|██████▌   | 799/1208 [3:31:20<1:46:19, 15.60s/it] 66%|██████▌   | 800/1208 [3:31:35<1:45:56, 15.58s/it]                                                      {'loss': 1.7647, 'learning_rate': 1e-05, 'epoch': 1.32} 66%|██████▌   | 800/1208 [3:31:35<1:45:56, 15.58s/it] 66%|██████▋   | 801/1208 [3:32:38<3:21:49, 29.75s/it] 66%|██████▋   | 802/1208 [3:32:54<2:53:30, 25.64s/it] 66%|██████▋   | 803/1208 [3:33:10<2:32:06, 22.53s/it] 67%|██████▋   | 804/1208 [3:33:25<2:17:06, 20.36s/it] 67%|██████▋   | 805/1208 [3:33:40<2:06:12, 18.79s/it] 67%|██████▋   | 806/1208 [3:33:55<1:58:52, 17.74s/it] 67%|██████▋   | 807/1208 [3:34:11<1:53:38, 17.00s/it] 67%|██████▋   | 808/1208 [3:34:26<1:49:35, 16.44s/it] 67%|██████▋   | 809/1208 [3:34:41<1:46:37, 16.03s/it] 67%|██████▋   | 810/1208 [3:34:56<1:44:39, 15.78s/it]                                                      {'loss': 1.7544, 'learning_rate': 1e-05, 'epoch': 1.34} 67%|██████▋   | 810/1208 [3:34:56<1:44:39, 15.78s/it] 67%|██████▋   | 811/1208 [3:35:11<1:42:52, 15.55s/it] 67%|██████▋   | 812/1208 [3:35:26<1:41:44, 15.42s/it] 67%|██████▋   | 813/1208 [3:35:41<1:41:17, 15.39s/it] 67%|██████▋   | 814/1208 [3:35:57<1:40:37, 15.32s/it] 67%|██████▋   | 815/1208 [3:36:12<1:40:16, 15.31s/it] 68%|██████▊   | 816/1208 [3:36:27<1:40:05, 15.32s/it] 68%|██████▊   | 817/1208 [3:36:43<1:39:59, 15.34s/it] 68%|██████▊   | 818/1208 [3:36:58<1:39:49, 15.36s/it] 68%|██████▊   | 819/1208 [3:37:13<1:39:20, 15.32s/it] 68%|██████▊   | 820/1208 [3:37:28<1:38:49, 15.28s/it]                                                      {'loss': 1.7743, 'learning_rate': 1e-05, 'epoch': 1.36} 68%|██████▊   | 820/1208 [3:37:28<1:38:49, 15.28s/it] 68%|██████▊   | 821/1208 [3:37:44<1:38:20, 15.25s/it] 68%|██████▊   | 822/1208 [3:37:59<1:37:59, 15.23s/it] 68%|██████▊   | 823/1208 [3:38:14<1:37:55, 15.26s/it] 68%|██████▊   | 824/1208 [3:38:29<1:37:46, 15.28s/it] 68%|██████▊   | 825/1208 [3:38:45<1:37:38, 15.30s/it] 68%|██████▊   | 826/1208 [3:39:00<1:37:18, 15.28s/it] 68%|██████▊   | 827/1208 [3:39:15<1:37:14, 15.31s/it] 69%|██████▊   | 828/1208 [3:39:31<1:37:03, 15.33s/it] 69%|██████▊   | 829/1208 [3:39:46<1:37:19, 15.41s/it] 69%|██████▊   | 830/1208 [3:40:02<1:37:18, 15.45s/it]                                                      {'loss': 1.7482, 'learning_rate': 1e-05, 'epoch': 1.37} 69%|██████▊   | 830/1208 [3:40:02<1:37:18, 15.45s/it] 69%|██████▉   | 831/1208 [3:40:18<1:37:28, 15.51s/it] 69%|██████▉   | 832/1208 [3:40:33<1:37:19, 15.53s/it] 69%|██████▉   | 833/1208 [3:40:48<1:36:37, 15.46s/it] 69%|██████▉   | 834/1208 [3:41:04<1:37:05, 15.58s/it] 69%|██████▉   | 835/1208 [3:41:20<1:37:18, 15.65s/it] 69%|██████▉   | 836/1208 [3:41:35<1:36:31, 15.57s/it] 69%|██████▉   | 837/1208 [3:41:51<1:35:45, 15.49s/it] 69%|██████▉   | 838/1208 [3:42:06<1:35:32, 15.49s/it] 69%|██████▉   | 839/1208 [3:42:22<1:34:50, 15.42s/it] 70%|██████▉   | 840/1208 [3:42:37<1:34:37, 15.43s/it]                                                      {'loss': 1.7648, 'learning_rate': 1e-05, 'epoch': 1.39} 70%|██████▉   | 840/1208 [3:42:37<1:34:37, 15.43s/it] 70%|██████▉   | 841/1208 [3:42:53<1:34:45, 15.49s/it] 70%|██████▉   | 842/1208 [3:43:08<1:34:29, 15.49s/it] 70%|██████▉   | 843/1208 [3:43:23<1:34:00, 15.45s/it] 70%|██████▉   | 844/1208 [3:43:39<1:33:51, 15.47s/it] 70%|██████▉   | 845/1208 [3:43:55<1:33:41, 15.49s/it] 70%|███████   | 846/1208 [3:44:10<1:33:18, 15.46s/it] 70%|███████   | 847/1208 [3:44:25<1:33:00, 15.46s/it] 70%|███████   | 848/1208 [3:44:41<1:32:37, 15.44s/it] 70%|███████   | 849/1208 [3:44:56<1:32:19, 15.43s/it] 70%|███████   | 850/1208 [3:45:12<1:31:54, 15.40s/it]                                                      {'loss': 1.7652, 'learning_rate': 1e-05, 'epoch': 1.41} 70%|███████   | 850/1208 [3:45:12<1:31:54, 15.40s/it] 70%|███████   | 851/1208 [3:45:27<1:31:54, 15.45s/it] 71%|███████   | 852/1208 [3:45:43<1:32:15, 15.55s/it] 71%|███████   | 853/1208 [3:45:58<1:31:37, 15.49s/it] 71%|███████   | 854/1208 [3:46:14<1:31:06, 15.44s/it] 71%|███████   | 855/1208 [3:46:29<1:30:51, 15.44s/it] 71%|███████   | 856/1208 [3:46:44<1:30:19, 15.40s/it] 71%|███████   | 857/1208 [3:47:00<1:30:40, 15.50s/it] 71%|███████   | 858/1208 [3:47:15<1:30:06, 15.45s/it] 71%|███████   | 859/1208 [3:47:31<1:29:50, 15.45s/it] 71%|███████   | 860/1208 [3:47:46<1:29:38, 15.46s/it]                                                      {'loss': 1.7626, 'learning_rate': 1e-05, 'epoch': 1.42} 71%|███████   | 860/1208 [3:47:46<1:29:38, 15.46s/it] 71%|███████▏  | 861/1208 [3:48:02<1:29:31, 15.48s/it] 71%|███████▏  | 862/1208 [3:48:17<1:29:15, 15.48s/it] 71%|███████▏  | 863/1208 [3:48:33<1:29:02, 15.49s/it] 72%|███████▏  | 864/1208 [3:48:49<1:29:17, 15.57s/it] 72%|███████▏  | 865/1208 [3:49:04<1:29:00, 15.57s/it] 72%|███████▏  | 866/1208 [3:49:20<1:29:03, 15.62s/it] 72%|███████▏  | 867/1208 [3:49:35<1:28:37, 15.59s/it] 72%|███████▏  | 868/1208 [3:49:51<1:28:11, 15.56s/it] 72%|███████▏  | 869/1208 [3:50:06<1:27:36, 15.51s/it] 72%|███████▏  | 870/1208 [3:50:22<1:27:23, 15.51s/it]                                                      {'loss': 1.7736, 'learning_rate': 1e-05, 'epoch': 1.44} 72%|███████▏  | 870/1208 [3:50:22<1:27:23, 15.51s/it] 72%|███████▏  | 871/1208 [3:50:37<1:26:50, 15.46s/it] 72%|███████▏  | 872/1208 [3:50:52<1:26:23, 15.43s/it] 72%|███████▏  | 873/1208 [3:51:08<1:26:37, 15.52s/it] 72%|███████▏  | 874/1208 [3:51:24<1:26:08, 15.47s/it] 72%|███████▏  | 875/1208 [3:51:39<1:25:32, 15.41s/it] 73%|███████▎  | 876/1208 [3:51:54<1:24:57, 15.35s/it] 73%|███████▎  | 877/1208 [3:52:09<1:24:39, 15.35s/it] 73%|███████▎  | 878/1208 [3:52:25<1:24:05, 15.29s/it] 73%|███████▎  | 879/1208 [3:52:40<1:23:50, 15.29s/it] 73%|███████▎  | 880/1208 [3:52:55<1:23:35, 15.29s/it]                                                      {'loss': 1.7723, 'learning_rate': 1e-05, 'epoch': 1.46} 73%|███████▎  | 880/1208 [3:52:55<1:23:35, 15.29s/it] 73%|███████▎  | 881/1208 [3:53:11<1:23:52, 15.39s/it] 73%|███████▎  | 882/1208 [3:53:27<1:24:16, 15.51s/it] 73%|███████▎  | 883/1208 [3:53:42<1:24:11, 15.54s/it] 73%|███████▎  | 884/1208 [3:53:58<1:24:10, 15.59s/it] 73%|███████▎  | 885/1208 [3:54:13<1:23:43, 15.55s/it] 73%|███████▎  | 886/1208 [3:54:29<1:23:26, 15.55s/it] 73%|███████▎  | 887/1208 [3:54:45<1:23:24, 15.59s/it] 74%|███████▎  | 888/1208 [3:55:00<1:23:27, 15.65s/it] 74%|███████▎  | 889/1208 [3:55:16<1:22:42, 15.56s/it] 74%|███████▎  | 890/1208 [3:55:31<1:22:25, 15.55s/it]                                                      {'loss': 1.7749, 'learning_rate': 1e-05, 'epoch': 1.47} 74%|███████▎  | 890/1208 [3:55:31<1:22:25, 15.55s/it] 74%|███████▍  | 891/1208 [3:55:46<1:21:30, 15.43s/it] 74%|███████▍  | 892/1208 [3:56:02<1:21:08, 15.41s/it] 74%|███████▍  | 893/1208 [3:56:17<1:20:59, 15.43s/it] 74%|███████▍  | 894/1208 [3:56:32<1:20:09, 15.32s/it] 74%|███████▍  | 895/1208 [3:56:48<1:20:19, 15.40s/it] 74%|███████▍  | 896/1208 [3:57:03<1:20:05, 15.40s/it] 74%|███████▍  | 897/1208 [3:57:19<1:19:53, 15.41s/it] 74%|███████▍  | 898/1208 [3:57:34<1:19:46, 15.44s/it] 74%|███████▍  | 899/1208 [3:57:50<1:19:43, 15.48s/it] 75%|███████▍  | 900/1208 [3:58:06<1:19:55, 15.57s/it]                                                      {'loss': 1.7616, 'learning_rate': 1e-05, 'epoch': 1.49} 75%|███████▍  | 900/1208 [3:58:06<1:19:55, 15.57s/it] 75%|███████▍  | 901/1208 [3:59:11<2:35:45, 30.44s/it] 75%|███████▍  | 902/1208 [3:59:26<2:12:08, 25.91s/it] 75%|███████▍  | 903/1208 [3:59:41<1:55:23, 22.70s/it] 75%|███████▍  | 904/1208 [3:59:57<1:43:46, 20.48s/it] 75%|███████▍  | 905/1208 [4:00:12<1:35:31, 18.92s/it] 75%|███████▌  | 906/1208 [4:00:27<1:29:42, 17.82s/it] 75%|███████▌  | 907/1208 [4:00:42<1:25:22, 17.02s/it] 75%|███████▌  | 908/1208 [4:00:57<1:22:24, 16.48s/it] 75%|███████▌  | 909/1208 [4:01:13<1:20:25, 16.14s/it] 75%|███████▌  | 910/1208 [4:01:28<1:18:50, 15.87s/it]                                                      {'loss': 1.7815, 'learning_rate': 1e-05, 'epoch': 1.51} 75%|███████▌  | 910/1208 [4:01:28<1:18:50, 15.87s/it] 75%|███████▌  | 911/1208 [4:01:43<1:17:41, 15.69s/it] 75%|███████▌  | 912/1208 [4:01:59<1:16:56, 15.60s/it] 76%|███████▌  | 913/1208 [4:02:14<1:16:18, 15.52s/it] 76%|███████▌  | 914/1208 [4:02:29<1:15:51, 15.48s/it] 76%|███████▌  | 915/1208 [4:02:45<1:15:16, 15.41s/it] 76%|███████▌  | 916/1208 [4:03:00<1:14:54, 15.39s/it] 76%|███████▌  | 917/1208 [4:03:15<1:14:37, 15.39s/it] 76%|███████▌  | 918/1208 [4:03:31<1:14:16, 15.37s/it] 76%|███████▌  | 919/1208 [4:03:46<1:14:03, 15.38s/it] 76%|███████▌  | 920/1208 [4:04:01<1:13:35, 15.33s/it]                                                      {'loss': 1.768, 'learning_rate': 1e-05, 'epoch': 1.52} 76%|███████▌  | 920/1208 [4:04:01<1:13:35, 15.33s/it] 76%|███████▌  | 921/1208 [4:04:17<1:13:22, 15.34s/it] 76%|███████▋  | 922/1208 [4:04:32<1:13:12, 15.36s/it] 76%|███████▋  | 923/1208 [4:04:48<1:13:07, 15.40s/it] 76%|███████▋  | 924/1208 [4:05:03<1:13:15, 15.48s/it] 77%|███████▋  | 925/1208 [4:05:19<1:12:45, 15.42s/it] 77%|███████▋  | 926/1208 [4:05:34<1:12:47, 15.49s/it] 77%|███████▋  | 927/1208 [4:05:50<1:12:41, 15.52s/it] 77%|███████▋  | 928/1208 [4:06:05<1:12:26, 15.52s/it] 77%|███████▋  | 929/1208 [4:06:21<1:12:28, 15.59s/it] 77%|███████▋  | 930/1208 [4:06:36<1:11:56, 15.53s/it]                                                      {'loss': 1.7874, 'learning_rate': 1e-05, 'epoch': 1.54} 77%|███████▋  | 930/1208 [4:06:36<1:11:56, 15.53s/it] 77%|███████▋  | 931/1208 [4:06:52<1:11:36, 15.51s/it] 77%|███████▋  | 932/1208 [4:07:08<1:11:40, 15.58s/it] 77%|███████▋  | 933/1208 [4:07:23<1:11:19, 15.56s/it] 77%|███████▋  | 934/1208 [4:07:39<1:10:48, 15.50s/it] 77%|███████▋  | 935/1208 [4:07:54<1:10:37, 15.52s/it] 77%|███████▋  | 936/1208 [4:08:10<1:10:30, 15.55s/it] 78%|███████▊  | 937/1208 [4:08:26<1:10:42, 15.65s/it] 78%|███████▊  | 938/1208 [4:08:41<1:10:11, 15.60s/it] 78%|███████▊  | 939/1208 [4:08:57<1:10:04, 15.63s/it] 78%|███████▊  | 940/1208 [4:09:13<1:09:57, 15.66s/it]                                                      {'loss': 1.7678, 'learning_rate': 1e-05, 'epoch': 1.56} 78%|███████▊  | 940/1208 [4:09:13<1:09:57, 15.66s/it] 78%|███████▊  | 941/1208 [4:09:28<1:09:08, 15.54s/it] 78%|███████▊  | 942/1208 [4:09:43<1:08:47, 15.52s/it] 78%|███████▊  | 943/1208 [4:09:59<1:08:37, 15.54s/it] 78%|███████▊  | 944/1208 [4:10:14<1:08:22, 15.54s/it] 78%|███████▊  | 945/1208 [4:10:30<1:07:59, 15.51s/it] 78%|███████▊  | 946/1208 [4:10:45<1:07:41, 15.50s/it] 78%|███████▊  | 947/1208 [4:11:01<1:07:27, 15.51s/it] 78%|███████▊  | 948/1208 [4:11:16<1:07:24, 15.55s/it] 79%|███████▊  | 949/1208 [4:11:32<1:07:03, 15.54s/it] 79%|███████▊  | 950/1208 [4:11:47<1:06:39, 15.50s/it]                                                      {'loss': 1.7661, 'learning_rate': 1e-05, 'epoch': 1.57} 79%|███████▊  | 950/1208 [4:11:47<1:06:39, 15.50s/it] 79%|███████▊  | 951/1208 [4:12:03<1:06:35, 15.55s/it] 79%|███████▉  | 952/1208 [4:12:18<1:06:12, 15.52s/it] 79%|███████▉  | 953/1208 [4:12:34<1:05:55, 15.51s/it] 79%|███████▉  | 954/1208 [4:12:50<1:05:41, 15.52s/it] 79%|███████▉  | 955/1208 [4:13:05<1:05:44, 15.59s/it] 79%|███████▉  | 956/1208 [4:13:21<1:05:37, 15.63s/it] 79%|███████▉  | 957/1208 [4:13:36<1:05:07, 15.57s/it] 79%|███████▉  | 958/1208 [4:13:52<1:04:38, 15.52s/it] 79%|███████▉  | 959/1208 [4:14:07<1:04:06, 15.45s/it] 79%|███████▉  | 960/1208 [4:14:22<1:03:37, 15.39s/it]                                                      {'loss': 1.7779, 'learning_rate': 1e-05, 'epoch': 1.59} 79%|███████▉  | 960/1208 [4:14:22<1:03:37, 15.39s/it] 80%|███████▉  | 961/1208 [4:14:38<1:03:34, 15.44s/it] 80%|███████▉  | 962/1208 [4:14:53<1:03:22, 15.46s/it] 80%|███████▉  | 963/1208 [4:15:09<1:03:09, 15.47s/it] 80%|███████▉  | 964/1208 [4:15:24<1:02:47, 15.44s/it] 80%|███████▉  | 965/1208 [4:15:40<1:02:30, 15.44s/it] 80%|███████▉  | 966/1208 [4:15:55<1:01:58, 15.37s/it] 80%|████████  | 967/1208 [4:16:10<1:01:53, 15.41s/it] 80%|████████  | 968/1208 [4:16:26<1:01:35, 15.40s/it] 80%|████████  | 969/1208 [4:16:41<1:01:13, 15.37s/it] 80%|████████  | 970/1208 [4:16:57<1:01:09, 15.42s/it]                                                      {'loss': 1.7686, 'learning_rate': 1e-05, 'epoch': 1.61} 80%|████████  | 970/1208 [4:16:57<1:01:09, 15.42s/it] 80%|████████  | 971/1208 [4:17:12<1:01:18, 15.52s/it] 80%|████████  | 972/1208 [4:17:28<1:00:55, 15.49s/it] 81%|████████  | 973/1208 [4:17:43<1:00:45, 15.51s/it] 81%|████████  | 974/1208 [4:17:59<1:00:24, 15.49s/it]Traceback (most recent call last):  File "train.py", line 130, in <module>    main()  File "train.py", line 118, in main    train_result = trainer.train()  File "/usr/local/miniconda3/envs/llm_env/lib/python3.8/site-packages/transformers/trainer.py", line 1645, in train    return inner_training_loop(  File "/usr/local/miniconda3/envs/llm_env/lib/python3.8/site-packages/transformers/trainer.py", line 1938, in _inner_training_loop    tr_loss_step = self.training_step(model, inputs)  File "/usr/local/miniconda3/envs/llm_env/lib/python3.8/site-packages/transformers/trainer.py", line 2759, in training_step    loss = self.compute_loss(model, inputs)  File "/hy-tmp/autodl-tmp/artboy/finetune/llm_code/component/trainer.py", line 72, in compute_loss    return self.loss_func(model, inputs, self.args, return_outputs)  File "/hy-tmp/autodl-tmp/artboy/finetune/llm_code/component/loss.py", line 43, in __call__    loss = self.loss_fn(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))  File "/usr/local/miniconda3/envs/llm_env/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl    return forward_call(*args, **kwargs)  File "/usr/local/miniconda3/envs/llm_env/lib/python3.8/site-packages/torch/nn/modules/loss.py", line 1174, in forward    return F.cross_entropy(input, target, weight=self.weight,  File "/usr/local/miniconda3/envs/llm_env/lib/python3.8/site-packages/torch/nn/functional.py", line 3029, in cross_entropy    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.92 GiB (GPU 0; 79.15 GiB total capacity; 69.79 GiB already allocated; 1.11 GiB free; 77.38 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF 81%|████████  | 974/1208 [4:18:03<1:01:59, 15.90s/it][2023-11-23 17:50:48,983] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 448049[2023-11-23 17:50:48,987] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 448050[2023-11-23 17:50:56,428] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 448051[2023-11-23 17:51:03,796] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 448052[2023-11-23 17:51:13,262] [ERROR] [launch.py:321:sigkill_handler] ['/usr/local/miniconda3/envs/llm_env/bin/python', '-u', 'train.py', '--local_rank=3', '--train_args_file', '/hy-tmp/autodl-tmp/artboy/finetune/llm_code/training_config/baichuan2_config.json'] exits with return code = 1/usr/local/miniconda3/envs/llm_env/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/miniconda3/envs/llm_env did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...  warn(msg)/usr/local/miniconda3/envs/llm_env/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.Either way, this might cause trouble in the future:If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.  warn(msg)