[2023-11-23 10:07:42,215] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)===================================BUG REPORT===================================Welcome to bitsandbytes. For bug reports, please runpython -m bitsandbytes and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues================================================================================bin /usr/local/miniconda3/envs/llm_env/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda118.soCUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.soCUDA SETUP: Highest compute capability among GPUs detected: 8.0CUDA SETUP: Detected CUDA version 118CUDA SETUP: Loading binary /usr/local/miniconda3/envs/llm_env/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...[2023-11-23 10:07:44,025] [WARNING] [runner.py:196:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.Detected CUDA_VISIBLE_DEVICES=0,1,2,3 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.[2023-11-23 10:07:44,026] [INFO] [runner.py:555:main] cmd = /usr/local/miniconda3/envs/llm_env/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=65224 --enable_each_rank_log=None train.py --train_args_file /hy-tmp/autodl-tmp/artboy/finetune/llm_code/training_config/baichuan2_config.json[2023-11-23 10:07:45,220] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)===================================BUG REPORT===================================Welcome to bitsandbytes. For bug reports, please runpython -m bitsandbytes and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues================================================================================bin /usr/local/miniconda3/envs/llm_env/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda118.so/usr/local/miniconda3/envs/llm_env/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/miniconda3/envs/llm_env did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...  warn(msg)CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths.../usr/local/miniconda3/envs/llm_env/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.Either way, this might cause trouble in the future:If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.  warn(msg)CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.soCUDA SETUP: Highest compute capability among GPUs detected: 8.0CUDA SETUP: Detected CUDA version 118CUDA SETUP: Loading binary /usr/local/miniconda3/envs/llm_env/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...[2023-11-23 10:07:46,588] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}[2023-11-23 10:07:46,588] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0[2023-11-23 10:07:46,588] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})[2023-11-23 10:07:46,588] [INFO] [launch.py:163:main] dist_world_size=4[2023-11-23 10:07:46,588] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3===================================BUG REPORT===================================Welcome to bitsandbytes. For bug reports, please runpython -m bitsandbytes and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues================================================================================bin /usr/local/miniconda3/envs/llm_env/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda118.so/usr/local/miniconda3/envs/llm_env/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/miniconda3/envs/llm_env did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...  warn(msg)CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths.../usr/local/miniconda3/envs/llm_env/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.Either way, this might cause trouble in the future:If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.  warn(msg)CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0CUDA SETUP: Highest compute capability among GPUs detected: 8.0CUDA SETUP: Detected CUDA version 118CUDA SETUP: Loading binary /usr/local/miniconda3/envs/llm_env/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...===================================BUG REPORT===================================Welcome to bitsandbytes. For bug reports, please runpython -m bitsandbytes and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues===================================================================================================================BUG REPORT===================================Welcome to bitsandbytes. For bug reports, please runpython -m bitsandbytes and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues================================================================================bin /usr/local/miniconda3/envs/llm_env/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda118.so/usr/local/miniconda3/envs/llm_env/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/miniconda3/envs/llm_env did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...  warn(msg)CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths.../usr/local/miniconda3/envs/llm_env/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.Either way, this might cause trouble in the future:If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.  warn(msg)CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0CUDA SETUP: Highest compute capability among GPUs detected: 8.0CUDA SETUP: Detected CUDA version 118CUDA SETUP: Loading binary /usr/local/miniconda3/envs/llm_env/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...bin /usr/local/miniconda3/envs/llm_env/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda118.so/usr/local/miniconda3/envs/llm_env/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/miniconda3/envs/llm_env did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...  warn(msg)CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths.../usr/local/miniconda3/envs/llm_env/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.Either way, this might cause trouble in the future:If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.  warn(msg)CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.soCUDA SETUP: Highest compute capability among GPUs detected: 8.0CUDA SETUP: Detected CUDA version 118CUDA SETUP: Loading binary /usr/local/miniconda3/envs/llm_env/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...===================================BUG REPORT===================================Welcome to bitsandbytes. For bug reports, please runpython -m bitsandbytes and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues================================================================================bin /usr/local/miniconda3/envs/llm_env/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda118.so/usr/local/miniconda3/envs/llm_env/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/miniconda3/envs/llm_env did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...  warn(msg)CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths.../usr/local/miniconda3/envs/llm_env/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.Either way, this might cause trouble in the future:If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.  warn(msg)CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0CUDA SETUP: Highest compute capability among GPUs detected: 8.0CUDA SETUP: Detected CUDA version 118CUDA SETUP: Loading binary /usr/local/miniconda3/envs/llm_env/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...[2023-11-23 10:07:49,171] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)[2023-11-23 10:07:49,357] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)[2023-11-23 10:07:49,456] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)[2023-11-23 10:07:49,485] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)[2023-11-23 10:07:49,930] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented[2023-11-23 10:07:49,930] [INFO] [comm.py:594:init_distributed] cdb=None[2023-11-23 10:07:50,014] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented[2023-11-23 10:07:50,014] [INFO] [comm.py:594:init_distributed] cdb=None[2023-11-23 10:07:50,014] [INFO] [comm.py:625:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl[2023-11-23 10:07:50,212] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented[2023-11-23 10:07:50,212] [INFO] [comm.py:594:init_distributed] cdb=None[2023-11-23 10:07:50,238] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented[2023-11-23 10:07:50,238] [INFO] [comm.py:594:init_distributed] cdb=None2023-11-23 10:07:50.948 | INFO     | __main__:setup_everything:35 - train_args:TrainingArguments(_n_gpu=1,adafactor=False,adam_beta1=0.9,adam_beta2=0.999,adam_epsilon=1e-08,auto_find_batch_size=False,bf16=False,bf16_full_eval=False,data_seed=None,dataloader_drop_last=False,dataloader_num_workers=5,dataloader_pin_memory=True,ddp_backend=None,ddp_bucket_cap_mb=None,ddp_find_unused_parameters=None,ddp_timeout=1800,debug=[],deepspeed=/hy-tmp/autodl-tmp/artboy/finetune/llm_code/deepspeed_config/deepspeed_config.json,disable_tqdm=False,do_eval=False,do_predict=False,do_train=False,eval_accumulation_steps=None,eval_delay=0,eval_steps=None,evaluation_strategy=no,fp16=True,fp16_backend=auto,fp16_full_eval=False,fp16_opt_level=O1,fsdp=[],fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},fsdp_min_num_params=0,fsdp_transformer_layer_cls_to_wrap=None,full_determinism=False,gradient_accumulation_steps=2,gradient_checkpointing=False,greater_is_better=None,group_by_length=False,half_precision_backend=auto,hub_model_id=None,hub_private_repo=False,hub_strategy=every_save,hub_token=<HUB_TOKEN>,ignore_data_skip=False,include_inputs_for_metrics=False,jit_mode_eval=False,label_names=None,label_smoothing_factor=0.0,learning_rate=1e-05,length_column_name=length,load_best_model_at_end=False,local_rank=0,log_level=passive,log_level_replica=warning,log_on_each_node=True,logging_dir=./output/baichuan2-sft-1e5-1123-1806/runs/Nov23_10-07-50_I1678c166a701b012d0,logging_first_step=False,logging_nan_inf_filter=True,logging_steps=10,logging_strategy=steps,lr_scheduler_type=cosine,max_grad_norm=1.0,max_steps=-1,metric_for_best_model=None,mp_parameters=,no_cuda=False,num_train_epochs=1,optim=adamw_hf,optim_args=None,output_dir=./output/baichuan2-sft-1e5-1123-1806,overwrite_output_dir=False,past_index=-1,per_device_eval_batch_size=8,per_device_train_batch_size=16,prediction_loss_only=False,push_to_hub=False,push_to_hub_model_id=None,push_to_hub_organization=None,push_to_hub_token=<PUSH_TO_HUB_TOKEN>,ray_scope=last,remove_unused_columns=False,report_to=['tensorboard'],resume_from_checkpoint=None,run_name=./output/baichuan2-sft-1e5-1123-1806,save_on_each_node=False,save_safetensors=False,save_steps=100,save_strategy=steps,save_total_limit=1,seed=42,sharded_ddp=[],skip_memory_metrics=True,tf32=None,torch_compile=False,torch_compile_backend=None,torch_compile_mode=None,torchdynamo=None,tpu_metrics_debug=False,tpu_num_cores=None,use_ipex=False,use_legacy_prediction_loop=False,use_mps_device=False,warmup_ratio=0.0,warmup_steps=200,weight_decay=0,xpu_backend=None,)2023-11-23 10:07:50.948 | INFO     | __main__:init_components:45 - Initializing components...2023-11-23 10:07:50.951 | INFO     | __main__:setup_everything:35 - train_args:TrainingArguments(_n_gpu=1,adafactor=False,adam_beta1=0.9,adam_beta2=0.999,adam_epsilon=1e-08,auto_find_batch_size=False,bf16=False,bf16_full_eval=False,data_seed=None,dataloader_drop_last=False,dataloader_num_workers=5,dataloader_pin_memory=True,ddp_backend=None,ddp_bucket_cap_mb=None,ddp_find_unused_parameters=None,ddp_timeout=1800,debug=[],deepspeed=/hy-tmp/autodl-tmp/artboy/finetune/llm_code/deepspeed_config/deepspeed_config.json,disable_tqdm=False,do_eval=False,do_predict=False,do_train=False,eval_accumulation_steps=None,eval_delay=0,eval_steps=None,evaluation_strategy=no,fp16=True,fp16_backend=auto,fp16_full_eval=False,fp16_opt_level=O1,fsdp=[],fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},fsdp_min_num_params=0,fsdp_transformer_layer_cls_to_wrap=None,full_determinism=False,gradient_accumulation_steps=2,gradient_checkpointing=False,greater_is_better=None,group_by_length=False,half_precision_backend=auto,hub_model_id=None,hub_private_repo=False,hub_strategy=every_save,hub_token=<HUB_TOKEN>,ignore_data_skip=False,include_inputs_for_metrics=False,jit_mode_eval=False,label_names=None,label_smoothing_factor=0.0,learning_rate=1e-05,length_column_name=length,load_best_model_at_end=False,local_rank=1,log_level=passive,log_level_replica=warning,log_on_each_node=True,logging_dir=./output/baichuan2-sft-1e5-1123-1806/runs/Nov23_10-07-50_I1678c166a701b012d0,logging_first_step=False,logging_nan_inf_filter=True,logging_steps=10,logging_strategy=steps,lr_scheduler_type=cosine,max_grad_norm=1.0,max_steps=-1,metric_for_best_model=None,mp_parameters=,no_cuda=False,num_train_epochs=1,optim=adamw_hf,optim_args=None,output_dir=./output/baichuan2-sft-1e5-1123-1806,overwrite_output_dir=False,past_index=-1,per_device_eval_batch_size=8,per_device_train_batch_size=16,prediction_loss_only=False,push_to_hub=False,push_to_hub_model_id=None,push_to_hub_organization=None,push_to_hub_token=<PUSH_TO_HUB_TOKEN>,ray_scope=last,remove_unused_columns=False,report_to=['tensorboard'],resume_from_checkpoint=None,run_name=./output/baichuan2-sft-1e5-1123-1806,save_on_each_node=False,save_safetensors=False,save_steps=100,save_strategy=steps,save_total_limit=1,seed=42,sharded_ddp=[],skip_memory_metrics=True,tf32=None,torch_compile=False,torch_compile_backend=None,torch_compile_mode=None,torchdynamo=None,tpu_metrics_debug=False,tpu_num_cores=None,use_ipex=False,use_legacy_prediction_loop=False,use_mps_device=False,warmup_ratio=0.0,warmup_steps=200,weight_decay=0,xpu_backend=None,)2023-11-23 10:07:50.952 | INFO     | __main__:init_components:45 - Initializing components...2023-11-23 10:07:50.956 | INFO     | __main__:setup_everything:35 - train_args:TrainingArguments(_n_gpu=1,adafactor=False,adam_beta1=0.9,adam_beta2=0.999,adam_epsilon=1e-08,auto_find_batch_size=False,bf16=False,bf16_full_eval=False,data_seed=None,dataloader_drop_last=False,dataloader_num_workers=5,dataloader_pin_memory=True,ddp_backend=None,ddp_bucket_cap_mb=None,ddp_find_unused_parameters=None,ddp_timeout=1800,debug=[],deepspeed=/hy-tmp/autodl-tmp/artboy/finetune/llm_code/deepspeed_config/deepspeed_config.json,disable_tqdm=False,do_eval=False,do_predict=False,do_train=False,eval_accumulation_steps=None,eval_delay=0,eval_steps=None,evaluation_strategy=no,fp16=True,fp16_backend=auto,fp16_full_eval=False,fp16_opt_level=O1,fsdp=[],fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},fsdp_min_num_params=0,fsdp_transformer_layer_cls_to_wrap=None,full_determinism=False,gradient_accumulation_steps=2,gradient_checkpointing=False,greater_is_better=None,group_by_length=False,half_precision_backend=auto,hub_model_id=None,hub_private_repo=False,hub_strategy=every_save,hub_token=<HUB_TOKEN>,ignore_data_skip=False,include_inputs_for_metrics=False,jit_mode_eval=False,label_names=None,label_smoothing_factor=0.0,learning_rate=1e-05,length_column_name=length,load_best_model_at_end=False,local_rank=2,log_level=passive,log_level_replica=warning,log_on_each_node=True,logging_dir=./output/baichuan2-sft-1e5-1123-1806/runs/Nov23_10-07-50_I1678c166a701b012d0,logging_first_step=False,logging_nan_inf_filter=True,logging_steps=10,logging_strategy=steps,lr_scheduler_type=cosine,max_grad_norm=1.0,max_steps=-1,metric_for_best_model=None,mp_parameters=,no_cuda=False,num_train_epochs=1,optim=adamw_hf,optim_args=None,output_dir=./output/baichuan2-sft-1e5-1123-1806,overwrite_output_dir=False,past_index=-1,per_device_eval_batch_size=8,per_device_train_batch_size=16,prediction_loss_only=False,push_to_hub=False,push_to_hub_model_id=None,push_to_hub_organization=None,push_to_hub_token=<PUSH_TO_HUB_TOKEN>,ray_scope=last,remove_unused_columns=False,report_to=['tensorboard'],resume_from_checkpoint=None,run_name=./output/baichuan2-sft-1e5-1123-1806,save_on_each_node=False,save_safetensors=False,save_steps=100,save_strategy=steps,save_total_limit=1,seed=42,sharded_ddp=[],skip_memory_metrics=True,tf32=None,torch_compile=False,torch_compile_backend=None,torch_compile_mode=None,torchdynamo=None,tpu_metrics_debug=False,tpu_num_cores=None,use_ipex=False,use_legacy_prediction_loop=False,use_mps_device=False,warmup_ratio=0.0,warmup_steps=200,weight_decay=0,xpu_backend=None,)2023-11-23 10:07:50.957 | INFO     | __main__:init_components:45 - Initializing components...2023-11-23 10:07:50.958 | INFO     | __main__:setup_everything:35 - train_args:TrainingArguments(_n_gpu=1,adafactor=False,adam_beta1=0.9,adam_beta2=0.999,adam_epsilon=1e-08,auto_find_batch_size=False,bf16=False,bf16_full_eval=False,data_seed=None,dataloader_drop_last=False,dataloader_num_workers=5,dataloader_pin_memory=True,ddp_backend=None,ddp_bucket_cap_mb=None,ddp_find_unused_parameters=None,ddp_timeout=1800,debug=[],deepspeed=/hy-tmp/autodl-tmp/artboy/finetune/llm_code/deepspeed_config/deepspeed_config.json,disable_tqdm=False,do_eval=False,do_predict=False,do_train=False,eval_accumulation_steps=None,eval_delay=0,eval_steps=None,evaluation_strategy=no,fp16=True,fp16_backend=auto,fp16_full_eval=False,fp16_opt_level=O1,fsdp=[],fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},fsdp_min_num_params=0,fsdp_transformer_layer_cls_to_wrap=None,full_determinism=False,gradient_accumulation_steps=2,gradient_checkpointing=False,greater_is_better=None,group_by_length=False,half_precision_backend=auto,hub_model_id=None,hub_private_repo=False,hub_strategy=every_save,hub_token=<HUB_TOKEN>,ignore_data_skip=False,include_inputs_for_metrics=False,jit_mode_eval=False,label_names=None,label_smoothing_factor=0.0,learning_rate=1e-05,length_column_name=length,load_best_model_at_end=False,local_rank=3,log_level=passive,log_level_replica=warning,log_on_each_node=True,logging_dir=./output/baichuan2-sft-1e5-1123-1806/runs/Nov23_10-07-49_I1678c166a701b012d0,logging_first_step=False,logging_nan_inf_filter=True,logging_steps=10,logging_strategy=steps,lr_scheduler_type=cosine,max_grad_norm=1.0,max_steps=-1,metric_for_best_model=None,mp_parameters=,no_cuda=False,num_train_epochs=1,optim=adamw_hf,optim_args=None,output_dir=./output/baichuan2-sft-1e5-1123-1806,overwrite_output_dir=False,past_index=-1,per_device_eval_batch_size=8,per_device_train_batch_size=16,prediction_loss_only=False,push_to_hub=False,push_to_hub_model_id=None,push_to_hub_organization=None,push_to_hub_token=<PUSH_TO_HUB_TOKEN>,ray_scope=last,remove_unused_columns=False,report_to=['tensorboard'],resume_from_checkpoint=None,run_name=./output/baichuan2-sft-1e5-1123-1806,save_on_each_node=False,save_safetensors=False,save_steps=100,save_strategy=steps,save_total_limit=1,seed=42,sharded_ddp=[],skip_memory_metrics=True,tf32=None,torch_compile=False,torch_compile_backend=None,torch_compile_mode=None,torchdynamo=None,tpu_metrics_debug=False,tpu_num_cores=None,use_ipex=False,use_legacy_prediction_loop=False,use_mps_device=False,warmup_ratio=0.0,warmup_steps=200,weight_decay=0,xpu_backend=None,)2023-11-23 10:07:50.958 | INFO     | __main__:init_components:45 - Initializing components...[2023-11-23 10:08:00,079] [INFO] [partition_parameters.py:453:__exit__] finished initializing model with 7.51B parametersLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.26s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.28s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.34s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.42s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.33s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.77s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.33s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.77s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.34s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.81s/it]2023-11-23 10:08:15.747 | INFO     | __main__:init_components:88 - Total model params: 0.00M2023-11-23 10:08:15.747 | INFO     | component.dataset:__init__:12 - Loading data: /hy-tmp/autodl-tmp/artboy/finetune/llm_code/data/psychology_data.jsonlLoading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.34s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.79s/it]2023-11-23 10:08:15.763 | INFO     | __main__:init_components:88 - Total model params: 0.00M2023-11-23 10:08:15.764 | INFO     | component.dataset:__init__:12 - Loading data: /hy-tmp/autodl-tmp/artboy/finetune/llm_code/data/psychology_data.jsonl2023-11-23 10:08:15.783 | INFO     | __main__:init_components:88 - Total model params: 0.00M2023-11-23 10:08:15.783 | INFO     | component.dataset:__init__:12 - Loading data: /hy-tmp/autodl-tmp/artboy/finetune/llm_code/data/psychology_data.jsonl2023-11-23 10:08:15.811 | INFO     | __main__:init_components:88 - Total model params: 0.00M2023-11-23 10:08:15.812 | INFO     | component.dataset:__init__:12 - Loading data: /hy-tmp/autodl-tmp/artboy/finetune/llm_code/data/psychology_data.jsonl2023-11-23 10:08:15.922 | INFO     | component.dataset:__init__:15 - there are 77250 data in dataset2023-11-23 10:08:15.931 | INFO     | __main__:main:117 - *** starting training ***2023-11-23 10:08:15.937 | INFO     | component.dataset:__init__:15 - there are 77250 data in dataset2023-11-23 10:08:15.947 | INFO     | __main__:main:117 - *** starting training ***2023-11-23 10:08:15.954 | INFO     | component.dataset:__init__:15 - there are 77250 data in dataset2023-11-23 10:08:15.963 | INFO     | __main__:main:117 - *** starting training ***2023-11-23 10:08:15.993 | INFO     | component.dataset:__init__:15 - there are 77250 data in dataset2023-11-23 10:08:16.004 | INFO     | __main__:main:117 - *** starting training ***Installed CUDA version 11.8 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combinationInstalled CUDA version 11.8 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combinationInstalled CUDA version 11.8 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combinationInstalled CUDA version 11.8 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combinationInstalled CUDA version 11.8 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combinationUsing /root/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...Installed CUDA version 11.8 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combinationUsing /root/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...Installed CUDA version 11.8 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combinationUsing /root/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...Installed CUDA version 11.8 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combinationUsing /root/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...Detected CUDA files, patching ldflagsEmitting ninja build file /root/.cache/torch_extensions/py38_cu117/cpu_adam/build.ninja...Building extension module cpu_adam...Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)ninja: no work to do.Loading extension module cpu_adam...Loading extension module cpu_adam...Time to load cpu_adam op: 2.995157480239868 secondsTime to load cpu_adam op: 3.080779790878296 secondsLoading extension module cpu_adam...Time to load cpu_adam op: 2.933215618133545 secondsLoading extension module cpu_adam...Time to load cpu_adam op: 3.092262029647827 secondsParameter Offload: Total persistent parameters: 266240 in 65 params  0%|          | 0/604 [00:00<?, ?it/s]/usr/local/miniconda3/envs/llm_env/lib/python3.8/site-packages/xformers/ops/fmha/flash.py:338: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()  and inp.query.storage().data_ptr() == inp.key.storage().data_ptr()/usr/local/miniconda3/envs/llm_env/lib/python3.8/site-packages/xformers/ops/fmha/flash.py:338: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()  and inp.query.storage().data_ptr() == inp.key.storage().data_ptr()/usr/local/miniconda3/envs/llm_env/lib/python3.8/site-packages/xformers/ops/fmha/flash.py:338: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()  and inp.query.storage().data_ptr() == inp.key.storage().data_ptr()/usr/local/miniconda3/envs/llm_env/lib/python3.8/site-packages/xformers/ops/fmha/flash.py:338: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()  and inp.query.storage().data_ptr() == inp.key.storage().data_ptr()  0%|          | 1/604 [00:16<2:49:29, 16.86s/it]  0%|          | 2/604 [00:29<2:26:21, 14.59s/it]  0%|          | 3/604 [00:42<2:18:02, 13.78s/it]  1%|          | 4/604 [00:55<2:14:10, 13.42s/it]  1%|          | 5/604 [01:08<2:11:44, 13.20s/it]  1%|          | 6/604 [01:20<2:09:28, 12.99s/it]  1%|          | 7/604 [01:33<2:08:40, 12.93s/it]  1%|▏         | 8/604 [01:46<2:07:44, 12.86s/it]  1%|▏         | 9/604 [01:59<2:06:59, 12.81s/it]  2%|▏         | 10/604 [02:11<2:06:48, 12.81s/it]tried to get lr value before scheduler/optimizer started stepping, returning lr=0tried to get lr value before scheduler/optimizer started stepping, returning lr=0tried to get lr value before scheduler/optimizer started stepping, returning lr=0tried to get lr value before scheduler/optimizer started stepping, returning lr=0                                                  {'loss': 3.0184, 'learning_rate': 0, 'epoch': 0.02}  2%|▏         | 10/604 [02:11<2:06:48, 12.81s/it]  2%|▏         | 11/604 [02:24<2:06:12, 12.77s/it]  2%|▏         | 12/604 [02:51<2:47:42, 17.00s/it]  2%|▏         | 13/604 [03:08<2:46:59, 16.95s/it]  2%|▏         | 14/604 [03:24<2:46:02, 16.89s/it]  2%|▏         | 15/604 [03:41<2:44:53, 16.80s/it]  3%|▎         | 16/604 [03:58<2:43:58, 16.73s/it]  3%|▎         | 17/604 [04:14<2:42:08, 16.57s/it][2023-11-23 10:13:18,531] [WARNING] [stage3.py:1850:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time  3%|▎         | 18/604 [04:30<2:41:15, 16.51s/it]  3%|▎         | 19/604 [04:47<2:41:01, 16.52s/it]  3%|▎         | 20/604 [05:03<2:39:46, 16.42s/it]                                                  {'loss': 2.6833, 'learning_rate': 4.1470233384070705e-06, 'epoch': 0.03}  3%|▎         | 20/604 [05:03<2:39:46, 16.42s/it]  3%|▎         | 21/604 [05:19<2:40:03, 16.47s/it]  4%|▎         | 22/604 [05:36<2:39:22, 16.43s/it]  4%|▍         | 23/604 [05:52<2:37:32, 16.27s/it]  4%|▍         | 24/604 [06:08<2:36:19, 16.17s/it]  4%|▍         | 25/604 [06:24<2:35:49, 16.15s/it]  4%|▍         | 26/604 [06:40<2:36:57, 16.29s/it]  4%|▍         | 27/604 [06:57<2:37:05, 16.33s/it]  5%|▍         | 28/604 [07:13<2:35:43, 16.22s/it]  5%|▍         | 29/604 [07:29<2:35:10, 16.19s/it]  5%|▍         | 30/604 [07:45<2:35:22, 16.24s/it]                                                  {'loss': 2.4149, 'learning_rate': 5.557309567291557e-06, 'epoch': 0.05}  5%|▍         | 30/604 [07:45<2:35:22, 16.24s/it]  5%|▌         | 31/604 [08:02<2:35:48, 16.31s/it]  5%|▌         | 32/604 [08:18<2:35:30, 16.31s/it]  5%|▌         | 33/604 [08:34<2:35:10, 16.31s/it]  6%|▌         | 34/604 [08:51<2:35:33, 16.37s/it]  6%|▌         | 35/604 [09:07<2:34:38, 16.31s/it]  6%|▌         | 36/604 [09:23<2:34:32, 16.33s/it]  6%|▌         | 37/604 [09:40<2:34:42, 16.37s/it]  6%|▋         | 38/604 [09:56<2:33:51, 16.31s/it]  6%|▋         | 39/604 [10:12<2:32:46, 16.22s/it]  7%|▋         | 40/604 [10:28<2:31:12, 16.09s/it]                                                  {'loss': 2.3243, 'learning_rate': 6.355406060132516e-06, 'epoch': 0.07}  7%|▋         | 40/604 [10:28<2:31:12, 16.09s/it]  7%|▋         | 41/604 [10:44<2:31:42, 16.17s/it]  7%|▋         | 42/604 [11:01<2:33:20, 16.37s/it]  7%|▋         | 43/604 [11:17<2:33:11, 16.38s/it]  7%|▋         | 44/604 [11:33<2:31:29, 16.23s/it]  7%|▋         | 45/604 [11:49<2:30:22, 16.14s/it]  8%|▊         | 46/604 [12:06<2:30:35, 16.19s/it]  8%|▊         | 47/604 [12:22<2:30:57, 16.26s/it]  8%|▊         | 48/604 [12:38<2:29:51, 16.17s/it]  8%|▊         | 49/604 [12:54<2:30:11, 16.24s/it]  8%|▊         | 50/604 [13:10<2:29:01, 16.14s/it]                                                  {'loss': 2.2645, 'learning_rate': 6.914575690124303e-06, 'epoch': 0.08}  8%|▊         | 50/604 [13:10<2:29:01, 16.14s/it]  8%|▊         | 51/604 [13:26<2:28:28, 16.11s/it]  9%|▊         | 52/604 [13:43<2:28:42, 16.16s/it]  9%|▉         | 53/604 [13:59<2:28:47, 16.20s/it]  9%|▉         | 54/604 [14:15<2:28:25, 16.19s/it]  9%|▉         | 55/604 [14:31<2:28:45, 16.26s/it]  9%|▉         | 56/604 [14:48<2:28:31, 16.26s/it]  9%|▉         | 57/604 [15:04<2:27:28, 16.18s/it] 10%|▉         | 58/604 [15:20<2:26:41, 16.12s/it] 10%|▉         | 59/604 [15:36<2:27:12, 16.21s/it] 10%|▉         | 60/604 [15:52<2:26:44, 16.19s/it]                                                  {'loss': 2.2278, 'learning_rate': 7.345389165780057e-06, 'epoch': 0.1} 10%|▉         | 60/604 [15:52<2:26:44, 16.19s/it] 10%|█         | 61/604 [16:08<2:26:16, 16.16s/it] 10%|█         | 62/604 [16:24<2:25:24, 16.10s/it] 10%|█         | 63/604 [16:40<2:24:51, 16.07s/it] 11%|█         | 64/604 [16:56<2:24:32, 16.06s/it] 11%|█         | 65/604 [17:12<2:23:47, 16.01s/it] 11%|█         | 66/604 [17:28<2:23:34, 16.01s/it] 11%|█         | 67/604 [17:44<2:23:34, 16.04s/it] 11%|█▏        | 68/604 [18:00<2:22:52, 15.99s/it] 11%|█▏        | 69/604 [18:16<2:22:25, 15.97s/it] 12%|█▏        | 70/604 [18:32<2:21:51, 15.94s/it]                                                  {'loss': 2.2226, 'learning_rate': 7.69591015753426e-06, 'epoch': 0.12} 12%|█▏        | 70/604 [18:32<2:21:51, 15.94s/it] 12%|█▏        | 71/604 [18:48<2:22:27, 16.04s/it] 12%|█▏        | 72/604 [19:04<2:21:24, 15.95s/it] 12%|█▏        | 73/604 [19:20<2:20:07, 15.83s/it] 12%|█▏        | 74/604 [19:36<2:20:41, 15.93s/it] 12%|█▏        | 75/604 [19:52<2:21:09, 16.01s/it] 13%|█▎        | 76/604 [20:08<2:20:47, 16.00s/it] 13%|█▎        | 77/604 [20:24<2:20:52, 16.04s/it] 13%|█▎        | 78/604 [20:40<2:20:30, 16.03s/it] 13%|█▎        | 79/604 [20:56<2:20:24, 16.05s/it] 13%|█▎        | 80/604 [21:12<2:20:35, 16.10s/it]                                                  {'loss': 2.1901, 'learning_rate': 7.991417296612165e-06, 'epoch': 0.13} 13%|█▎        | 80/604 [21:12<2:20:35, 16.10s/it] 13%|█▎        | 81/604 [21:28<2:20:19, 16.10s/it] 14%|█▎        | 82/604 [21:44<2:19:03, 15.98s/it] 14%|█▎        | 83/604 [22:00<2:19:18, 16.04s/it] 14%|█▍        | 84/604 [22:16<2:18:54, 16.03s/it] 14%|█▍        | 85/604 [22:32<2:18:21, 16.00s/it] 14%|█▍        | 86/604 [22:49<2:19:19, 16.14s/it] 14%|█▍        | 87/604 [23:05<2:18:31, 16.08s/it] 15%|█▍        | 88/604 [23:21<2:18:20, 16.09s/it] 15%|█▍        | 89/604 [23:37<2:17:39, 16.04s/it] 15%|█▍        | 90/604 [23:52<2:16:26, 15.93s/it]                                                  {'loss': 2.1893, 'learning_rate': 8.246859427588061e-06, 'epoch': 0.15} 15%|█▍        | 90/604 [23:52<2:16:26, 15.93s/it] 15%|█▌        | 91/604 [24:09<2:17:38, 16.10s/it] 15%|█▌        | 92/604 [24:25<2:16:24, 15.99s/it] 15%|█▌        | 93/604 [24:40<2:15:27, 15.90s/it] 16%|█▌        | 94/604 [24:56<2:16:01, 16.00s/it] 16%|█▌        | 95/604 [25:12<2:15:07, 15.93s/it] 16%|█▌        | 96/604 [25:28<2:14:49, 15.92s/it] 16%|█▌        | 97/604 [25:44<2:14:10, 15.88s/it] 16%|█▌        | 98/604 [26:00<2:14:19, 15.93s/it] 16%|█▋        | 99/604 [26:16<2:13:55, 15.91s/it] 17%|█▋        | 100/604 [26:32<2:14:27, 16.01s/it]                                                   {'loss': 2.1914, 'learning_rate': 8.471814840824796e-06, 'epoch': 0.17} 17%|█▋        | 100/604 [26:32<2:14:27, 16.01s/it]/usr/local/miniconda3/envs/llm_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.  warnings.warn(/usr/local/miniconda3/envs/llm_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.  warnings.warn(/usr/local/miniconda3/envs/llm_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.  warnings.warn(/usr/local/miniconda3/envs/llm_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.  warnings.warn( 17%|█▋        | 101/604 [27:29<3:57:04, 28.28s/it] 17%|█▋        | 102/604 [27:44<3:24:25, 24.43s/it] 17%|█▋        | 103/604 [28:00<3:00:40, 21.64s/it] 17%|█▋        | 104/604 [28:15<2:44:25, 19.73s/it] 17%|█▋        | 105/604 [28:30<2:33:35, 18.47s/it] 18%|█▊        | 106/604 [28:46<2:25:35, 17.54s/it] 18%|█▊        | 107/604 [29:01<2:20:07, 16.92s/it] 18%|█▊        | 108/604 [29:17<2:15:54, 16.44s/it] 18%|█▊        | 109/604 [29:32<2:13:07, 16.14s/it] 18%|█▊        | 110/604 [29:47<2:10:33, 15.86s/it]                                                   {'loss': 2.1908, 'learning_rate': 8.672790873470093e-06, 'epoch': 0.18} 18%|█▊        | 110/604 [29:47<2:10:33, 15.86s/it] 18%|█▊        | 111/604 [30:02<2:08:48, 15.68s/it] 19%|█▊        | 112/604 [30:18<2:07:38, 15.57s/it] 19%|█▊        | 113/604 [30:33<2:07:50, 15.62s/it] 19%|█▉        | 114/604 [30:49<2:07:28, 15.61s/it] 19%|█▉        | 115/604 [31:05<2:07:31, 15.65s/it] 19%|█▉        | 116/604 [31:20<2:06:32, 15.56s/it] 19%|█▉        | 117/604 [31:36<2:07:18, 15.69s/it] 20%|█▉        | 118/604 [31:52<2:07:17, 15.72s/it] 20%|█▉        | 119/604 [32:08<2:07:28, 15.77s/it] 20%|█▉        | 120/604 [32:23<2:06:58, 15.74s/it]                                                   {'loss': 2.1814, 'learning_rate': 8.854410858528194e-06, 'epoch': 0.2} 20%|█▉        | 120/604 [32:23<2:06:58, 15.74s/it] 20%|██        | 121/604 [32:39<2:06:53, 15.76s/it] 20%|██        | 122/604 [32:56<2:09:31, 16.12s/it] 20%|██        | 123/604 [33:12<2:08:10, 15.99s/it] 21%|██        | 124/604 [33:28<2:07:23, 15.92s/it] 21%|██        | 125/604 [33:43<2:06:06, 15.80s/it] 21%|██        | 126/604 [33:59<2:06:01, 15.82s/it] 21%|██        | 127/604 [34:15<2:05:07, 15.74s/it] 21%|██        | 128/604 [34:31<2:05:35, 15.83s/it] 21%|██▏       | 129/604 [34:47<2:05:34, 15.86s/it] 22%|██▏       | 130/604 [35:02<2:05:11, 15.85s/it]                                                   {'loss': 2.173, 'learning_rate': 9.020077814299048e-06, 'epoch': 0.22} 22%|██▏       | 130/604 [35:02<2:05:11, 15.85s/it] 22%|██▏       | 131/604 [35:18<2:04:41, 15.82s/it] 22%|██▏       | 132/604 [35:34<2:03:39, 15.72s/it] 22%|██▏       | 133/604 [35:50<2:03:48, 15.77s/it] 22%|██▏       | 134/604 [36:06<2:04:37, 15.91s/it] 22%|██▏       | 135/604 [36:22<2:04:40, 15.95s/it] 23%|██▎       | 136/604 [36:37<2:03:25, 15.82s/it] 23%|██▎       | 137/604 [36:54<2:04:05, 15.94s/it] 23%|██▎       | 138/604 [37:10<2:03:51, 15.95s/it] 23%|██▎       | 139/604 [37:26<2:04:18, 16.04s/it] 23%|██▎       | 140/604 [37:42<2:03:54, 16.02s/it]                                                   {'loss': 2.1728, 'learning_rate': 9.172369392299995e-06, 'epoch': 0.23} 23%|██▎       | 140/604 [37:42<2:03:54, 16.02s/it] 23%|██▎       | 141/604 [37:58<2:03:13, 15.97s/it] 24%|██▎       | 142/604 [38:13<2:01:57, 15.84s/it] 24%|██▎       | 143/604 [38:29<2:01:36, 15.83s/it] 24%|██▍       | 144/604 [38:45<2:01:34, 15.86s/it] 24%|██▍       | 145/604 [39:01<2:01:16, 15.85s/it] 24%|██▍       | 146/604 [39:17<2:00:53, 15.84s/it] 24%|██▍       | 147/604 [39:32<2:00:39, 15.84s/it] 25%|██▍       | 148/604 [39:48<2:00:20, 15.83s/it] 25%|██▍       | 149/604 [40:04<1:59:16, 15.73s/it] 25%|██▍       | 150/604 [40:19<1:59:04, 15.74s/it]                                                   {'loss': 2.1796, 'learning_rate': 9.313284938885426e-06, 'epoch': 0.25} 25%|██▍       | 150/604 [40:19<1:59:04, 15.74s/it] 25%|██▌       | 151/604 [40:35<1:59:09, 15.78s/it] 25%|██▌       | 152/604 [40:51<1:58:55, 15.79s/it] 25%|██▌       | 153/604 [41:07<1:58:19, 15.74s/it] 25%|██▌       | 154/604 [41:23<1:58:39, 15.82s/it] 26%|██▌       | 155/604 [41:39<1:59:08, 15.92s/it] 26%|██▌       | 156/604 [41:55<1:59:16, 15.97s/it] 26%|██▌       | 157/604 [42:11<1:58:04, 15.85s/it] 26%|██▌       | 158/604 [42:27<1:58:11, 15.90s/it] 26%|██▋       | 159/604 [42:43<1:57:59, 15.91s/it] 26%|██▋       | 160/604 [42:59<1:57:55, 15.94s/it]                                                   {'loss': 2.1342, 'learning_rate': 9.44440651580982e-06, 'epoch': 0.26} 26%|██▋       | 160/604 [42:59<1:57:55, 15.94s/it] 27%|██▋       | 161/604 [43:14<1:57:34, 15.92s/it] 27%|██▋       | 162/604 [43:30<1:57:01, 15.89s/it] 27%|██▋       | 163/604 [43:46<1:56:05, 15.79s/it] 27%|██▋       | 164/604 [44:02<1:56:31, 15.89s/it] 27%|██▋       | 165/604 [44:18<1:55:47, 15.83s/it] 27%|██▋       | 166/604 [44:33<1:55:27, 15.82s/it] 28%|██▊       | 167/604 [44:49<1:54:36, 15.74s/it] 28%|██▊       | 168/604 [45:05<1:55:17, 15.87s/it] 28%|██▊       | 169/604 [45:21<1:54:55, 15.85s/it] 28%|██▊       | 170/604 [45:37<1:54:48, 15.87s/it]                                                   {'loss': 2.1276, 'learning_rate': 9.567007507371588e-06, 'epoch': 0.28} 28%|██▊       | 170/604 [45:37<1:54:48, 15.87s/it] 28%|██▊       | 171/604 [45:52<1:53:24, 15.72s/it] 28%|██▊       | 172/604 [46:08<1:53:48, 15.81s/it] 29%|██▊       | 173/604 [46:24<1:53:05, 15.74s/it] 29%|██▉       | 174/604 [46:39<1:52:37, 15.72s/it] 29%|██▉       | 175/604 [46:55<1:52:05, 15.68s/it] 29%|██▉       | 176/604 [47:11<1:52:49, 15.82s/it] 29%|██▉       | 177/604 [47:27<1:51:43, 15.70s/it] 29%|██▉       | 178/604 [47:42<1:51:30, 15.71s/it] 30%|██▉       | 179/604 [47:58<1:51:41, 15.77s/it] 30%|██▉       | 180/604 [48:14<1:51:40, 15.80s/it]                                                   {'loss': 2.1283, 'learning_rate': 9.682128041841535e-06, 'epoch': 0.3} 30%|██▉       | 180/604 [48:14<1:51:40, 15.80s/it] 30%|██▉       | 181/604 [48:30<1:51:15, 15.78s/it] 30%|███       | 182/604 [48:46<1:52:25, 15.99s/it] 30%|███       | 183/604 [49:02<1:52:26, 16.02s/it] 30%|███       | 184/604 [49:18<1:51:35, 15.94s/it] 31%|███       | 185/604 [49:34<1:51:31, 15.97s/it] 31%|███       | 186/604 [49:50<1:50:31, 15.86s/it] 31%|███       | 187/604 [50:06<1:50:39, 15.92s/it] 31%|███       | 188/604 [50:22<1:50:31, 15.94s/it] 31%|███▏      | 189/604 [50:38<1:49:52, 15.89s/it] 31%|███▏      | 190/604 [50:53<1:49:24, 15.86s/it]                                                   {'loss': 2.1266, 'learning_rate': 9.79062869769246e-06, 'epoch': 0.31} 31%|███▏      | 190/604 [50:53<1:49:24, 15.86s/it] 32%|███▏      | 191/604 [51:09<1:48:14, 15.73s/it] 32%|███▏      | 192/604 [51:25<1:48:30, 15.80s/it] 32%|███▏      | 193/604 [51:41<1:49:13, 15.95s/it] 32%|███▏      | 194/604 [51:57<1:48:42, 15.91s/it] 32%|███▏      | 195/604 [52:13<1:48:35, 15.93s/it] 32%|███▏      | 196/604 [52:29<1:47:54, 15.87s/it] 33%|███▎      | 197/604 [52:44<1:47:08, 15.79s/it] 33%|███▎      | 198/604 [53:00<1:46:50, 15.79s/it] 33%|███▎      | 199/604 [53:16<1:46:18, 15.75s/it] 33%|███▎      | 200/604 [53:31<1:45:38, 15.69s/it]                                                   {'loss': 2.12, 'learning_rate': 9.893229590500635e-06, 'epoch': 0.33} 33%|███▎      | 200/604 [53:31<1:45:38, 15.69s/it] 33%|███▎      | 201/604 [54:41<3:34:58, 32.00s/it] 33%|███▎      | 202/604 [54:57<3:02:33, 27.25s/it] 34%|███▎      | 203/604 [55:13<2:37:55, 23.63s/it] 34%|███▍      | 204/604 [55:28<2:21:20, 21.20s/it] 34%|███▍      | 205/604 [55:43<2:08:59, 19.40s/it] 34%|███▍      | 206/604 [55:59<2:00:50, 18.22s/it] 34%|███▍      | 207/604 [56:14<1:54:44, 17.34s/it] 34%|███▍      | 208/604 [56:30<1:50:37, 16.76s/it] 35%|███▍      | 209/604 [56:45<1:47:29, 16.33s/it] 35%|███▍      | 210/604 [57:00<1:45:36, 16.08s/it]                                                   {'loss': 2.1342, 'learning_rate': 9.990539370375978e-06, 'epoch': 0.35} 35%|███▍      | 210/604 [57:00<1:45:36, 16.08s/it] 35%|███▍      | 211/604 [57:16<1:44:03, 15.89s/it] 35%|███▌      | 212/604 [57:31<1:42:31, 15.69s/it] 35%|███▌      | 213/604 [57:46<1:41:38, 15.60s/it] 35%|███▌      | 214/604 [58:02<1:41:04, 15.55s/it] 36%|███▌      | 215/604 [58:17<1:40:18, 15.47s/it] 36%|███▌      | 216/604 [58:33<1:40:14, 15.50s/it] 36%|███▌      | 217/604 [58:48<1:39:47, 15.47s/it] 36%|███▌      | 218/604 [59:04<1:40:37, 15.64s/it] 36%|███▋      | 219/604 [59:20<1:40:58, 15.74s/it] 36%|███▋      | 220/604 [59:36<1:40:53, 15.76s/it]                                                   {'loss': 2.1249, 'learning_rate': 1e-05, 'epoch': 0.36} 36%|███▋      | 220/604 [59:36<1:40:53, 15.76s/it] 37%|███▋      | 221/604 [59:52<1:41:44, 15.94s/it] 37%|███▋      | 222/604 [1:00:08<1:41:15, 15.90s/it] 37%|███▋      | 223/604 [1:00:24<1:41:19, 15.96s/it] 37%|███▋      | 224/604 [1:00:40<1:40:57, 15.94s/it] 37%|███▋      | 225/604 [1:00:56<1:40:13, 15.87s/it] 37%|███▋      | 226/604 [1:01:11<1:39:16, 15.76s/it] 38%|███▊      | 227/604 [1:01:27<1:39:27, 15.83s/it] 38%|███▊      | 228/604 [1:01:43<1:38:54, 15.78s/it] 38%|███▊      | 229/604 [1:01:59<1:38:27, 15.75s/it] 38%|███▊      | 230/604 [1:02:14<1:37:47, 15.69s/it]                                                     {'loss': 2.1123, 'learning_rate': 1e-05, 'epoch': 0.38} 38%|███▊      | 230/604 [1:02:14<1:37:47, 15.69s/it] 38%|███▊      | 231/604 [1:02:30<1:37:24, 15.67s/it] 38%|███▊      | 232/604 [1:02:46<1:37:26, 15.72s/it] 39%|███▊      | 233/604 [1:03:01<1:37:04, 15.70s/it] 39%|███▊      | 234/604 [1:03:17<1:37:07, 15.75s/it] 39%|███▉      | 235/604 [1:03:33<1:36:57, 15.77s/it] 39%|███▉      | 236/604 [1:03:49<1:37:04, 15.83s/it] 39%|███▉      | 237/604 [1:04:04<1:36:00, 15.70s/it] 39%|███▉      | 238/604 [1:04:20<1:35:35, 15.67s/it] 40%|███▉      | 239/604 [1:04:36<1:35:56, 15.77s/it] 40%|███▉      | 240/604 [1:04:52<1:36:09, 15.85s/it]                                                     {'loss': 2.1144, 'learning_rate': 1e-05, 'epoch': 0.4} 40%|███▉      | 240/604 [1:04:52<1:36:09, 15.85s/it] 40%|███▉      | 241/604 [1:05:08<1:35:35, 15.80s/it] 40%|████      | 242/604 [1:05:23<1:35:12, 15.78s/it] 40%|████      | 243/604 [1:05:40<1:35:44, 15.91s/it] 40%|████      | 244/604 [1:05:55<1:35:07, 15.85s/it] 41%|████      | 245/604 [1:06:11<1:34:56, 15.87s/it] 41%|████      | 246/604 [1:06:27<1:35:14, 15.96s/it] 41%|████      | 247/604 [1:06:44<1:36:26, 16.21s/it] 41%|████      | 248/604 [1:07:01<1:36:36, 16.28s/it] 41%|████      | 249/604 [1:07:16<1:34:53, 16.04s/it] 41%|████▏     | 250/604 [1:07:32<1:33:59, 15.93s/it]                                                     {'loss': 2.0951, 'learning_rate': 1e-05, 'epoch': 0.41} 41%|████▏     | 250/604 [1:07:32<1:33:59, 15.93s/it] 42%|████▏     | 251/604 [1:07:48<1:33:33, 15.90s/it] 42%|████▏     | 252/604 [1:08:03<1:32:42, 15.80s/it] 42%|████▏     | 253/604 [1:08:19<1:32:17, 15.78s/it] 42%|████▏     | 254/604 [1:08:35<1:32:06, 15.79s/it] 42%|████▏     | 255/604 [1:08:50<1:31:44, 15.77s/it] 42%|████▏     | 256/604 [1:09:07<1:32:23, 15.93s/it] 43%|████▎     | 257/604 [1:09:23<1:32:08, 15.93s/it] 43%|████▎     | 258/604 [1:09:39<1:31:56, 15.94s/it] 43%|████▎     | 259/604 [1:09:55<1:31:49, 15.97s/it] 43%|████▎     | 260/604 [1:10:11<1:32:08, 16.07s/it]                                                     {'loss': 2.0968, 'learning_rate': 1e-05, 'epoch': 0.43} 43%|████▎     | 260/604 [1:10:11<1:32:08, 16.07s/it] 43%|████▎     | 261/604 [1:10:27<1:31:42, 16.04s/it] 43%|████▎     | 262/604 [1:10:43<1:31:06, 15.98s/it] 44%|████▎     | 263/604 [1:10:58<1:30:12, 15.87s/it] 44%|████▎     | 264/604 [1:11:15<1:30:46, 16.02s/it] 44%|████▍     | 265/604 [1:11:31<1:31:16, 16.15s/it] 44%|████▍     | 266/604 [1:11:47<1:30:25, 16.05s/it] 44%|████▍     | 267/604 [1:12:03<1:29:13, 15.89s/it] 44%|████▍     | 268/604 [1:12:18<1:28:48, 15.86s/it] 45%|████▍     | 269/604 [1:12:35<1:29:19, 16.00s/it] 45%|████▍     | 270/604 [1:12:51<1:29:25, 16.06s/it]                                                     {'loss': 2.1008, 'learning_rate': 1e-05, 'epoch': 0.45} 45%|████▍     | 270/604 [1:12:51<1:29:25, 16.06s/it] 45%|████▍     | 271/604 [1:13:06<1:28:08, 15.88s/it] 45%|████▌     | 272/604 [1:13:22<1:27:17, 15.78s/it] 45%|████▌     | 273/604 [1:13:38<1:27:39, 15.89s/it] 45%|████▌     | 274/604 [1:13:54<1:27:28, 15.90s/it] 46%|████▌     | 275/604 [1:14:10<1:26:42, 15.81s/it] 46%|████▌     | 276/604 [1:14:25<1:26:00, 15.73s/it] 46%|████▌     | 277/604 [1:14:41<1:26:28, 15.87s/it] 46%|████▌     | 278/604 [1:14:57<1:26:14, 15.87s/it] 46%|████▌     | 279/604 [1:15:13<1:25:49, 15.84s/it] 46%|████▋     | 280/604 [1:15:29<1:25:42, 15.87s/it]                                                     {'loss': 2.1007, 'learning_rate': 1e-05, 'epoch': 0.46} 46%|████▋     | 280/604 [1:15:29<1:25:42, 15.87s/it] 47%|████▋     | 281/604 [1:15:45<1:25:13, 15.83s/it] 47%|████▋     | 282/604 [1:16:01<1:25:00, 15.84s/it] 47%|████▋     | 283/604 [1:16:16<1:24:52, 15.87s/it] 47%|████▋     | 284/604 [1:16:32<1:24:43, 15.88s/it] 47%|████▋     | 285/604 [1:16:48<1:24:17, 15.86s/it] 47%|████▋     | 286/604 [1:17:04<1:23:50, 15.82s/it] 48%|████▊     | 287/604 [1:17:20<1:23:45, 15.85s/it] 48%|████▊     | 288/604 [1:17:35<1:22:54, 15.74s/it] 48%|████▊     | 289/604 [1:17:51<1:22:31, 15.72s/it] 48%|████▊     | 290/604 [1:18:07<1:22:14, 15.72s/it]                                                     {'loss': 2.0891, 'learning_rate': 1e-05, 'epoch': 0.48} 48%|████▊     | 290/604 [1:18:07<1:22:14, 15.72s/it] 48%|████▊     | 291/604 [1:18:23<1:22:22, 15.79s/it] 48%|████▊     | 292/604 [1:18:38<1:21:52, 15.74s/it] 49%|████▊     | 293/604 [1:18:54<1:21:19, 15.69s/it] 49%|████▊     | 294/604 [1:19:09<1:20:47, 15.64s/it] 49%|████▉     | 295/604 [1:19:25<1:20:02, 15.54s/it] 49%|████▉     | 296/604 [1:19:40<1:19:58, 15.58s/it] 49%|████▉     | 297/604 [1:19:56<1:19:31, 15.54s/it] 49%|████▉     | 298/604 [1:20:11<1:18:55, 15.48s/it] 50%|████▉     | 299/604 [1:20:27<1:18:55, 15.53s/it] 50%|████▉     | 300/604 [1:20:42<1:18:57, 15.58s/it]                                                     {'loss': 2.0924, 'learning_rate': 1e-05, 'epoch': 0.5} 50%|████▉     | 300/604 [1:20:42<1:18:57, 15.58s/it] 50%|████▉     | 301/604 [1:21:48<2:34:26, 30.58s/it] 50%|█████     | 302/604 [1:22:04<2:12:07, 26.25s/it] 50%|█████     | 303/604 [1:22:20<1:55:18, 22.99s/it] 50%|█████     | 304/604 [1:22:35<1:43:09, 20.63s/it] 50%|█████     | 305/604 [1:22:50<1:34:58, 19.06s/it] 51%|█████     | 306/604 [1:23:06<1:29:24, 18.00s/it] 51%|█████     | 307/604 [1:23:21<1:25:04, 17.19s/it] 51%|█████     | 308/604 [1:23:36<1:22:15, 16.67s/it] 51%|█████     | 309/604 [1:23:52<1:20:00, 16.27s/it] 51%|█████▏    | 310/604 [1:24:07<1:18:14, 15.97s/it]                                                     {'loss': 2.1062, 'learning_rate': 1e-05, 'epoch': 0.51} 51%|█████▏    | 310/604 [1:24:07<1:18:14, 15.97s/it] 51%|█████▏    | 311/604 [1:24:22<1:17:18, 15.83s/it] 52%|█████▏    | 312/604 [1:24:38<1:16:44, 15.77s/it] 52%|█████▏    | 313/604 [1:24:54<1:16:21, 15.74s/it] 52%|█████▏    | 314/604 [1:25:09<1:15:36, 15.64s/it] 52%|█████▏    | 315/604 [1:25:25<1:15:07, 15.60s/it] 52%|█████▏    | 316/604 [1:25:40<1:14:43, 15.57s/it] 52%|█████▏    | 317/604 [1:25:55<1:14:01, 15.48s/it] 53%|█████▎    | 318/604 [1:26:11<1:14:08, 15.55s/it] 53%|█████▎    | 319/604 [1:26:27<1:13:56, 15.57s/it] 53%|█████▎    | 320/604 [1:26:42<1:13:00, 15.43s/it]                                                     {'loss': 2.086, 'learning_rate': 1e-05, 'epoch': 0.53} 53%|█████▎    | 320/604 [1:26:42<1:13:00, 15.43s/it] 53%|█████▎    | 321/604 [1:26:57<1:12:58, 15.47s/it] 53%|█████▎    | 322/604 [1:27:13<1:12:50, 15.50s/it] 53%|█████▎    | 323/604 [1:27:29<1:12:53, 15.57s/it] 54%|█████▎    | 324/604 [1:27:44<1:12:26, 15.52s/it] 54%|█████▍    | 325/604 [1:27:59<1:11:50, 15.45s/it] 54%|█████▍    | 326/604 [1:28:15<1:12:02, 15.55s/it] 54%|█████▍    | 327/604 [1:28:31<1:12:18, 15.66s/it] 54%|█████▍    | 328/604 [1:28:47<1:11:41, 15.58s/it] 54%|█████▍    | 329/604 [1:29:02<1:11:26, 15.59s/it] 55%|█████▍    | 330/604 [1:29:18<1:11:20, 15.62s/it]                                                     {'loss': 2.0991, 'learning_rate': 1e-05, 'epoch': 0.55} 55%|█████▍    | 330/604 [1:29:18<1:11:20, 15.62s/it] 55%|█████▍    | 331/604 [1:29:33<1:10:59, 15.60s/it] 55%|█████▍    | 332/604 [1:29:49<1:10:55, 15.64s/it] 55%|█████▌    | 333/604 [1:30:05<1:10:32, 15.62s/it] 55%|█████▌    | 334/604 [1:30:20<1:10:20, 15.63s/it] 55%|█████▌    | 335/604 [1:30:36<1:10:42, 15.77s/it] 56%|█████▌    | 336/604 [1:30:52<1:10:37, 15.81s/it] 56%|█████▌    | 337/604 [1:31:08<1:10:22, 15.82s/it] 56%|█████▌    | 338/604 [1:31:24<1:10:00, 15.79s/it] 56%|█████▌    | 339/604 [1:31:40<1:09:50, 15.81s/it] 56%|█████▋    | 340/604 [1:31:55<1:09:20, 15.76s/it]                                                     {'loss': 2.1011, 'learning_rate': 1e-05, 'epoch': 0.56} 56%|█████▋    | 340/604 [1:31:55<1:09:20, 15.76s/it] 56%|█████▋    | 341/604 [1:32:11<1:09:09, 15.78s/it] 57%|█████▋    | 342/604 [1:32:27<1:08:53, 15.78s/it] 57%|█████▋    | 343/604 [1:32:43<1:08:28, 15.74s/it] 57%|█████▋    | 344/604 [1:32:59<1:08:21, 15.77s/it] 57%|█████▋    | 345/604 [1:33:14<1:07:49, 15.71s/it] 57%|█████▋    | 346/604 [1:33:30<1:07:30, 15.70s/it] 57%|█████▋    | 347/604 [1:33:46<1:07:25, 15.74s/it] 58%|█████▊    | 348/604 [1:34:02<1:07:45, 15.88s/it] 58%|█████▊    | 349/604 [1:34:18<1:07:38, 15.91s/it] 58%|█████▊    | 350/604 [1:34:33<1:06:53, 15.80s/it]                                                     {'loss': 2.1142, 'learning_rate': 1e-05, 'epoch': 0.58} 58%|█████▊    | 350/604 [1:34:33<1:06:53, 15.80s/it] 58%|█████▊    | 351/604 [1:34:49<1:06:54, 15.87s/it] 58%|█████▊    | 352/604 [1:35:05<1:06:29, 15.83s/it] 58%|█████▊    | 353/604 [1:35:21<1:05:54, 15.76s/it] 59%|█████▊    | 354/604 [1:35:37<1:05:42, 15.77s/it] 59%|█████▉    | 355/604 [1:35:52<1:05:00, 15.66s/it] 59%|█████▉    | 356/604 [1:36:08<1:04:42, 15.66s/it] 59%|█████▉    | 357/604 [1:36:23<1:04:24, 15.65s/it] 59%|█████▉    | 358/604 [1:36:39<1:04:07, 15.64s/it] 59%|█████▉    | 359/604 [1:36:54<1:03:44, 15.61s/it] 60%|█████▉    | 360/604 [1:37:10<1:03:10, 15.53s/it]                                                     {'loss': 2.1132, 'learning_rate': 1e-05, 'epoch': 0.6} 60%|█████▉    | 360/604 [1:37:10<1:03:10, 15.53s/it] 60%|█████▉    | 361/604 [1:37:25<1:02:39, 15.47s/it] 60%|█████▉    | 362/604 [1:37:40<1:02:14, 15.43s/it] 60%|██████    | 363/604 [1:37:56<1:01:56, 15.42s/it] 60%|██████    | 364/604 [1:38:11<1:01:26, 15.36s/it] 60%|██████    | 365/604 [1:38:27<1:01:26, 15.42s/it] 61%|██████    | 366/604 [1:38:42<1:01:07, 15.41s/it] 61%|██████    | 367/604 [1:38:57<1:00:33, 15.33s/it] 61%|██████    | 368/604 [1:39:12<1:00:16, 15.33s/it] 61%|██████    | 369/604 [1:39:28<1:00:04, 15.34s/it] 61%|██████▏   | 370/604 [1:39:43<59:53, 15.36s/it]                                                     {'loss': 2.0926, 'learning_rate': 1e-05, 'epoch': 0.61} 61%|██████▏   | 370/604 [1:39:43<59:53, 15.36s/it] 61%|██████▏   | 371/604 [1:39:59<59:38, 15.36s/it] 62%|██████▏   | 372/604 [1:40:14<59:42, 15.44s/it] 62%|██████▏   | 373/604 [1:40:30<59:41, 15.51s/it] 62%|██████▏   | 374/604 [1:40:45<59:25, 15.50s/it] 62%|██████▏   | 375/604 [1:41:01<59:15, 15.53s/it] 62%|██████▏   | 376/604 [1:41:17<59:24, 15.63s/it] 62%|██████▏   | 377/604 [1:41:32<59:05, 15.62s/it] 63%|██████▎   | 378/604 [1:41:48<58:28, 15.52s/it] 63%|██████▎   | 379/604 [1:42:03<57:54, 15.44s/it] 63%|██████▎   | 380/604 [1:42:18<57:39, 15.44s/it]                                                   {'loss': 2.0831, 'learning_rate': 1e-05, 'epoch': 0.63} 63%|██████▎   | 380/604 [1:42:18<57:39, 15.44s/it] 63%|██████▎   | 381/604 [1:42:34<57:15, 15.41s/it] 63%|██████▎   | 382/604 [1:42:49<57:22, 15.51s/it] 63%|██████▎   | 383/604 [1:43:05<56:47, 15.42s/it] 64%|██████▎   | 384/604 [1:43:20<56:37, 15.44s/it] 64%|██████▎   | 385/604 [1:43:36<56:27, 15.47s/it] 64%|██████▍   | 386/604 [1:43:51<56:15, 15.48s/it] 64%|██████▍   | 387/604 [1:44:06<55:46, 15.42s/it] 64%|██████▍   | 388/604 [1:44:22<55:45, 15.49s/it] 64%|██████▍   | 389/604 [1:44:38<55:36, 15.52s/it] 65%|██████▍   | 390/604 [1:44:54<55:52, 15.66s/it]                                                   {'loss': 2.0907, 'learning_rate': 1e-05, 'epoch': 0.65} 65%|██████▍   | 390/604 [1:44:54<55:52, 15.66s/it] 65%|██████▍   | 391/604 [1:45:09<55:15, 15.57s/it] 65%|██████▍   | 392/604 [1:45:24<54:48, 15.51s/it] 65%|██████▌   | 393/604 [1:45:40<54:53, 15.61s/it] 65%|██████▌   | 394/604 [1:45:56<54:29, 15.57s/it] 65%|██████▌   | 395/604 [1:46:11<54:09, 15.55s/it] 66%|██████▌   | 396/604 [1:46:27<53:57, 15.56s/it] 66%|██████▌   | 397/604 [1:46:42<53:41, 15.56s/it] 66%|██████▌   | 398/604 [1:46:58<53:10, 15.49s/it] 66%|██████▌   | 399/604 [1:47:13<52:51, 15.47s/it] 66%|██████▌   | 400/604 [1:47:29<52:35, 15.47s/it]                                                   {'loss': 2.0898, 'learning_rate': 1e-05, 'epoch': 0.66} 66%|██████▌   | 400/604 [1:47:29<52:35, 15.47s/it] 66%|██████▋   | 401/604 [1:48:38<1:46:47, 31.57s/it] 67%|██████▋   | 402/604 [1:48:54<1:30:31, 26.89s/it] 67%|██████▋   | 403/604 [1:49:09<1:18:17, 23.37s/it] 67%|██████▋   | 404/604 [1:49:24<1:09:36, 20.88s/it] 67%|██████▋   | 405/604 [1:49:39<1:03:33, 19.16s/it] 67%|██████▋   | 406/604 [1:49:54<59:07, 17.91s/it]   67%|██████▋   | 407/604 [1:50:09<56:04, 17.08s/it] 68%|██████▊   | 408/604 [1:50:24<53:59, 16.53s/it] 68%|██████▊   | 409/604 [1:50:40<52:28, 16.15s/it] 68%|██████▊   | 410/604 [1:50:55<50:59, 15.77s/it]                                                   {'loss': 2.076, 'learning_rate': 1e-05, 'epoch': 0.68} 68%|██████▊   | 410/604 [1:50:55<50:59, 15.77s/it] 68%|██████▊   | 411/604 [1:51:09<49:50, 15.49s/it] 68%|██████▊   | 412/604 [1:51:24<49:03, 15.33s/it] 68%|██████▊   | 413/604 [1:51:39<48:29, 15.23s/it] 69%|██████▊   | 414/604 [1:51:54<48:05, 15.19s/it] 69%|██████▊   | 415/604 [1:52:10<47:43, 15.15s/it] 69%|██████▉   | 416/604 [1:52:25<47:21, 15.12s/it] 69%|██████▉   | 417/604 [1:52:39<46:53, 15.05s/it] 69%|██████▉   | 418/604 [1:52:55<46:48, 15.10s/it] 69%|██████▉   | 419/604 [1:53:10<46:31, 15.09s/it] 70%|██████▉   | 420/604 [1:53:25<46:23, 15.13s/it]                                                   {'loss': 2.0817, 'learning_rate': 1e-05, 'epoch': 0.7} 70%|██████▉   | 420/604 [1:53:25<46:23, 15.13s/it] 70%|██████▉   | 421/604 [1:53:40<45:59, 15.08s/it] 70%|██████▉   | 422/604 [1:53:55<45:42, 15.07s/it] 70%|███████   | 423/604 [1:54:10<45:32, 15.10s/it] 70%|███████   | 424/604 [1:54:25<45:12, 15.07s/it] 70%|███████   | 425/604 [1:54:40<45:10, 15.14s/it] 71%|███████   | 426/604 [1:54:56<45:05, 15.20s/it] 71%|███████   | 427/604 [1:55:11<44:53, 15.22s/it] 71%|███████   | 428/604 [1:55:27<44:54, 15.31s/it] 71%|███████   | 429/604 [1:55:42<44:50, 15.37s/it] 71%|███████   | 430/604 [1:55:58<44:40, 15.41s/it]                                                   {'loss': 2.0891, 'learning_rate': 1e-05, 'epoch': 0.71} 71%|███████   | 430/604 [1:55:58<44:40, 15.41s/it] 71%|███████▏  | 431/604 [1:56:13<44:14, 15.34s/it] 72%|███████▏  | 432/604 [1:56:28<43:59, 15.34s/it] 72%|███████▏  | 433/604 [1:56:43<43:39, 15.32s/it] 72%|███████▏  | 434/604 [1:56:59<43:27, 15.34s/it] 72%|███████▏  | 435/604 [1:57:14<43:01, 15.28s/it] 72%|███████▏  | 436/604 [1:57:29<42:56, 15.33s/it] 72%|███████▏  | 437/604 [1:57:45<42:37, 15.32s/it] 73%|███████▎  | 438/604 [1:58:00<42:22, 15.31s/it] 73%|███████▎  | 439/604 [1:58:15<42:15, 15.37s/it] 73%|███████▎  | 440/604 [1:58:31<42:01, 15.38s/it]                                                   {'loss': 2.0791, 'learning_rate': 1e-05, 'epoch': 0.73} 73%|███████▎  | 440/604 [1:58:31<42:01, 15.38s/it] 73%|███████▎  | 441/604 [1:58:46<41:54, 15.43s/it] 73%|███████▎  | 442/604 [1:59:02<41:41, 15.44s/it] 73%|███████▎  | 443/604 [1:59:17<41:17, 15.39s/it] 74%|███████▎  | 444/604 [1:59:33<41:08, 15.43s/it] 74%|███████▎  | 445/604 [1:59:48<41:08, 15.52s/it] 74%|███████▍  | 446/604 [2:00:04<40:58, 15.56s/it] 74%|███████▍  | 447/604 [2:00:20<40:51, 15.61s/it] 74%|███████▍  | 448/604 [2:00:35<40:34, 15.61s/it] 74%|███████▍  | 449/604 [2:00:51<40:18, 15.61s/it] 75%|███████▍  | 450/604 [2:01:06<39:52, 15.53s/it]                                                   {'loss': 2.0857, 'learning_rate': 1e-05, 'epoch': 0.75} 75%|███████▍  | 450/604 [2:01:06<39:52, 15.53s/it] 75%|███████▍  | 451/604 [2:01:22<39:22, 15.44s/it] 75%|███████▍  | 452/604 [2:01:37<39:04, 15.42s/it] 75%|███████▌  | 453/604 [2:01:52<38:45, 15.40s/it] 75%|███████▌  | 454/604 [2:02:07<38:20, 15.34s/it] 75%|███████▌  | 455/604 [2:02:23<38:05, 15.34s/it] 75%|███████▌  | 456/604 [2:02:38<37:36, 15.25s/it] 76%|███████▌  | 457/604 [2:02:53<37:22, 15.26s/it] 76%|███████▌  | 458/604 [2:03:08<37:06, 15.25s/it] 76%|███████▌  | 459/604 [2:03:24<36:45, 15.21s/it] 76%|███████▌  | 460/604 [2:03:39<36:23, 15.16s/it]                                                   {'loss': 2.0762, 'learning_rate': 1e-05, 'epoch': 0.76} 76%|███████▌  | 460/604 [2:03:39<36:23, 15.16s/it] 76%|███████▋  | 461/604 [2:03:54<36:16, 15.22s/it] 76%|███████▋  | 462/604 [2:04:09<36:11, 15.29s/it] 77%|███████▋  | 463/604 [2:04:25<35:54, 15.28s/it] 77%|███████▋  | 464/604 [2:04:40<35:42, 15.30s/it] 77%|███████▋  | 465/604 [2:04:56<35:40, 15.40s/it] 77%|███████▋  | 466/604 [2:05:11<35:19, 15.36s/it] 77%|███████▋  | 467/604 [2:05:26<34:58, 15.32s/it] 77%|███████▋  | 468/604 [2:05:41<34:42, 15.31s/it] 78%|███████▊  | 469/604 [2:05:57<34:36, 15.38s/it] 78%|███████▊  | 470/604 [2:06:13<34:30, 15.45s/it]                                                   {'loss': 2.0851, 'learning_rate': 1e-05, 'epoch': 0.78} 78%|███████▊  | 470/604 [2:06:13<34:30, 15.45s/it] 78%|███████▊  | 471/604 [2:06:28<34:13, 15.44s/it] 78%|███████▊  | 472/604 [2:06:44<34:02, 15.47s/it] 78%|███████▊  | 473/604 [2:06:59<34:05, 15.61s/it] 78%|███████▊  | 474/604 [2:07:15<33:40, 15.54s/it] 79%|███████▊  | 475/604 [2:07:30<33:17, 15.49s/it] 79%|███████▉  | 476/604 [2:07:46<33:03, 15.49s/it] 79%|███████▉  | 477/604 [2:08:01<32:44, 15.46s/it] 79%|███████▉  | 478/604 [2:08:16<32:18, 15.38s/it] 79%|███████▉  | 479/604 [2:08:32<32:05, 15.41s/it] 79%|███████▉  | 480/604 [2:08:47<31:50, 15.41s/it]                                                   {'loss': 2.0749, 'learning_rate': 1e-05, 'epoch': 0.79} 79%|███████▉  | 480/604 [2:08:47<31:50, 15.41s/it] 80%|███████▉  | 481/604 [2:09:02<31:31, 15.38s/it] 80%|███████▉  | 482/604 [2:09:18<31:10, 15.34s/it] 80%|███████▉  | 483/604 [2:09:34<31:19, 15.53s/it] 80%|████████  | 484/604 [2:09:50<31:23, 15.69s/it] 80%|████████  | 485/604 [2:10:06<31:09, 15.71s/it] 80%|████████  | 486/604 [2:10:21<30:47, 15.65s/it] 81%|████████  | 487/604 [2:10:37<30:29, 15.64s/it] 81%|████████  | 488/604 [2:10:53<30:31, 15.79s/it] 81%|████████  | 489/604 [2:11:08<30:13, 15.77s/it] 81%|████████  | 490/604 [2:11:24<29:58, 15.78s/it]                                                   {'loss': 2.0752, 'learning_rate': 1e-05, 'epoch': 0.81} 81%|████████  | 490/604 [2:11:24<29:58, 15.78s/it] 81%|████████▏ | 491/604 [2:11:40<29:38, 15.74s/it] 81%|████████▏ | 492/604 [2:11:55<29:14, 15.66s/it] 82%|████████▏ | 493/604 [2:12:11<28:57, 15.65s/it] 82%|████████▏ | 494/604 [2:12:27<28:55, 15.78s/it] 82%|████████▏ | 495/604 [2:12:43<28:38, 15.77s/it] 82%|████████▏ | 496/604 [2:12:59<28:21, 15.76s/it] 82%|████████▏ | 497/604 [2:13:14<27:53, 15.64s/it] 82%|████████▏ | 498/604 [2:13:30<27:36, 15.62s/it] 83%|████████▎ | 499/604 [2:13:45<27:18, 15.61s/it] 83%|████████▎ | 500/604 [2:14:01<27:07, 15.65s/it]                                                   {'loss': 2.0864, 'learning_rate': 1e-05, 'epoch': 0.83} 83%|████████▎ | 500/604 [2:14:01<27:07, 15.65s/it] 83%|████████▎ | 501/604 [2:15:05<51:57, 30.27s/it] 83%|████████▎ | 502/604 [2:15:22<44:21, 26.09s/it] 83%|████████▎ | 503/604 [2:15:37<38:28, 22.86s/it] 83%|████████▎ | 504/604 [2:15:52<34:18, 20.59s/it] 84%|████████▎ | 505/604 [2:16:08<31:24, 19.03s/it] 84%|████████▍ | 506/604 [2:16:23<29:17, 17.93s/it] 84%|████████▍ | 507/604 [2:16:38<27:43, 17.15s/it] 84%|████████▍ | 508/604 [2:16:54<26:42, 16.69s/it] 84%|████████▍ | 509/604 [2:17:09<25:54, 16.36s/it] 84%|████████▍ | 510/604 [2:17:25<25:03, 16.00s/it]                                                   {'loss': 2.0791, 'learning_rate': 1e-05, 'epoch': 0.84} 84%|████████▍ | 510/604 [2:17:25<25:03, 16.00s/it] 85%|████████▍ | 511/604 [2:17:40<24:24, 15.75s/it] 85%|████████▍ | 512/604 [2:17:55<23:55, 15.60s/it] 85%|████████▍ | 513/604 [2:18:11<23:37, 15.58s/it] 85%|████████▌ | 514/604 [2:18:26<23:16, 15.52s/it] 85%|████████▌ | 515/604 [2:18:41<22:57, 15.47s/it] 85%|████████▌ | 516/604 [2:18:57<22:40, 15.46s/it] 86%|████████▌ | 517/604 [2:19:12<22:23, 15.44s/it] 86%|████████▌ | 518/604 [2:19:28<22:06, 15.42s/it] 86%|████████▌ | 519/604 [2:19:43<21:49, 15.41s/it] 86%|████████▌ | 520/604 [2:19:58<21:32, 15.38s/it]                                                   {'loss': 2.0543, 'learning_rate': 1e-05, 'epoch': 0.86} 86%|████████▌ | 520/604 [2:19:58<21:32, 15.38s/it] 86%|████████▋ | 521/604 [2:20:14<21:16, 15.38s/it] 86%|████████▋ | 522/604 [2:20:29<21:02, 15.40s/it] 87%|████████▋ | 523/604 [2:20:44<20:41, 15.33s/it] 87%|████████▋ | 524/604 [2:21:00<20:25, 15.31s/it] 87%|████████▋ | 525/604 [2:21:15<20:06, 15.27s/it] 87%|████████▋ | 526/604 [2:21:30<19:55, 15.33s/it] 87%|████████▋ | 527/604 [2:21:45<19:38, 15.30s/it] 87%|████████▋ | 528/604 [2:22:01<19:29, 15.39s/it] 88%|████████▊ | 529/604 [2:22:16<19:16, 15.42s/it] 88%|████████▊ | 530/604 [2:22:32<19:06, 15.49s/it]                                                   {'loss': 2.0846, 'learning_rate': 1e-05, 'epoch': 0.88} 88%|████████▊ | 530/604 [2:22:32<19:06, 15.49s/it] 88%|████████▊ | 531/604 [2:22:48<18:49, 15.48s/it] 88%|████████▊ | 532/604 [2:23:03<18:32, 15.46s/it] 88%|████████▊ | 533/604 [2:23:19<18:22, 15.53s/it] 88%|████████▊ | 534/604 [2:23:34<18:00, 15.43s/it] 89%|████████▊ | 535/604 [2:23:49<17:43, 15.41s/it] 89%|████████▊ | 536/604 [2:24:05<17:26, 15.40s/it] 89%|████████▉ | 537/604 [2:24:20<17:14, 15.44s/it] 89%|████████▉ | 538/604 [2:24:36<17:02, 15.50s/it] 89%|████████▉ | 539/604 [2:24:51<16:47, 15.49s/it] 89%|████████▉ | 540/604 [2:25:07<16:37, 15.59s/it]                                                   {'loss': 2.0539, 'learning_rate': 1e-05, 'epoch': 0.89} 89%|████████▉ | 540/604 [2:25:07<16:37, 15.59s/it] 90%|████████▉ | 541/604 [2:25:23<16:22, 15.60s/it] 90%|████████▉ | 542/604 [2:25:38<16:06, 15.60s/it] 90%|████████▉ | 543/604 [2:25:54<15:54, 15.64s/it] 90%|█████████ | 544/604 [2:26:10<15:37, 15.62s/it] 90%|█████████ | 545/604 [2:26:25<15:19, 15.58s/it] 90%|█████████ | 546/604 [2:26:41<15:00, 15.53s/it] 91%|█████████ | 547/604 [2:26:56<14:49, 15.60s/it] 91%|█████████ | 548/604 [2:27:12<14:32, 15.58s/it] 91%|█████████ | 549/604 [2:27:27<14:15, 15.55s/it] 91%|█████████ | 550/604 [2:27:43<14:03, 15.62s/it]                                                   {'loss': 2.051, 'learning_rate': 1e-05, 'epoch': 0.91} 91%|█████████ | 550/604 [2:27:43<14:03, 15.62s/it] 91%|█████████ | 551/604 [2:27:58<13:44, 15.55s/it] 91%|█████████▏| 552/604 [2:28:14<13:20, 15.40s/it] 92%|█████████▏| 553/604 [2:28:29<13:01, 15.33s/it] 92%|█████████▏| 554/604 [2:28:44<12:47, 15.36s/it] 92%|█████████▏| 555/604 [2:29:00<12:34, 15.40s/it] 92%|█████████▏| 556/604 [2:29:15<12:22, 15.47s/it] 92%|█████████▏| 557/604 [2:29:31<12:10, 15.55s/it] 92%|█████████▏| 558/604 [2:29:46<11:53, 15.52s/it] 93%|█████████▎| 559/604 [2:30:02<11:41, 15.58s/it] 93%|█████████▎| 560/604 [2:30:18<11:23, 15.53s/it]                                                   {'loss': 2.0436, 'learning_rate': 1e-05, 'epoch': 0.93} 93%|█████████▎| 560/604 [2:30:18<11:23, 15.53s/it] 93%|█████████▎| 561/604 [2:30:33<11:09, 15.57s/it] 93%|█████████▎| 562/604 [2:30:49<10:56, 15.64s/it] 93%|█████████▎| 563/604 [2:31:05<10:40, 15.61s/it] 93%|█████████▎| 564/604 [2:31:20<10:24, 15.61s/it] 94%|█████████▎| 565/604 [2:31:36<10:08, 15.61s/it] 94%|█████████▎| 566/604 [2:31:51<09:50, 15.55s/it] 94%|█████████▍| 567/604 [2:32:07<09:34, 15.54s/it] 94%|█████████▍| 568/604 [2:32:22<09:17, 15.49s/it] 94%|█████████▍| 569/604 [2:32:38<09:02, 15.51s/it] 94%|█████████▍| 570/604 [2:32:53<08:49, 15.57s/it]                                                   {'loss': 2.0555, 'learning_rate': 1e-05, 'epoch': 0.94} 94%|█████████▍| 570/604 [2:32:53<08:49, 15.57s/it] 95%|█████████▍| 571/604 [2:33:09<08:32, 15.52s/it] 95%|█████████▍| 572/604 [2:33:24<08:17, 15.54s/it] 95%|█████████▍| 573/604 [2:33:40<07:59, 15.47s/it] 95%|█████████▌| 574/604 [2:33:55<07:43, 15.44s/it] 95%|█████████▌| 575/604 [2:34:10<07:27, 15.44s/it] 95%|█████████▌| 576/604 [2:34:26<07:11, 15.40s/it] 96%|█████████▌| 577/604 [2:34:41<06:56, 15.42s/it] 96%|█████████▌| 578/604 [2:34:57<06:42, 15.48s/it] 96%|█████████▌| 579/604 [2:35:12<06:26, 15.45s/it] 96%|█████████▌| 580/604 [2:35:28<06:10, 15.43s/it]                                                   {'loss': 2.0844, 'learning_rate': 1e-05, 'epoch': 0.96} 96%|█████████▌| 580/604 [2:35:28<06:10, 15.43s/it] 96%|█████████▌| 581/604 [2:35:43<05:56, 15.50s/it] 96%|█████████▋| 582/604 [2:35:59<05:41, 15.50s/it] 97%|█████████▋| 583/604 [2:36:14<05:25, 15.51s/it] 97%|█████████▋| 584/604 [2:36:30<05:10, 15.50s/it] 97%|█████████▋| 585/604 [2:36:45<04:53, 15.45s/it] 97%|█████████▋| 586/604 [2:37:01<04:38, 15.48s/it] 97%|█████████▋| 587/604 [2:37:16<04:22, 15.44s/it] 97%|█████████▋| 588/604 [2:37:32<04:07, 15.49s/it] 98%|█████████▊| 589/604 [2:37:47<03:52, 15.48s/it] 98%|█████████▊| 590/604 [2:38:03<03:37, 15.50s/it]                                                   {'loss': 2.0628, 'learning_rate': 1e-05, 'epoch': 0.98} 98%|█████████▊| 590/604 [2:38:03<03:37, 15.50s/it] 98%|█████████▊| 591/604 [2:38:18<03:21, 15.49s/it] 98%|█████████▊| 592/604 [2:38:33<03:05, 15.46s/it] 98%|█████████▊| 593/604 [2:38:49<02:49, 15.44s/it] 98%|█████████▊| 594/604 [2:39:05<02:34, 15.50s/it] 99%|█████████▊| 595/604 [2:39:20<02:18, 15.41s/it] 99%|█████████▊| 596/604 [2:39:35<02:03, 15.44s/it] 99%|█████████▉| 597/604 [2:39:51<01:48, 15.43s/it] 99%|█████████▉| 598/604 [2:40:06<01:32, 15.43s/it] 99%|█████████▉| 599/604 [2:40:22<01:17, 15.44s/it] 99%|█████████▉| 600/604 [2:40:37<01:01, 15.39s/it]                                                   {'loss': 2.0762, 'learning_rate': 1e-05, 'epoch': 0.99} 99%|█████████▉| 600/604 [2:40:37<01:01, 15.39s/it]100%|█████████▉| 601/604 [2:41:40<01:29, 29.70s/it]100%|█████████▉| 602/604 [2:41:56<00:51, 25.56s/it]100%|█████████▉| 603/604 [2:42:11<00:22, 22.51s/it]100%|██████████| 604/604 [2:42:26<00:00, 20.27s/it]                                                   {'train_runtime': 9748.8335, 'train_samples_per_second': 7.924, 'train_steps_per_second': 0.062, 'train_loss': 2.145057728748448, 'epoch': 1.0}100%|██████████| 604/604 [2:42:28<00:00, 20.27s/it]100%|██████████| 604/604 [2:42:28<00:00, 16.14s/it]***** train metrics *****  epoch                    =        1.0  train_loss               =     2.1451  train_runtime            = 2:42:28.83  train_samples_per_second =      7.924  train_steps_per_second   =      0.062[2023-11-23 12:51:41,180] [INFO] [launch.py:347:main] Process 285724 exits successfully.[2023-11-23 12:51:41,181] [INFO] [launch.py:347:main] Process 285789 exits successfully.[2023-11-23 12:51:41,181] [INFO] [launch.py:347:main] Process 285725 exits successfully.[2023-11-23 12:51:49,190] [INFO] [launch.py:347:main] Process 285723 exits successfully./usr/local/miniconda3/envs/llm_env/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/miniconda3/envs/llm_env did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...  warn(msg)/usr/local/miniconda3/envs/llm_env/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.Either way, this might cause trouble in the future:If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.  warn(msg)